{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data related to power electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset from power_electronics\n"
     ]
    }
   ],
   "source": [
    "from engibench.problems.power_electronics.v0 import PowerElectronics\n",
    "\n",
    "print(\"[INFO] Loading dataset from power_electronics\")\n",
    "problem = PowerElectronics()\n",
    "ds = problem.dataset\n",
    "{\"train\": ds[\"train\"].to_pandas(), \"val\": ds[\"val\"].to_pandas(), \"test\": ds[\"test\"].to_pandas()}\n",
    "df_test = ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute log(r + 1e-8)\n",
    "log_r = np.log(df_test[\"Voltage_Ripple\"])\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(log_r, bins=200, alpha=0.7)\n",
    "plt.xlabel(\"log(r)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of log(r)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute log(r + 1e-8)\n",
    "log_g = np.log(df_test[\"DcGain\"])\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(log_g, bins=200, alpha=0.7)\n",
    "plt.xlabel(\"log(g)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of log(g)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['initial_design', 'DcGain', 'Voltage_Ripple'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DcGain</th>\n",
       "      <th>Voltage_Ripple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1383.000000</td>\n",
       "      <td>1383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.583973</td>\n",
       "      <td>4.164992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.954406</td>\n",
       "      <td>32.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.007811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.040142</td>\n",
       "      <td>0.100892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.205809</td>\n",
       "      <td>0.281515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.718619</td>\n",
       "      <td>1.031684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.293211</td>\n",
       "      <td>830.689200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DcGain  Voltage_Ripple\n",
       "count  1383.000000     1383.000000\n",
       "mean      0.583973        4.164992\n",
       "std       0.954406       32.428571\n",
       "min       0.000129        0.007811\n",
       "25%       0.040142        0.100892\n",
       "50%       0.205809        0.281515\n",
       "75%       0.718619        1.031684\n",
       "max       6.293211      830.689200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1.1336983289813492e-05, 8.362074166725276e-06...\n",
       "1       [7.0582242311733215e-06, 1.00603958348501e-05,...\n",
       "2       [1.8402470791292748e-05, 1.8468282763119614e-0...\n",
       "3       [1e-06, 1e-06, 2e-05, 1e-06, 2e-05, 2e-05, 0.0...\n",
       "4       [3.2740880623352055e-06, 4.021212109152507e-06...\n",
       "                              ...                        \n",
       "1378    [1.7845478360431024e-06, 4.346925834935858e-06...\n",
       "1379    [2e-05, 1e-06, 2e-05, 2e-05, 1e-06, 1e-06, 1e-...\n",
       "1380    [1e-06, 1e-06, 1e-06, 2e-05, 2e-05, 1e-06, 1e-...\n",
       "1381    [1.1206742324890197e-06, 4.620108745017289e-06...\n",
       "1382    [3.3463320640288854e-06, 4.301970595844075e-06...\n",
       "Name: initial_design, Length: 1383, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"initial_design\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with ax platform and botorch in backend\n",
    "\n",
    "## Output: g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "[INFO 04-28 16:11:40] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter learning_rate. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 04-28 16:11:40] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter hidden_layers. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `is_ordered` is not specified for `ChoiceParameter` \"hidden_layers\". Defaulting to `True`  since the parameter is not of type string.. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied `is_ordered` has no effect in this particular case.\n",
      "  return ChoiceParameter(\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"hidden_layers\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return ChoiceParameter(\n",
      "[INFO 04-28 16:11:40] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter hidden_size. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `is_ordered` is not specified for `ChoiceParameter` \"hidden_size\". Defaulting to `True`  since the parameter is not of type string.. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied `is_ordered` has no effect in this particular case.\n",
      "  return ChoiceParameter(\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"hidden_size\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return ChoiceParameter(\n",
      "[INFO 04-28 16:11:40] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter batch_size. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `is_ordered` is not specified for `ChoiceParameter` \"batch_size\". Defaulting to `True`  since the parameter is not of type string.. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied `is_ordered` has no effect in this particular case.\n",
      "  return ChoiceParameter(\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"batch_size\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return ChoiceParameter(\n",
      "[INFO 04-28 16:11:40] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter l2_lambda. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `is_ordered` is not specified for `ChoiceParameter` \"activation\". Defaulting to `True`  since there are exactly two choices.. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied `is_ordered` has no effect in this particular case.\n",
      "  return ChoiceParameter(\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/utils/instantiation.py:258: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"activation\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return ChoiceParameter(\n",
      "[INFO 04-28 16:11:40] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='learning_rate', parameter_type=FLOAT, range=[1e-05, 0.001], log_scale=True), ChoiceParameter(name='hidden_layers', parameter_type=INT, values=[2, 3, 4, 5], is_ordered=True, sort_values=True), ChoiceParameter(name='hidden_size', parameter_type=INT, values=[16, 32, 64, 128, 256], is_ordered=True, sort_values=True), ChoiceParameter(name='batch_size', parameter_type=INT, values=[8, 16, 32, 64, 128], is_ordered=True, sort_values=True), RangeParameter(name='l2_lambda', parameter_type=FLOAT, range=[1e-06, 0.001], log_scale=True), ChoiceParameter(name='activation', parameter_type=STRING, values=['relu', 'tanh'], is_ordered=True, sort_values=False)], parameter_constraints=[]).\n",
      "[INFO 04-28 16:11:40] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 04-28 16:11:40] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False\n",
      "[INFO 04-28 16:11:40] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12\n",
      "[INFO 04-28 16:11:40] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12\n",
      "[INFO 04-28 16:11:40] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 04-28 16:11:40] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 12 trials, BoTorch for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.\n",
      "[INFO 04-28 16:11:40] ax.service.managed_loop: Started full optimization with 50 steps.\n",
      "[INFO 04-28 16:11:40] ax.service.managed_loop: Running optimization trial 1...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflorian-felten\u001b[0m (\u001b[33mengibench\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161146-v5fypm7d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849505\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/v5fypm7d\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.2166, Val: 0.2168\n",
      "[Epoch 2/30] Train: 0.2130, Val: 0.2134\n",
      "[Epoch 3/30] Train: 0.2098, Val: 0.2105\n",
      "[Epoch 4/30] Train: 0.2069, Val: 0.2079\n",
      "[Epoch 5/30] Train: 0.2044, Val: 0.2056\n",
      "[Epoch 6/30] Train: 0.2022, Val: 0.2036\n",
      "[Epoch 7/30] Train: 0.2002, Val: 0.2019\n",
      "[Epoch 8/30] Train: 0.1986, Val: 0.2005\n",
      "[Epoch 9/30] Train: 0.1972, Val: 0.1993\n",
      "[Epoch 10/30] Train: 0.1960, Val: 0.1984\n",
      "[Epoch 11/30] Train: 0.1951, Val: 0.1976\n",
      "[Epoch 12/30] Train: 0.1943, Val: 0.1970\n",
      "[Epoch 13/30] Train: 0.1937, Val: 0.1965\n",
      "[Epoch 14/30] Train: 0.1932, Val: 0.1961\n",
      "[Epoch 15/30] Train: 0.1928, Val: 0.1958\n",
      "[Epoch 16/30] Train: 0.1925, Val: 0.1955\n",
      "[Epoch 17/30] Train: 0.1922, Val: 0.1953\n",
      "[Epoch 18/30] Train: 0.1920, Val: 0.1951\n",
      "[Epoch 19/30] Train: 0.1918, Val: 0.1949\n",
      "[Epoch 20/30] Train: 0.1916, Val: 0.1948\n",
      "[Epoch 21/30] Train: 0.1915, Val: 0.1947\n",
      "[Epoch 22/30] Train: 0.1913, Val: 0.1946\n",
      "[Epoch 23/30] Train: 0.1912, Val: 0.1945\n",
      "[Epoch 24/30] Train: 0.1911, Val: 0.1943\n",
      "[Epoch 25/30] Train: 0.1910, Val: 0.1942\n",
      "[Epoch 26/30] Train: 0.1909, Val: 0.1941\n",
      "[Epoch 27/30] Train: 0.1908, Val: 0.1941\n",
      "[Epoch 28/30] Train: 0.1907, Val: 0.1940\n",
      "[Epoch 29/30] Train: 0.1906, Val: 0.1939\n",
      "[Epoch 30/30] Train: 0.1906, Val: 0.1938\n",
      "Best Val Loss: 0.1938 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.1938\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849505_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849505_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.1392, True=0.0577\n",
      "  Sample 1: Pred=0.0555, True=0.6128\n",
      "  Sample 2: Pred=0.2134, True=0.0985\n",
      "  Sample 3: Pred=0.2105, True=1.1945\n",
      "  Sample 4: Pred=0.0608, True=0.0140\n",
      "[INFO] Test MSE: 1.022809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.023 MB of 0.023 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.023 MB of 0.023 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.023 MB of 0.023 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▆▅▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 1.02281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.19055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.19379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849505\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/v5fypm7d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161146-v5fypm7d/logs\u001b[0m\n",
      "[INFO 04-28 16:13:31] ax.service.managed_loop: Running optimization trial 2...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161334-fczmbw8h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849614\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/fczmbw8h\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1837, Val: 0.1331\n",
      "[Epoch 2/30] Train: 0.1070, Val: 0.0897\n",
      "[Epoch 3/30] Train: 0.0803, Val: 0.0743\n",
      "[Epoch 4/30] Train: 0.0701, Val: 0.0665\n",
      "[Epoch 5/30] Train: 0.0636, Val: 0.0626\n",
      "[Epoch 6/30] Train: 0.0597, Val: 0.0594\n",
      "[Epoch 7/30] Train: 0.0561, Val: 0.0584\n",
      "[Epoch 8/30] Train: 0.0539, Val: 0.0568\n",
      "[Epoch 9/30] Train: 0.0519, Val: 0.0547\n",
      "[Epoch 10/30] Train: 0.0497, Val: 0.0528\n",
      "[Epoch 11/30] Train: 0.0483, Val: 0.0512\n",
      "[Epoch 12/30] Train: 0.0466, Val: 0.0512\n",
      "[Epoch 13/30] Train: 0.0456, Val: 0.0515\n",
      "[Epoch 14/30] Train: 0.0446, Val: 0.0499\n",
      "[Epoch 15/30] Train: 0.0437, Val: 0.0487\n",
      "[Epoch 16/30] Train: 0.0426, Val: 0.0486\n",
      "[Epoch 17/30] Train: 0.0422, Val: 0.0479\n",
      "[Epoch 18/30] Train: 0.0412, Val: 0.0495\n",
      "[Epoch 19/30] Train: 0.0410, Val: 0.0463\n",
      "[Epoch 20/30] Train: 0.0401, Val: 0.0468\n",
      "[Epoch 21/30] Train: 0.0396, Val: 0.0476\n",
      "[Epoch 22/30] Train: 0.0392, Val: 0.0457\n",
      "[Epoch 23/30] Train: 0.0387, Val: 0.0451\n",
      "[Epoch 24/30] Train: 0.0383, Val: 0.0453\n",
      "[Epoch 25/30] Train: 0.0380, Val: 0.0442\n",
      "[Epoch 26/30] Train: 0.0374, Val: 0.0438\n",
      "[Epoch 27/30] Train: 0.0370, Val: 0.0447\n",
      "[Epoch 28/30] Train: 0.0366, Val: 0.0435\n",
      "[Epoch 29/30] Train: 0.0362, Val: 0.0430\n",
      "[Epoch 30/30] Train: 0.0359, Val: 0.0439\n",
      "Best Val Loss: 0.0430 at epoch 29\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0430\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849614_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849614_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0203, True=0.0577\n",
      "  Sample 1: Pred=0.4236, True=0.6128\n",
      "  Sample 2: Pred=0.0584, True=0.0985\n",
      "  Sample 3: Pred=0.5376, True=1.1945\n",
      "  Sample 4: Pred=0.0213, True=0.0140\n",
      "[INFO] Test MSE: 0.210484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.21048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.03591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.04388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849614\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/fczmbw8h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161334-fczmbw8h/logs\u001b[0m\n",
      "[INFO 04-28 16:14:15] ax.service.managed_loop: Running optimization trial 3...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161417-3s3awxvd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849657\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/3s3awxvd\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1622, Val: 0.1026\n",
      "[Epoch 2/30] Train: 0.0869, Val: 0.0762\n",
      "[Epoch 3/30] Train: 0.0722, Val: 0.0672\n",
      "[Epoch 4/30] Train: 0.0658, Val: 0.0639\n",
      "[Epoch 5/30] Train: 0.0607, Val: 0.0583\n",
      "[Epoch 6/30] Train: 0.0567, Val: 0.0556\n",
      "[Epoch 7/30] Train: 0.0539, Val: 0.0532\n",
      "[Epoch 8/30] Train: 0.0509, Val: 0.0533\n",
      "[Epoch 9/30] Train: 0.0494, Val: 0.0506\n",
      "[Epoch 10/30] Train: 0.0478, Val: 0.0501\n",
      "[Epoch 11/30] Train: 0.0462, Val: 0.0491\n",
      "[Epoch 12/30] Train: 0.0453, Val: 0.0490\n",
      "[Epoch 13/30] Train: 0.0444, Val: 0.0475\n",
      "[Epoch 14/30] Train: 0.0441, Val: 0.0462\n",
      "[Epoch 15/30] Train: 0.0431, Val: 0.0465\n",
      "[Epoch 16/30] Train: 0.0426, Val: 0.0467\n",
      "[Epoch 17/30] Train: 0.0422, Val: 0.0464\n",
      "[Epoch 18/30] Train: 0.0415, Val: 0.0460\n",
      "[Epoch 19/30] Train: 0.0406, Val: 0.0461\n",
      "[Epoch 20/30] Train: 0.0407, Val: 0.0445\n",
      "[Epoch 21/30] Train: 0.0403, Val: 0.0439\n",
      "[Epoch 22/30] Train: 0.0396, Val: 0.0441\n",
      "[Epoch 23/30] Train: 0.0396, Val: 0.0446\n",
      "[Epoch 24/30] Train: 0.0388, Val: 0.0438\n",
      "[Epoch 25/30] Train: 0.0387, Val: 0.0439\n",
      "[Epoch 26/30] Train: 0.0384, Val: 0.0448\n",
      "[Epoch 27/30] Train: 0.0383, Val: 0.0437\n",
      "[Epoch 28/30] Train: 0.0382, Val: 0.0445\n",
      "[Epoch 29/30] Train: 0.0378, Val: 0.0442\n",
      "[Epoch 30/30] Train: 0.0375, Val: 0.0428\n",
      "Best Val Loss: 0.0428 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0428\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849657_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849657_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0207, True=0.0577\n",
      "  Sample 1: Pred=0.3674, True=0.6128\n",
      "  Sample 2: Pred=0.0891, True=0.0985\n",
      "  Sample 3: Pred=1.3140, True=1.1945\n",
      "  Sample 4: Pred=0.0200, True=0.0140\n",
      "[INFO] Test MSE: 0.184818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.18482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.03749\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.04276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849657\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/3s3awxvd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161417-3s3awxvd/logs\u001b[0m\n",
      "[INFO 04-28 16:14:51] ax.service.managed_loop: Running optimization trial 4...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161453-vt4dj3we\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849693\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/vt4dj3we\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1963, Val: 0.1940\n",
      "[Epoch 2/30] Train: 0.1903, Val: 0.1922\n",
      "[Epoch 3/30] Train: 0.1887, Val: 0.1897\n",
      "[Epoch 4/30] Train: 0.1845, Val: 0.1833\n",
      "[Epoch 5/30] Train: 0.1690, Val: 0.1540\n",
      "[Epoch 6/30] Train: 0.1324, Val: 0.1229\n",
      "[Epoch 7/30] Train: 0.1187, Val: 0.1199\n",
      "[Epoch 8/30] Train: 0.1161, Val: 0.1150\n",
      "[Epoch 9/30] Train: 0.1119, Val: 0.1150\n",
      "[Epoch 10/30] Train: 0.1086, Val: 0.1094\n",
      "[Epoch 11/30] Train: 0.1053, Val: 0.1096\n",
      "[Epoch 12/30] Train: 0.1023, Val: 0.1033\n",
      "[Epoch 13/30] Train: 0.1011, Val: 0.0992\n",
      "[Epoch 14/30] Train: 0.0996, Val: 0.1001\n",
      "[Epoch 15/30] Train: 0.0974, Val: 0.0985\n",
      "[Epoch 16/30] Train: 0.0957, Val: 0.0942\n",
      "[Epoch 17/30] Train: 0.0945, Val: 0.0947\n",
      "[Epoch 18/30] Train: 0.0946, Val: 0.0961\n",
      "[Epoch 19/30] Train: 0.0936, Val: 0.0904\n",
      "[Epoch 20/30] Train: 0.0925, Val: 0.0942\n",
      "[Epoch 21/30] Train: 0.0922, Val: 0.0905\n",
      "[Epoch 22/30] Train: 0.0898, Val: 0.0887\n",
      "[Epoch 23/30] Train: 0.0903, Val: 0.0906\n",
      "[Epoch 24/30] Train: 0.0897, Val: 0.0902\n",
      "[Epoch 25/30] Train: 0.0882, Val: 0.0867\n",
      "[Epoch 26/30] Train: 0.0867, Val: 0.0845\n",
      "[Epoch 27/30] Train: 0.0864, Val: 0.0877\n",
      "[Epoch 28/30] Train: 0.0858, Val: 0.0892\n",
      "[Epoch 29/30] Train: 0.0848, Val: 0.0848\n",
      "[Epoch 30/30] Train: 0.0842, Val: 0.0842\n",
      "Best Val Loss: 0.0842 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0842\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849693_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849693_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0465, True=0.0577\n",
      "  Sample 1: Pred=0.0909, True=0.6128\n",
      "  Sample 2: Pred=0.0992, True=0.0985\n",
      "  Sample 3: Pred=0.1850, True=1.1945\n",
      "  Sample 4: Pred=0.0189, True=0.0140\n",
      "[INFO] Test MSE: 0.832472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.039 MB of 1.039 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.039 MB of 1.039 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.039 MB of 1.039 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.039 MB of 1.039 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ███▇▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ███▇▅▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.83247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.08419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.08419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849693\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/vt4dj3we\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161453-vt4dj3we/logs\u001b[0m\n",
      "[INFO 04-28 16:15:21] ax.service.managed_loop: Running optimization trial 5...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161524-lw8sw8tl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849724\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/lw8sw8tl\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.2022, Val: 0.1963\n",
      "[Epoch 2/30] Train: 0.1924, Val: 0.1947\n",
      "[Epoch 3/30] Train: 0.1915, Val: 0.1943\n",
      "[Epoch 4/30] Train: 0.1912, Val: 0.1938\n",
      "[Epoch 5/30] Train: 0.1908, Val: 0.1934\n",
      "[Epoch 6/30] Train: 0.1904, Val: 0.1929\n",
      "[Epoch 7/30] Train: 0.1900, Val: 0.1924\n",
      "[Epoch 8/30] Train: 0.1895, Val: 0.1922\n",
      "[Epoch 9/30] Train: 0.1890, Val: 0.1915\n",
      "[Epoch 10/30] Train: 0.1883, Val: 0.1908\n",
      "[Epoch 11/30] Train: 0.1878, Val: 0.1901\n",
      "[Epoch 12/30] Train: 0.1869, Val: 0.1891\n",
      "[Epoch 13/30] Train: 0.1860, Val: 0.1880\n",
      "[Epoch 14/30] Train: 0.1849, Val: 0.1871\n",
      "[Epoch 15/30] Train: 0.1833, Val: 0.1856\n",
      "[Epoch 16/30] Train: 0.1813, Val: 0.1829\n",
      "[Epoch 17/30] Train: 0.1787, Val: 0.1794\n",
      "[Epoch 18/30] Train: 0.1748, Val: 0.1751\n",
      "[Epoch 19/30] Train: 0.1698, Val: 0.1690\n",
      "[Epoch 20/30] Train: 0.1628, Val: 0.1615\n",
      "[Epoch 21/30] Train: 0.1544, Val: 0.1522\n",
      "[Epoch 22/30] Train: 0.1454, Val: 0.1440\n",
      "[Epoch 23/30] Train: 0.1371, Val: 0.1357\n",
      "[Epoch 24/30] Train: 0.1299, Val: 0.1299\n",
      "[Epoch 25/30] Train: 0.1250, Val: 0.1258\n",
      "[Epoch 26/30] Train: 0.1217, Val: 0.1239\n",
      "[Epoch 27/30] Train: 0.1193, Val: 0.1228\n",
      "[Epoch 28/30] Train: 0.1179, Val: 0.1199\n",
      "[Epoch 29/30] Train: 0.1161, Val: 0.1184\n",
      "[Epoch 30/30] Train: 0.1154, Val: 0.1185\n",
      "Best Val Loss: 0.1184 at epoch 29\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.1184\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849724_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849724_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0339, True=0.0577\n",
      "  Sample 1: Pred=0.1331, True=0.6128\n",
      "  Sample 2: Pred=0.0597, True=0.0985\n",
      "  Sample 3: Pred=0.1315, True=1.1945\n",
      "  Sample 4: Pred=0.0169, True=0.0140\n",
      "[INFO] Test MSE: 0.944531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.013 MB of 0.013 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.154 MB of 0.154 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▄▃▃▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██████████▇▇▇▇▇▇▆▆▆▅▄▃▃▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.94453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.11541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.11846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849724\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/lw8sw8tl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161524-lw8sw8tl/logs\u001b[0m\n",
      "[INFO 04-28 16:15:39] ax.service.managed_loop: Running optimization trial 6...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161541-10ea53qq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849741\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/10ea53qq\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1957, Val: 0.1564\n",
      "[Epoch 2/30] Train: 0.1254, Val: 0.1016\n",
      "[Epoch 3/30] Train: 0.0938, Val: 0.0854\n",
      "[Epoch 4/30] Train: 0.0845, Val: 0.0798\n",
      "[Epoch 5/30] Train: 0.0795, Val: 0.0787\n",
      "[Epoch 6/30] Train: 0.0763, Val: 0.0733\n",
      "[Epoch 7/30] Train: 0.0738, Val: 0.0716\n",
      "[Epoch 8/30] Train: 0.0721, Val: 0.0709\n",
      "[Epoch 9/30] Train: 0.0704, Val: 0.0693\n",
      "[Epoch 10/30] Train: 0.0693, Val: 0.0675\n",
      "[Epoch 11/30] Train: 0.0681, Val: 0.0666\n",
      "[Epoch 12/30] Train: 0.0673, Val: 0.0654\n",
      "[Epoch 13/30] Train: 0.0661, Val: 0.0641\n",
      "[Epoch 14/30] Train: 0.0652, Val: 0.0668\n",
      "[Epoch 15/30] Train: 0.0645, Val: 0.0643\n",
      "[Epoch 16/30] Train: 0.0638, Val: 0.0622\n",
      "[Epoch 17/30] Train: 0.0631, Val: 0.0629\n",
      "[Epoch 18/30] Train: 0.0620, Val: 0.0621\n",
      "[Epoch 19/30] Train: 0.0617, Val: 0.0605\n",
      "[Epoch 20/30] Train: 0.0609, Val: 0.0600\n",
      "[Epoch 21/30] Train: 0.0602, Val: 0.0591\n",
      "[Epoch 22/30] Train: 0.0591, Val: 0.0583\n",
      "[Epoch 23/30] Train: 0.0585, Val: 0.0574\n",
      "[Epoch 24/30] Train: 0.0578, Val: 0.0573\n",
      "[Epoch 25/30] Train: 0.0571, Val: 0.0563\n",
      "[Epoch 26/30] Train: 0.0565, Val: 0.0553\n",
      "[Epoch 27/30] Train: 0.0560, Val: 0.0546\n",
      "[Epoch 28/30] Train: 0.0553, Val: 0.0554\n",
      "[Epoch 29/30] Train: 0.0549, Val: 0.0553\n",
      "[Epoch 30/30] Train: 0.0542, Val: 0.0554\n",
      "Best Val Loss: 0.0546 at epoch 27\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0546\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849741_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849741_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0189, True=0.0577\n",
      "  Sample 1: Pred=0.2504, True=0.6128\n",
      "  Sample 2: Pred=0.0481, True=0.0985\n",
      "  Sample 3: Pred=0.1766, True=1.1945\n",
      "  Sample 4: Pred=0.0204, True=0.0140\n",
      "[INFO] Test MSE: 0.400870\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.013 MB of 0.013 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.029 MB of 0.029 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.029 MB of 0.029 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.029 MB of 0.029 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.40087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.05424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.05539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849741\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/10ea53qq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161541-10ea53qq/logs\u001b[0m\n",
      "[INFO 04-28 16:17:06] ax.service.managed_loop: Running optimization trial 7...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161708-l1e8ctz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849828\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/l1e8ctz8\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1701, Val: 0.1362\n",
      "[Epoch 2/30] Train: 0.1162, Val: 0.1010\n",
      "[Epoch 3/30] Train: 0.0960, Val: 0.0906\n",
      "[Epoch 4/30] Train: 0.0876, Val: 0.0828\n",
      "[Epoch 5/30] Train: 0.0811, Val: 0.0779\n",
      "[Epoch 6/30] Train: 0.0761, Val: 0.0736\n",
      "[Epoch 7/30] Train: 0.0715, Val: 0.0714\n",
      "[Epoch 8/30] Train: 0.0674, Val: 0.0669\n",
      "[Epoch 9/30] Train: 0.0641, Val: 0.0646\n",
      "[Epoch 10/30] Train: 0.0611, Val: 0.0619\n",
      "[Epoch 11/30] Train: 0.0588, Val: 0.0609\n",
      "[Epoch 12/30] Train: 0.0566, Val: 0.0595\n",
      "[Epoch 13/30] Train: 0.0551, Val: 0.0579\n",
      "[Epoch 14/30] Train: 0.0533, Val: 0.0562\n",
      "[Epoch 15/30] Train: 0.0520, Val: 0.0555\n",
      "[Epoch 16/30] Train: 0.0508, Val: 0.0550\n",
      "[Epoch 17/30] Train: 0.0498, Val: 0.0546\n",
      "[Epoch 18/30] Train: 0.0485, Val: 0.0527\n",
      "[Epoch 19/30] Train: 0.0479, Val: 0.0522\n",
      "[Epoch 20/30] Train: 0.0470, Val: 0.0521\n",
      "[Epoch 21/30] Train: 0.0458, Val: 0.0518\n",
      "[Epoch 22/30] Train: 0.0451, Val: 0.0510\n",
      "[Epoch 23/30] Train: 0.0445, Val: 0.0497\n",
      "[Epoch 24/30] Train: 0.0439, Val: 0.0496\n",
      "[Epoch 25/30] Train: 0.0434, Val: 0.0493\n",
      "[Epoch 26/30] Train: 0.0426, Val: 0.0494\n",
      "[Epoch 27/30] Train: 0.0422, Val: 0.0484\n",
      "[Epoch 28/30] Train: 0.0417, Val: 0.0488\n",
      "[Epoch 29/30] Train: 0.0408, Val: 0.0487\n",
      "[Epoch 30/30] Train: 0.0404, Val: 0.0489\n",
      "Best Val Loss: 0.0484 at epoch 27\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0484\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849828_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849828_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0312, True=0.0577\n",
      "  Sample 1: Pred=0.2605, True=0.6128\n",
      "  Sample 2: Pred=0.0782, True=0.0985\n",
      "  Sample 3: Pred=0.6072, True=1.1945\n",
      "  Sample 4: Pred=0.0197, True=0.0140\n",
      "[INFO] Test MSE: 0.296325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.284 MB of 0.284 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.285 MB of 0.285 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.285 MB of 0.285 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.285 MB of 0.285 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.29633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.04044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.04893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849828\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/l1e8ctz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161708-l1e8ctz8/logs\u001b[0m\n",
      "[INFO 04-28 16:17:27] ax.service.managed_loop: Running optimization trial 8...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161729-206j29wr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849849\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/206j29wr\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.2001, Val: 0.1943\n",
      "[Epoch 2/30] Train: 0.1903, Val: 0.1924\n",
      "[Epoch 3/30] Train: 0.1890, Val: 0.1916\n",
      "[Epoch 4/30] Train: 0.1882, Val: 0.1911\n",
      "[Epoch 5/30] Train: 0.1875, Val: 0.1899\n",
      "[Epoch 6/30] Train: 0.1865, Val: 0.1887\n",
      "[Epoch 7/30] Train: 0.1852, Val: 0.1871\n",
      "[Epoch 8/30] Train: 0.1833, Val: 0.1849\n",
      "[Epoch 9/30] Train: 0.1803, Val: 0.1813\n",
      "[Epoch 10/30] Train: 0.1749, Val: 0.1739\n",
      "[Epoch 11/30] Train: 0.1651, Val: 0.1614\n",
      "[Epoch 12/30] Train: 0.1501, Val: 0.1447\n",
      "[Epoch 13/30] Train: 0.1352, Val: 0.1321\n",
      "[Epoch 14/30] Train: 0.1260, Val: 0.1259\n",
      "[Epoch 15/30] Train: 0.1218, Val: 0.1230\n",
      "[Epoch 16/30] Train: 0.1196, Val: 0.1210\n",
      "[Epoch 17/30] Train: 0.1181, Val: 0.1196\n",
      "[Epoch 18/30] Train: 0.1169, Val: 0.1192\n",
      "[Epoch 19/30] Train: 0.1159, Val: 0.1165\n",
      "[Epoch 20/30] Train: 0.1147, Val: 0.1158\n",
      "[Epoch 21/30] Train: 0.1137, Val: 0.1144\n",
      "[Epoch 22/30] Train: 0.1126, Val: 0.1128\n",
      "[Epoch 23/30] Train: 0.1118, Val: 0.1127\n",
      "[Epoch 24/30] Train: 0.1108, Val: 0.1109\n",
      "[Epoch 25/30] Train: 0.1098, Val: 0.1103\n",
      "[Epoch 26/30] Train: 0.1088, Val: 0.1083\n",
      "[Epoch 27/30] Train: 0.1079, Val: 0.1077\n",
      "[Epoch 28/30] Train: 0.1070, Val: 0.1065\n",
      "[Epoch 29/30] Train: 0.1062, Val: 0.1051\n",
      "[Epoch 30/30] Train: 0.1054, Val: 0.1043\n",
      "Best Val Loss: 0.1043 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.1043\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849849_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849849_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0319, True=0.0577\n",
      "  Sample 1: Pred=0.1490, True=0.6128\n",
      "  Sample 2: Pred=0.0868, True=0.0985\n",
      "  Sample 3: Pred=0.1995, True=1.1945\n",
      "  Sample 4: Pred=0.0171, True=0.0140\n",
      "[INFO] Test MSE: 0.896379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▇▇▇▇▇▇▇▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██████▇▇▇▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.89638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.10537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.10426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849849\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/206j29wr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161729-206j29wr/logs\u001b[0m\n",
      "[INFO 04-28 16:19:54] ax.service.managed_loop: Running optimization trial 9...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_161956-cnnsdz0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849996\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/cnnsdz0i\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1947, Val: 0.1954\n",
      "[Epoch 2/30] Train: 0.1917, Val: 0.1938\n",
      "[Epoch 3/30] Train: 0.1910, Val: 0.1933\n",
      "[Epoch 4/30] Train: 0.1899, Val: 0.1927\n",
      "[Epoch 5/30] Train: 0.1893, Val: 0.1914\n",
      "[Epoch 6/30] Train: 0.1879, Val: 0.1902\n",
      "[Epoch 7/30] Train: 0.1862, Val: 0.1875\n",
      "[Epoch 8/30] Train: 0.1828, Val: 0.1838\n",
      "[Epoch 9/30] Train: 0.1772, Val: 0.1757\n",
      "[Epoch 10/30] Train: 0.1670, Val: 0.1637\n",
      "[Epoch 11/30] Train: 0.1533, Val: 0.1494\n",
      "[Epoch 12/30] Train: 0.1404, Val: 0.1380\n",
      "[Epoch 13/30] Train: 0.1313, Val: 0.1312\n",
      "[Epoch 14/30] Train: 0.1261, Val: 0.1293\n",
      "[Epoch 15/30] Train: 0.1233, Val: 0.1251\n",
      "[Epoch 16/30] Train: 0.1216, Val: 0.1261\n",
      "[Epoch 17/30] Train: 0.1199, Val: 0.1236\n",
      "[Epoch 18/30] Train: 0.1187, Val: 0.1208\n",
      "[Epoch 19/30] Train: 0.1172, Val: 0.1202\n",
      "[Epoch 20/30] Train: 0.1160, Val: 0.1185\n",
      "[Epoch 21/30] Train: 0.1144, Val: 0.1170\n",
      "[Epoch 22/30] Train: 0.1132, Val: 0.1162\n",
      "[Epoch 23/30] Train: 0.1116, Val: 0.1144\n",
      "[Epoch 24/30] Train: 0.1104, Val: 0.1123\n",
      "[Epoch 25/30] Train: 0.1087, Val: 0.1107\n",
      "[Epoch 26/30] Train: 0.1070, Val: 0.1088\n",
      "[Epoch 27/30] Train: 0.1056, Val: 0.1067\n",
      "[Epoch 28/30] Train: 0.1042, Val: 0.1061\n",
      "[Epoch 29/30] Train: 0.1025, Val: 0.1052\n",
      "[Epoch 30/30] Train: 0.1013, Val: 0.1035\n",
      "Best Val Loss: 0.1035 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.1035\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849996_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745849996_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0437, True=0.0577\n",
      "  Sample 1: Pred=0.1314, True=0.6128\n",
      "  Sample 2: Pred=0.0825, True=0.0985\n",
      "  Sample 3: Pred=0.0793, True=1.1945\n",
      "  Sample 4: Pred=0.0146, True=0.0140\n",
      "[INFO] Test MSE: 0.915317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.536 MB of 0.536 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.536 MB of 0.536 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.536 MB of 0.536 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.536 MB of 0.536 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.536 MB of 0.536 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.536 MB of 0.536 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █████▇▇▇▇▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██████▇▇▆▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.91532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.10129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.10346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745849996\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/cnnsdz0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_161956-cnnsdz0i/logs\u001b[0m\n",
      "[INFO 04-28 16:21:08] ax.service.managed_loop: Running optimization trial 10...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_162110-kq4h397i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850070\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/kq4h397i\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.2319, Val: 0.2288\n",
      "[Epoch 2/30] Train: 0.2228, Val: 0.2217\n",
      "[Epoch 3/30] Train: 0.2177, Val: 0.2179\n",
      "[Epoch 4/30] Train: 0.2146, Val: 0.2142\n",
      "[Epoch 5/30] Train: 0.2088, Val: 0.2053\n",
      "[Epoch 6/30] Train: 0.1961, Val: 0.1883\n",
      "[Epoch 7/30] Train: 0.1753, Val: 0.1643\n",
      "[Epoch 8/30] Train: 0.1521, Val: 0.1412\n",
      "[Epoch 9/30] Train: 0.1321, Val: 0.1228\n",
      "[Epoch 10/30] Train: 0.1171, Val: 0.1105\n",
      "[Epoch 11/30] Train: 0.1077, Val: 0.1032\n",
      "[Epoch 12/30] Train: 0.1016, Val: 0.0982\n",
      "[Epoch 13/30] Train: 0.0971, Val: 0.0942\n",
      "[Epoch 14/30] Train: 0.0937, Val: 0.0925\n",
      "[Epoch 15/30] Train: 0.0910, Val: 0.0891\n",
      "[Epoch 16/30] Train: 0.0887, Val: 0.0871\n",
      "[Epoch 17/30] Train: 0.0866, Val: 0.0854\n",
      "[Epoch 18/30] Train: 0.0849, Val: 0.0836\n",
      "[Epoch 19/30] Train: 0.0833, Val: 0.0822\n",
      "[Epoch 20/30] Train: 0.0817, Val: 0.0806\n",
      "[Epoch 21/30] Train: 0.0804, Val: 0.0796\n",
      "[Epoch 22/30] Train: 0.0791, Val: 0.0785\n",
      "[Epoch 23/30] Train: 0.0780, Val: 0.0774\n",
      "[Epoch 24/30] Train: 0.0767, Val: 0.0761\n",
      "[Epoch 25/30] Train: 0.0756, Val: 0.0756\n",
      "[Epoch 26/30] Train: 0.0744, Val: 0.0740\n",
      "[Epoch 27/30] Train: 0.0734, Val: 0.0732\n",
      "[Epoch 28/30] Train: 0.0724, Val: 0.0723\n",
      "[Epoch 29/30] Train: 0.0714, Val: 0.0718\n",
      "[Epoch 30/30] Train: 0.0707, Val: 0.0709\n",
      "Best Val Loss: 0.0709 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0709\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850070_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850070_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0285, True=0.0577\n",
      "  Sample 1: Pred=0.0728, True=0.6128\n",
      "  Sample 2: Pred=0.0566, True=0.0985\n",
      "  Sample 3: Pred=0.6772, True=1.1945\n",
      "  Sample 4: Pred=0.0091, True=0.0140\n",
      "[INFO] Test MSE: 0.593040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.041 MB of 0.041 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.042 MB of 0.042 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.042 MB of 0.042 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.042 MB of 0.042 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ██▇▇▇▆▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ███▇▇▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.59304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.07069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.07092\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850070\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/kq4h397i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_162110-kq4h397i/logs\u001b[0m\n",
      "[INFO 04-28 16:21:26] ax.service.managed_loop: Running optimization trial 11...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_162128-bbckyo4g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850088\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/bbckyo4g\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1147, Val: 0.0819\n",
      "[Epoch 2/30] Train: 0.0760, Val: 0.0700\n",
      "[Epoch 3/30] Train: 0.0662, Val: 0.0657\n",
      "[Epoch 4/30] Train: 0.0600, Val: 0.0582\n",
      "[Epoch 5/30] Train: 0.0541, Val: 0.0559\n",
      "[Epoch 6/30] Train: 0.0505, Val: 0.0515\n",
      "[Epoch 7/30] Train: 0.0475, Val: 0.0509\n",
      "[Epoch 8/30] Train: 0.0454, Val: 0.0482\n",
      "[Epoch 9/30] Train: 0.0441, Val: 0.0508\n",
      "[Epoch 10/30] Train: 0.0425, Val: 0.0468\n",
      "[Epoch 11/30] Train: 0.0410, Val: 0.0457\n",
      "[Epoch 12/30] Train: 0.0402, Val: 0.0451\n",
      "[Epoch 13/30] Train: 0.0394, Val: 0.0454\n",
      "[Epoch 14/30] Train: 0.0382, Val: 0.0458\n",
      "[Epoch 15/30] Train: 0.0376, Val: 0.0450\n",
      "[Epoch 16/30] Train: 0.0365, Val: 0.0459\n",
      "[Epoch 17/30] Train: 0.0357, Val: 0.0428\n",
      "[Epoch 18/30] Train: 0.0354, Val: 0.0443\n",
      "[Epoch 19/30] Train: 0.0347, Val: 0.0433\n",
      "[Epoch 20/30] Train: 0.0342, Val: 0.0425\n",
      "[Epoch 21/30] Train: 0.0334, Val: 0.0413\n",
      "[Epoch 22/30] Train: 0.0331, Val: 0.0416\n",
      "[Epoch 23/30] Train: 0.0330, Val: 0.0420\n",
      "[Epoch 24/30] Train: 0.0324, Val: 0.0432\n",
      "[Epoch 25/30] Train: 0.0316, Val: 0.0415\n",
      "[Epoch 26/30] Train: 0.0316, Val: 0.0399\n",
      "[Epoch 27/30] Train: 0.0310, Val: 0.0407\n",
      "[Epoch 28/30] Train: 0.0309, Val: 0.0386\n",
      "[Epoch 29/30] Train: 0.0302, Val: 0.0412\n",
      "[Epoch 30/30] Train: 0.0303, Val: 0.0391\n",
      "Best Val Loss: 0.0386 at epoch 28\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0386\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850088_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850088_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0163, True=0.0577\n",
      "  Sample 1: Pred=0.4964, True=0.6128\n",
      "  Sample 2: Pred=0.0820, True=0.0985\n",
      "  Sample 3: Pred=0.7407, True=1.1945\n",
      "  Sample 4: Pred=0.0088, True=0.0140\n",
      "[INFO] Test MSE: 0.506941\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.013 MB of 0.013 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.090 MB of 0.090 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.091 MB of 0.091 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.091 MB of 0.091 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.091 MB of 0.091 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.50694\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.03029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.03913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850088\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/bbckyo4g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_162128-bbckyo4g/logs\u001b[0m\n",
      "[INFO 04-28 16:23:17] ax.service.managed_loop: Running optimization trial 12...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/modelbridge/cross_validation.py:439: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_162320-0yx98s0a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/0yx98s0a\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.2127, Val: 0.2074\n",
      "[Epoch 2/30] Train: 0.1999, Val: 0.1986\n",
      "[Epoch 3/30] Train: 0.1940, Val: 0.1964\n",
      "[Epoch 4/30] Train: 0.1926, Val: 0.1955\n",
      "[Epoch 5/30] Train: 0.1919, Val: 0.1950\n",
      "[Epoch 6/30] Train: 0.1914, Val: 0.1944\n",
      "[Epoch 7/30] Train: 0.1910, Val: 0.1940\n",
      "[Epoch 8/30] Train: 0.1906, Val: 0.1936\n",
      "[Epoch 9/30] Train: 0.1902, Val: 0.1932\n",
      "[Epoch 10/30] Train: 0.1898, Val: 0.1928\n",
      "[Epoch 11/30] Train: 0.1895, Val: 0.1924\n",
      "[Epoch 12/30] Train: 0.1890, Val: 0.1919\n",
      "[Epoch 13/30] Train: 0.1885, Val: 0.1914\n",
      "[Epoch 14/30] Train: 0.1880, Val: 0.1909\n",
      "[Epoch 15/30] Train: 0.1874, Val: 0.1903\n",
      "[Epoch 16/30] Train: 0.1867, Val: 0.1895\n",
      "[Epoch 17/30] Train: 0.1858, Val: 0.1886\n",
      "[Epoch 18/30] Train: 0.1847, Val: 0.1874\n",
      "[Epoch 19/30] Train: 0.1834, Val: 0.1856\n",
      "[Epoch 20/30] Train: 0.1814, Val: 0.1834\n",
      "[Epoch 21/30] Train: 0.1785, Val: 0.1808\n",
      "[Epoch 22/30] Train: 0.1743, Val: 0.1745\n",
      "[Epoch 23/30] Train: 0.1675, Val: 0.1661\n",
      "[Epoch 24/30] Train: 0.1576, Val: 0.1552\n",
      "[Epoch 25/30] Train: 0.1464, Val: 0.1456\n",
      "[Epoch 26/30] Train: 0.1370, Val: 0.1368\n",
      "[Epoch 27/30] Train: 0.1310, Val: 0.1318\n",
      "[Epoch 28/30] Train: 0.1273, Val: 0.1290\n",
      "[Epoch 29/30] Train: 0.1252, Val: 0.1275\n",
      "[Epoch 30/30] Train: 0.1238, Val: 0.1262\n",
      "Best Val Loss: 0.1262 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.1262\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850200_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850200_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0395, True=0.0577\n",
      "  Sample 1: Pred=0.1441, True=0.6128\n",
      "  Sample 2: Pred=0.0525, True=0.0985\n",
      "  Sample 3: Pred=0.1156, True=1.1945\n",
      "  Sample 4: Pred=0.0195, True=0.0140\n",
      "[INFO] Test MSE: 0.961120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.017 MB of 0.017 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.037 MB of 0.037 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.037 MB of 0.037 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.037 MB of 0.037 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▄▄▃▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▄▃▃▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.96112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.12377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.12624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850200\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/0yx98s0a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_162320-0yx98s0a/logs\u001b[0m\n",
      "[INFO 04-28 16:23:44] ax.service.managed_loop: Running optimization trial 13...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning: Failed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n",
      "  best_X, best_acq_val = generate_starting_points(\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_162347-eceunzsz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850227\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/eceunzsz\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.0933, Val: 0.0771\n",
      "[Epoch 2/30] Train: 0.0628, Val: 0.0641\n",
      "[Epoch 3/30] Train: 0.0560, Val: 0.0529\n",
      "[Epoch 4/30] Train: 0.0518, Val: 0.0521\n",
      "[Epoch 5/30] Train: 0.0498, Val: 0.0549\n",
      "[Epoch 6/30] Train: 0.0483, Val: 0.0510\n",
      "[Epoch 7/30] Train: 0.0464, Val: 0.0460\n",
      "[Epoch 8/30] Train: 0.0460, Val: 0.0466\n",
      "[Epoch 9/30] Train: 0.0453, Val: 0.0457\n",
      "[Epoch 10/30] Train: 0.0443, Val: 0.0450\n",
      "[Epoch 11/30] Train: 0.0431, Val: 0.0465\n",
      "[Epoch 12/30] Train: 0.0437, Val: 0.0432\n",
      "[Epoch 13/30] Train: 0.0425, Val: 0.0453\n",
      "[Epoch 14/30] Train: 0.0429, Val: 0.0470\n",
      "[Epoch 15/30] Train: 0.0417, Val: 0.0492\n",
      "[Epoch 16/30] Train: 0.0419, Val: 0.0443\n",
      "[Epoch 17/30] Train: 0.0414, Val: 0.0431\n",
      "[Epoch 18/30] Train: 0.0418, Val: 0.0450\n",
      "[Epoch 19/30] Train: 0.0417, Val: 0.0441\n",
      "[Epoch 20/30] Train: 0.0411, Val: 0.0454\n",
      "[Epoch 21/30] Train: 0.0411, Val: 0.0432\n",
      "[Epoch 22/30] Train: 0.0416, Val: 0.0429\n",
      "[Epoch 23/30] Train: 0.0402, Val: 0.0440\n",
      "[Epoch 24/30] Train: 0.0405, Val: 0.0437\n",
      "[Epoch 25/30] Train: 0.0404, Val: 0.0522\n",
      "[Epoch 26/30] Train: 0.0404, Val: 0.0464\n",
      "[Epoch 27/30] Train: 0.0402, Val: 0.0418\n",
      "[Epoch 28/30] Train: 0.0402, Val: 0.0443\n",
      "[Epoch 29/30] Train: 0.0402, Val: 0.0452\n",
      "[Epoch 30/30] Train: 0.0403, Val: 0.0426\n",
      "Best Val Loss: 0.0418 at epoch 27\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0418\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850227_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850227_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0226, True=0.0577\n",
      "  Sample 1: Pred=0.3823, True=0.6128\n",
      "  Sample 2: Pred=0.0737, True=0.0985\n",
      "  Sample 3: Pred=0.5492, True=1.1945\n",
      "  Sample 4: Pred=0.0193, True=0.0140\n",
      "[INFO] Test MSE: 0.219121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.154 MB of 0.154 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▃▃▄▃▂▂▂▂▂▁▂▂▂▂▁▂▁▂▁▁▁▁▃▂▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.21912\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.04035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.0426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850227\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/eceunzsz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_162347-eceunzsz/logs\u001b[0m\n",
      "[INFO 04-28 16:25:59] ax.service.managed_loop: Running optimization trial 14...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning: Failed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 0\n",
      "  best_X, best_acq_val = generate_starting_points(\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_162603-s3e87e4t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850363\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/s3e87e4t\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.2139, Val: 0.2135\n",
      "[Epoch 2/30] Train: 0.2084, Val: 0.2071\n",
      "[Epoch 3/30] Train: 0.2009, Val: 0.1993\n",
      "[Epoch 4/30] Train: 0.1924, Val: 0.1908\n",
      "[Epoch 5/30] Train: 0.1837, Val: 0.1820\n",
      "[Epoch 6/30] Train: 0.1750, Val: 0.1738\n",
      "[Epoch 7/30] Train: 0.1672, Val: 0.1664\n",
      "[Epoch 8/30] Train: 0.1604, Val: 0.1598\n",
      "[Epoch 9/30] Train: 0.1541, Val: 0.1535\n",
      "[Epoch 10/30] Train: 0.1482, Val: 0.1473\n",
      "[Epoch 11/30] Train: 0.1424, Val: 0.1412\n",
      "[Epoch 12/30] Train: 0.1368, Val: 0.1353\n",
      "[Epoch 13/30] Train: 0.1315, Val: 0.1300\n",
      "[Epoch 14/30] Train: 0.1267, Val: 0.1251\n",
      "[Epoch 15/30] Train: 0.1223, Val: 0.1206\n",
      "[Epoch 16/30] Train: 0.1183, Val: 0.1167\n",
      "[Epoch 17/30] Train: 0.1148, Val: 0.1133\n",
      "[Epoch 18/30] Train: 0.1117, Val: 0.1104\n",
      "[Epoch 19/30] Train: 0.1089, Val: 0.1074\n",
      "[Epoch 20/30] Train: 0.1063, Val: 0.1049\n",
      "[Epoch 21/30] Train: 0.1039, Val: 0.1026\n",
      "[Epoch 22/30] Train: 0.1016, Val: 0.1001\n",
      "[Epoch 23/30] Train: 0.0996, Val: 0.0982\n",
      "[Epoch 24/30] Train: 0.0977, Val: 0.0965\n",
      "[Epoch 25/30] Train: 0.0959, Val: 0.0946\n",
      "[Epoch 26/30] Train: 0.0943, Val: 0.0930\n",
      "[Epoch 27/30] Train: 0.0927, Val: 0.0914\n",
      "[Epoch 28/30] Train: 0.0913, Val: 0.0902\n",
      "[Epoch 29/30] Train: 0.0900, Val: 0.0888\n",
      "[Epoch 30/30] Train: 0.0887, Val: 0.0875\n",
      "Best Val Loss: 0.0875 at epoch 30\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0875\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850363_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850363_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0366, True=0.0577\n",
      "  Sample 1: Pred=0.0722, True=0.6128\n",
      "  Sample 2: Pred=0.0706, True=0.0985\n",
      "  Sample 3: Pred=0.2634, True=1.1945\n",
      "  Sample 4: Pred=0.0164, True=0.0140\n",
      "[INFO] Test MSE: 0.834065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.054 MB of 0.054 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.054 MB of 0.054 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.054 MB of 0.054 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ██▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.83406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.08867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.0875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850363\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/s3e87e4t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_162603-s3e87e4t/logs\u001b[0m\n",
      "[INFO 04-28 16:28:06] ax.service.managed_loop: Running optimization trial 15...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning: Failed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n",
      "  best_X, best_acq_val = generate_starting_points(\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_162810-q96uxea1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850490\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/q96uxea1\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1044, Val: 0.0700\n",
      "[Epoch 2/30] Train: 0.0612, Val: 0.0570\n",
      "[Epoch 3/30] Train: 0.0523, Val: 0.0526\n",
      "[Epoch 4/30] Train: 0.0473, Val: 0.0473\n",
      "[Epoch 5/30] Train: 0.0435, Val: 0.0528\n",
      "[Epoch 6/30] Train: 0.0411, Val: 0.0474\n",
      "[Epoch 7/30] Train: 0.0387, Val: 0.0434\n",
      "[Epoch 8/30] Train: 0.0373, Val: 0.0441\n",
      "[Epoch 9/30] Train: 0.0361, Val: 0.0417\n",
      "[Epoch 10/30] Train: 0.0346, Val: 0.0411\n",
      "[Epoch 11/30] Train: 0.0329, Val: 0.0420\n",
      "[Epoch 12/30] Train: 0.0328, Val: 0.0416\n",
      "[Epoch 13/30] Train: 0.0316, Val: 0.0390\n",
      "[Epoch 14/30] Train: 0.0302, Val: 0.0413\n",
      "[Epoch 15/30] Train: 0.0296, Val: 0.0415\n",
      "[Epoch 16/30] Train: 0.0290, Val: 0.0413\n",
      "[Epoch 17/30] Train: 0.0280, Val: 0.0400\n",
      "[Epoch 18/30] Train: 0.0280, Val: 0.0389\n",
      "[Epoch 19/30] Train: 0.0264, Val: 0.0402\n",
      "[Epoch 20/30] Train: 0.0258, Val: 0.0393\n",
      "[Epoch 21/30] Train: 0.0254, Val: 0.0388\n",
      "[Epoch 22/30] Train: 0.0252, Val: 0.0393\n",
      "[Epoch 23/30] Train: 0.0243, Val: 0.0435\n",
      "[Epoch 24/30] Train: 0.0234, Val: 0.0408\n",
      "[Epoch 25/30] Train: 0.0233, Val: 0.0432\n",
      "[Epoch 26/30] Train: 0.0223, Val: 0.0392\n",
      "[Epoch 27/30] Train: 0.0219, Val: 0.0396\n",
      "[Epoch 28/30] Train: 0.0214, Val: 0.0425\n",
      "[Epoch 29/30] Train: 0.0211, Val: 0.0412\n",
      "[Epoch 30/30] Train: 0.0205, Val: 0.0402\n",
      "Best Val Loss: 0.0388 at epoch 21\n",
      "[INFO] Best model index=0, seed=1, val_loss=0.0388\n",
      "Pipeline saved to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850490_DcGain.pkl\n",
      "[INFO] Saved pipeline to my_models/final_pipeline_power_electronics__bayes_optimize__DcGain__1__1745850490_DcGain.pkl\n",
      "[INFO] Uploaded model artifact to W&B.\n",
      "[INFO] Test samples (avg ensemble):\n",
      "  Sample 0: Pred=0.0381, True=0.0577\n",
      "  Sample 1: Pred=0.2680, True=0.6128\n",
      "  Sample 2: Pred=0.1096, True=0.0985\n",
      "  Sample 3: Pred=0.6170, True=1.1945\n",
      "  Sample 4: Pred=0.0135, True=0.0140\n",
      "[INFO] Test MSE: 0.133973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.154 MB of 0.154 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.155 MB of 0.155 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▃▄▃▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▂▁▂▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       seed 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_mse 0.13397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.02051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 0.04024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850490\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/q96uxea1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250428_162810-q96uxea1/logs\u001b[0m\n",
      "[INFO 04-28 16:30:23] ax.service.managed_loop: Running optimization trial 16...\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning: Failed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n",
      "  best_X, best_acq_val = generate_starting_points(\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1588: RuntimeWarning: If using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n",
      "  return obj.__class__(**changes)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/wandb/run-20250428_163027-ntjf1qzo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpower_electronics__bayes_optimize__DcGain__1__1745850627\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/engibench/engiopt/runs/ntjf1qzo\u001b[0m\n",
      "[INFO] Using device: mps\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "[DataPreprocessor] Applied log-transform to DcGain\n",
      "=== Training model for seed=1 ===\n",
      "[Epoch 1/30] Train: 0.1265, Val: 0.0809\n",
      "[Epoch 2/30] Train: 0.0744, Val: 0.0707\n",
      "[Epoch 3/30] Train: 0.0644, Val: 0.0623\n",
      "[Epoch 4/30] Train: 0.0586, Val: 0.0593\n",
      "[Epoch 5/30] Train: 0.0553, Val: 0.0564\n",
      "[Epoch 6/30] Train: 0.0527, Val: 0.0553\n",
      "[Epoch 7/30] Train: 0.0512, Val: 0.0520\n",
      "[Epoch 8/30] Train: 0.0494, Val: 0.0551\n",
      "[Epoch 9/30] Train: 0.0480, Val: 0.0513\n",
      "[Epoch 10/30] Train: 0.0465, Val: 0.0499\n",
      "[Epoch 11/30] Train: 0.0457, Val: 0.0495\n",
      "[Epoch 12/30] Train: 0.0448, Val: 0.0483\n",
      "[Epoch 13/30] Train: 0.0439, Val: 0.0470\n",
      "[Epoch 14/30] Train: 0.0436, Val: 0.0461\n",
      "[Epoch 15/30] Train: 0.0429, Val: 0.0469\n",
      "[Epoch 16/30] Train: 0.0421, Val: 0.0461\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/bayes_optimize.py\", line 219, in <module>\n",
      "    optimise(cli_args)\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/bayes_optimize.py\", line 199, in optimise\n",
      "    best_params, best_vals, experiment, _ = optimize(\n",
      "                                            ^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/managed_loop.py\", line 305, in optimize\n",
      "    loop.full_run()\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/managed_loop.py\", line 237, in full_run\n",
      "    self.run_trial()\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/utils/common/executils.py\", line 169, in actual_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/managed_loop.py\", line 215, in run_trial\n",
      "    arm.name: self._call_evaluation_function(arm.parameters, weight)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/ax/service/managed_loop.py\", line 153, in _call_evaluation_function\n",
      "    evaluation = self.evaluation_function(parameterization)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/bayes_optimize.py\", line 201, in <lambda>\n",
      "    evaluation_function=lambda hp: _train_and_eval(hp, opt_args),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/bayes_optimize.py\", line 151, in _train_and_eval\n",
      "    best_val = train_main(train_args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/mlp_tabular_only.py\", line 323, in main\n",
      "    ensemble_models, ensemble_val_losses, seeds = train_ensemble(args, train_loader, val_loader, device)\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/mlp_tabular_only.py\", line 192, in train_ensemble\n",
      "    model_i, best_val_loss_i = train_one_model(args, train_loader, val_loader, device=device)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/engiopt/surrogate_model/training_utils.py\", line 273, in train_one_model\n",
      "    loss.backward()\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python bayes_optimize.py \\\n",
    "    --problem_id \"power_electronics\" \\\n",
    "    --target_col \"DcGain\" \\\n",
    "    --log_target \\\n",
    "    --params_cols '[\"initial_design_0\",\"initial_design_1\",\"initial_design_2\",\"initial_design_3\",\"initial_design_4\",\"initial_design_5\",\"initial_design_6\",\"initial_design_7\",\"initial_design_8\",\"initial_design_9\"]' \\\n",
    "    --flatten_columns '[\"initial_design\"]' \\\n",
    "    --n_epochs 30 \\\n",
    "    --patience 20 \\\n",
    "    --seed 1 \\\n",
    "    --track \\\n",
    "    --wandb_project \"engiopt\" \\\n",
    "    --wandb_entity \"engibench\" \\\n",
    "    --n_ensembles 1 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model \\\n",
    "    --device \"mps\" \\\n",
    "    --total_trials 50 \\\n",
    "    --learning_rate_bounds 1e-5 1e-3 \\\n",
    "    --hidden_layers_choices 2 3 4 5 \\\n",
    "    --hidden_size_choices 16 32 64 128 256 \\\n",
    "    --batch_size_choices 8 16 32 64 128 \\\n",
    "    --l2_lambda_bounds 1e-6 1e-3 \\\n",
    "    --activation_choices \"relu\" \"tanh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with ax platform and botorch in backend\n",
    "\n",
    "## Output: r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/ffelte/Documents/EngiLearn/.venv/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mValue error\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;91mError parsing --seed\u001b[0m\u001b[91m:\u001b[0m invalid literal for int() with base 10: 'seed'         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m────────────────────────────────────────────────────────────────────────────\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m Argument helptext:                                                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m     --seed \u001b[1mINT\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m     \u001b[2mRandom seed.\u001b[0m \u001b[36m(default: 42)\u001b[0m                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m────────────────────────────────────────────────────────────────────────────\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m For full helptext, see \u001b[1mbayes_optimize.py --help\u001b[0m                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python bayes_optimize.py \\\n",
    "    --problem_id \"power_electronics\" \\\n",
    "    --target_col \"Voltage_Ripple\" \\\n",
    "    --log_target \\\n",
    "    --params_cols '[\"initial_design_0\",\"initial_design_1\",\"initial_design_2\",\"initial_design_3\",\"initial_design_4\",\"initial_design_5\",\"initial_design_6\",\"initial_design_7\",\"initial_design_8\",\"initial_design_9\"]' \\\n",
    "    --flatten_columns '[\"initial_design\"]' \\\n",
    "    --n_epochs 30 \\\n",
    "    --patience 20 \\\n",
    "    --seed 1 \\\n",
    "    --track \\\n",
    "    --wandb_project \"engiopt\" \\\n",
    "    --wandb_entity \"engibench\" \\\n",
    "    --n_ensembles 1 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model \\\n",
    "    --device \"mps\" \\\n",
    "    --total_trials 50 \\\n",
    "    --learning_rate_bounds 1e-5 1e-3 \\\n",
    "    --hidden_layers_choices 2 3 4 5 \\\n",
    "    --hidden_size_choices 16 32 64 128 256 \\\n",
    "    --batch_size_choices 8 16 32 64 128 \\\n",
    "    --l2_lambda_bounds 1e-6 1e-3 \\\n",
    "    --activation_choices \"relu\" \"tanh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mlp_tabular_only.py \\\n",
    "    --problem_id \"power_electronics\" \\\n",
    "    --target_col \"DcGain\" \\\n",
    "    --log_target \\\n",
    "    --params_cols '[\"initial_design_0\",\"initial_design_1\",\"initial_design_2\",\"initial_design_3\",\"initial_design_4\",\"initial_design_5\",\"initial_design_6\",\"initial_design_7\",\"initial_design_8\",\"initial_design_9\"]' \\\n",
    "    --l2_lambda 1e-6 \\\n",
    "    --flatten_columns '[\"initial_design\"]' \\\n",
    "    --learning_rate 4e-4 \\\n",
    "    --lr_decay 0.95 \\\n",
    "    --activation \"relu\" \\\n",
    "    --hidden_layers 5 \\\n",
    "    --hidden_size 128 \\\n",
    "    --n_epochs 30 \\\n",
    "    --batch_size 16 \\\n",
    "    --patience 20 \\\n",
    "    --scale_target \\\n",
    "    --track \\\n",
    "    --wandb_project \"engiopt\" \\\n",
    "    --wandb_entity \"engibench\" \\\n",
    "    --seed 1 \\\n",
    "    --n_ensembles 2 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model \\\n",
    "    --device \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Creation to predict g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters : {'learning_rate': 0.0005352461015612833, 'hidden_layers': 5, 'hidden_size': 256, 'batch_size': 8, 'l2_lambda': 1e-06, 'activation': 'relu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mlp_tabular_only.py \\\n",
    "    --problem_id \"power_electronics\" \\\n",
    "    --target_col \"DcGain\" \\\n",
    "    --log_target \\\n",
    "    --params_cols '[\"initial_design_0\",\"initial_design_1\",\"initial_design_2\",\"initial_design_3\",\"initial_design_4\",\"initial_design_5\",\"initial_design_6\",\"initial_design_7\",\"initial_design_8\",\"initial_design_9\"]' \\\n",
    "    --l2_lambda 1e-6 \\\n",
    "    --flatten_columns '[\"initial_design\"]' \\\n",
    "    --learning_rate 0.0005352461015612833 \\\n",
    "    --lr_decay 0.95 \\\n",
    "    --activation \"relu\" \\\n",
    "    --hidden_layers 5 \\\n",
    "    --hidden_size 256 \\\n",
    "    --n_epochs 150 \\\n",
    "    --batch_size 8 \\\n",
    "    --patience 50 \\\n",
    "    --scale_target \\\n",
    "    --track \\\n",
    "    --wandb_project \"engiopt\" \\\n",
    "    --wandb_entity \"engibench\" \\\n",
    "    --seed 1 \\\n",
    "    --n_ensembles 7 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model \\\n",
    "    --device \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Creation to predict r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters : {'learning_rate': 0.00041678932436208844, 'hidden_layers': 4, 'hidden_size': 256, 'batch_size': 32, 'l2_lambda': 0.001, 'activation': 'relu'}\n",
    "Metric val : ({'objective': 0.0892840633613089}, {'objective': {'objective': 2.446301998255898e-07}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mlp_tabular_only.py \\\n",
    "    --problem_id \"power_electronics\" \\\n",
    "    --target_col \"Voltage_Ripple\" \\\n",
    "    --log_target \\\n",
    "    --params_cols '[\"initial_design_0\",\"initial_design_1\",\"initial_design_2\",\"initial_design_3\",\"initial_design_4\",\"initial_design_5\",\"initial_design_6\",\"initial_design_7\",\"initial_design_8\",\"initial_design_9\"]' \\\n",
    "    --l2_lambda 1e-3 \\\n",
    "    --flatten_columns '[\"initial_design\"]' \\\n",
    "    --learning_rate 0.00041678932436208844 \\\n",
    "    --lr_decay 0.95 \\\n",
    "    --activation \"relu\" \\\n",
    "    --hidden_layers 4 \\\n",
    "    --hidden_size 256 \\\n",
    "    --n_epochs 150 \\\n",
    "    --batch_size 32 \\\n",
    "    --patience 50 \\\n",
    "    --scale_target \\\n",
    "    --track \\\n",
    "    --wandb_project \"engiopt\" \\\n",
    "    --wandb_entity \"engibench\" \\\n",
    "    --seed 1 \\\n",
    "    --n_ensembles 7 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model \\\n",
    "    --device \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running inference on one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import engiopt.surrogate_model.model_pipeline\n",
    "from engiopt.surrogate_model.model_pipeline import ModelPipeline\n",
    "\n",
    "sys.modules[\"model_pipeline\"] = engiopt.surrogate_model.model_pipeline\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1) Load pipeline\n",
    "pipeline = ModelPipeline.load(\"my_models/final_pipeline_engiopt__mlp_tabular__18__1744568179_Voltage_Ripple.pkl\")\n",
    "\n",
    "# 2) Prepare new raw data in a DataFrame\n",
    "raw_data = df_test\n",
    "\n",
    "# 3) Predict\n",
    "# Since the pipeline now expects raw input, simply pass the DataFrame.\n",
    "y_pred = pipeline.predict(raw_data, batch_size=64, device=device)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# 4) Evaluate (if you have ground-truth values)\n",
    "# The evaluate method also expects raw data.\n",
    "y_true = raw_data[\" Voltage_Ripple\"]  # For example, substitute with your actual ground-truth array\n",
    "print(\"Truth:\", y_true)\n",
    "metrics = pipeline.evaluate(raw_data, y_true, batch_size=64, device=device, metrics=[\"mse\", \"rmse\", \"rel_err\", \"mae\"])\n",
    "print(\"Evaluation metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pymoo optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_pe_optimization.py \\\n",
    "  --model_gain_path  f\"{wandb_entity}/{wandb_project}/power_electronics__mlp_tabular__18__1745680073_model:latest\" \\\n",
    "  --model_ripple_path f\"{wandb_entity}/{wandb_project}/power_electronics__mlp_tabular__18__1745682222_model:latest\" \\\n",
    "  --device \"mps\" \\\n",
    "  --pop_size 500 \\\n",
    "  --n_gen 100 \\\n",
    "  --seed 1 \\\n",
    "  --track \\\n",
    "  --wandb_entity \"engibench\" \\\n",
    "  --wandb_project \"engiopt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Pareto front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Load the combined Pareto front CSV (or use your existing DataFrame)\n",
    "df_front = pd.read_csv(\"results/pareto_front.csv\")\n",
    "\n",
    "# Create a scatter plot:\n",
    "#   - x-axis: predicted r (objective f_r)\n",
    "#   - y-axis: absolute deviation |g-0.25| (objective f_abs_g_minus_0.25)\n",
    "#   - hover_data: shows the design variables (C1...T1)\n",
    "fig = px.scatter(\n",
    "    df_front,\n",
    "    x=\"f0\",\n",
    "    y=\"f1\",\n",
    "    hover_data=[\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\"],\n",
    "    title=\"Pareto Front of Power Electronics Design Optimization\",\n",
    "    labels={\"f0\": \"|DcGain - 0.25|\", \"f1\": \"Voltage_Ripple\"},\n",
    ")\n",
    "\n",
    "# Optional: improve layout and add interactivity\n",
    "fig.update_traces(marker={\"size\": 10, \"color\": \"red\", \"opacity\": 0.8})\n",
    "fig.update_layout(hovermode=\"closest\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMD \n",
    "### Computation between surrogate model Pareto front and its recomputed objective values with baseline problem.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engibench.problems.power_electronics import PowerElectronics\n",
    "from hyppo.ksample import MMD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# 1) Load your surrogate-computed Pareto front (has x0…x9, f0, f1)\n",
    "df_front = pd.read_csv(\"results/pareto_front.csv\")\n",
    "dvars = [f\"x{i}\" for i in range(10)]\n",
    "df_X = df_front[dvars]  # noqa: N816\n",
    "\n",
    "# constant terms for simulation\n",
    "const_terms = np.array([1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n",
    "\n",
    "# 2) Simulate each design (with error-catching) and track failures\n",
    "problem = PowerElectronics()\n",
    "\n",
    "\n",
    "def simulate_objectives(x_row):\n",
    "    \"\"\"Return (|DcGain-0.25|, Voltage_Ripple) or (nan,nan) on failure.\"\"\"\n",
    "    try:\n",
    "        sim = problem.simulate(np.hstack([x_row.to_numpy(), const_terms]))\n",
    "        return np.abs(sim[0] - 0.25), sim[1]\n",
    "    except Exception:  # noqa: BLE001\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "f0_sims, f1_sims = [], []\n",
    "for _, row in df_X.iterrows():\n",
    "    f0, f1 = simulate_objectives(row)\n",
    "    f0_sims.append(f0)\n",
    "    f1_sims.append(f1)\n",
    "\n",
    "df_front[\"f0_sim\"] = f0_sims\n",
    "df_front[\"f1_sim\"] = f1_sims\n",
    "\n",
    "# Identify failed rows\n",
    "failed_mask = df_front[[\"f0_sim\", \"f1_sim\"]].isna().any(axis=1)\n",
    "n_failed = failed_mask.sum()\n",
    "failure_rate = n_failed / len(df_front)\n",
    "\n",
    "# Save the failed designs for inspection\n",
    "df_front.loc[failed_mask, [*dvars, \"f0\", \"f1\"]].to_csv(\"results/pareto_failures.csv\", index=False)\n",
    "\n",
    "# 3) Prepare two DataFrames—dropping failures—for plotting & MMD\n",
    "df_valid = df_front.loc[~failed_mask].reset_index(drop=True)\n",
    "\n",
    "df_pred = (\n",
    "    df_valid.rename(columns={\"f0\": \"r\", \"f1\": \"abs_g\"}).assign(kind=\"Surrogate\").loc[:, [*dvars, \"r\", \"abs_g\", \"kind\"]]\n",
    ")\n",
    "df_sim = (\n",
    "    df_valid.rename(columns={\"f0_sim\": \"abs_g\", \"f1_sim\": \"r\"})\n",
    "    .assign(kind=\"Simulated\")\n",
    "    .loc[:, [*dvars, \"r\", \"abs_g\", \"kind\"]]\n",
    ")\n",
    "df_both = pd.concat([df_pred, df_sim], ignore_index=True)\n",
    "\n",
    "# 4) Plot\n",
    "fig = px.scatter(\n",
    "    df_both,\n",
    "    x=\"r\",\n",
    "    y=\"abs_g\",\n",
    "    color=\"kind\",\n",
    "    hover_data=dvars,\n",
    "    title=\"Surrogate vs Simulated Pareto Front\",\n",
    "    labels={\"r\": \"|DcGain - 0.25|\", \"abs_g\": \"Voltage_Ripple\"},\n",
    ")\n",
    "fig.update_traces(marker={\"size\": 9, \"opacity\": 0.8})\n",
    "fig.update_layout(legend={\"x\": 0.02, \"y\": 0.98}, hovermode=\"closest\")\n",
    "fig.add_annotation(\n",
    "    text=f\"Simulation failure rate: {failure_rate:.1%} ({n_failed} simulations failed)\",\n",
    "    xref=\"paper\",\n",
    "    yref=\"paper\",\n",
    "    x=0.5,\n",
    "    y=1.05,\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 5) Two-sample MMD² test on valid designs only\n",
    "X = df_pred[[\"r\", \"abs_g\"]].to_numpy()\n",
    "Y = df_sim[[\"r\", \"abs_g\"]].to_numpy()\n",
    "\n",
    "mmd = MMD(compute_kernel=\"rbf\", bias=False)\n",
    "stat, p_val = mmd.test(X, Y, reps=1000, auto=False)\n",
    "\n",
    "print(f\"\\nMMD² = {stat:.4e}, permutation p-value = {p_val:.3f}\")\n",
    "if p_val < 0.05:  # noqa: PLR2004\n",
    "    print(\"→ Clouds differ significantly (reject H₀ at α=0.05)\")  # noqa: RUF001\n",
    "else:\n",
    "    print(\"→ No significant difference detected (fail to reject H₀)\")\n",
    "\n",
    "# 6) Save enriched Pareto front (with sim results and failure flags)\n",
    "df_front.to_csv(\"results/pareto_front_with_sim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
