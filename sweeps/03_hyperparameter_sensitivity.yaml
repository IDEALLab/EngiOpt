program: engiopt/lvae_2d/lvae_2d.py
method: bayes
name: 03_hyperparameter_sensitivity
description: >
  Bayesian optimization to understand hyperparameter sensitivity.
  Focuses on architecture and training dynamics, not pruning specifics.

  Research Questions:
  1. What is the sensitivity to learning rate and batch size?
  2. How does latent dimension affect pruning behavior?
  3. When should pruning start (pruning_epoch)?
  4. What is the optimal volume loss weight?

metric:
  name: val_rec
  goal: minimize

parameters:
  # Problem selection
  problem_id:
    values: ["beams2d", "heatconduction2d", "photonics2d"]

  # Fixed pruning setup (use best from sweep 01)
  pruning_strategy:
    value: "plummet"
  plummet_threshold:
    value: 0.02

  # Moderate safeguards
  min_active_dims:
    value: 5
  max_prune_per_epoch:
    value: 20
  cooldown_epochs:
    value: 10
  k_consecutive:
    value: 3
  recon_tol:
    value: 0.10

  # Architecture hyperparameters (main focus)
  latent_dim:
    distribution: int_uniform
    min: 100
    max: 500

  # Training hyperparameters
  lr:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.001

  batch_size:
    values: [64, 128, 256]

  # Volume loss weight (critical for pruning)
  w_v:
    distribution: log_uniform_values
    min: 0.001
    max: 0.1

  # Pruning timing
  pruning_epoch:
    distribution: int_uniform
    min: 200
    max: 1000

  # Pruning momentum
  beta:
    distribution: uniform
    min: 0.8
    max: 0.99

  # Volume smoothing
  eta:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.001

  # Schedule parameters
  polynomial_schedule_n:
    distribution: int_uniform
    min: 50
    max: 200

  polynomial_schedule_p:
    values: [1, 2, 3, 4]

  # Fixed
  n_epochs:
    value: 2500

  seed:
    values: [1, 2, 3]

# Bayesian optimization settings
early_terminate:
  type: hyperband
  min_iter: 500
  eta: 3

command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
