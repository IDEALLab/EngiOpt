{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96ee2a7-dc47-4661-9c5b-aa725d2b6fe9",
   "metadata": {},
   "source": [
    "# Demonstration of surrogate model capabilities\n",
    "\n",
    "This is a simple jupyter notebook to demonstrate the autogluon surrogate model capability.\n",
    "\n",
    "Author: Soheyl Massoudi <smassoudi@ethz.ch>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c7a2c",
   "metadata": {},
   "source": [
    "# Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f8295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2001e45",
   "metadata": {},
   "source": [
    "# Step 2: Load and Convert Parquet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2c037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/engibench/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./data/airfoil_data.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory and file path\n",
    "csv_directory = \"./data\"\n",
    "csv_file_name = \"airfoil_data.csv\"\n",
    "csv_path = os.path.join(csv_directory, csv_file_name)\n",
    "absolute_csv_path = os.path.abspath(csv_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(csv_directory, exist_ok=True)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"IDEALLab/airfoil_2d_v0\", split=\"train\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(absolute_csv_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f88b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initial',\n",
       " 'optimized',\n",
       " 'mach',\n",
       " 'reynolds',\n",
       " 'cl_target',\n",
       " 'area_target',\n",
       " 'alpha',\n",
       " 'area_initial',\n",
       " 'cd_val',\n",
       " 'cl_val',\n",
       " 'cl_con',\n",
       " 'area_con']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7fcff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84491ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000194365938981,\n",
       " 0.9897566593945221,\n",
       " 0.9794544269870155,\n",
       " 0.9691149136515305,\n",
       " 0.9587402901643374,\n",
       " 0.9483327419340516,\n",
       " 0.9378945048481954,\n",
       " 0.9274277968572291,\n",
       " 0.9169348081262216,\n",
       " 0.9064178020370062,\n",
       " 0.8958790284346422,\n",
       " 0.8853207187330372,\n",
       " 0.8747448930482233,\n",
       " 0.8641523083004454,\n",
       " 0.8535437159737151,\n",
       " 0.8429198967977035,\n",
       " 0.8322817488654743,\n",
       " 0.8216301200165563,\n",
       " 0.8109658171290856,\n",
       " 0.8002897285258235,\n",
       " 0.7896027290487984,\n",
       " 0.7789056461533759,\n",
       " 0.7681993584263407,\n",
       " 0.7574847603720507,\n",
       " 0.7467626918304485,\n",
       " 0.7360340279427758,\n",
       " 0.7253037272659675,\n",
       " 0.7145732743519277,\n",
       " 0.7038473442473052,\n",
       " 0.693126502752131,\n",
       " 0.6824101435303259,\n",
       " 0.6716976408464019,\n",
       " 0.6609883248618159,\n",
       " 0.6502815861483213,\n",
       " 0.639576822934429,\n",
       " 0.6288733614389102,\n",
       " 0.6181705789840656,\n",
       " 0.6074655520675161,\n",
       " 0.5967516390819786,\n",
       " 0.5860288162351749,\n",
       " 0.5752975145222803,\n",
       " 0.5645582221285628,\n",
       " 0.5538114139478776,\n",
       " 0.5430575344024076,\n",
       " 0.5322970595788166,\n",
       " 0.5215304770239737,\n",
       " 0.5107583657552618,\n",
       " 0.49998899952531595,\n",
       " 0.48922838434100185,\n",
       " 0.4784765533366514,\n",
       " 0.4677335678615218,\n",
       " 0.45699934751237103,\n",
       " 0.4462733015736579,\n",
       " 0.4355373234767379,\n",
       " 0.4247826022819237,\n",
       " 0.41400930053745666,\n",
       " 0.40321743727109993,\n",
       " 0.39241068759564895,\n",
       " 0.381612066370135,\n",
       " 0.37080496447631633,\n",
       " 0.3600064410541813,\n",
       " 0.34922406851115434,\n",
       " 0.33845788337211447,\n",
       " 0.3277078356554862,\n",
       " 0.3169698841651151,\n",
       " 0.3062390461963021,\n",
       " 0.2955166628909653,\n",
       " 0.28480410301254583,\n",
       " 0.27410278009006916,\n",
       " 0.26341204683290753,\n",
       " 0.25272635368206947,\n",
       " 0.24204543660455097,\n",
       " 0.23136953646457306,\n",
       " 0.22069888051414568,\n",
       " 0.21003374815071363,\n",
       " 0.19937439529743484,\n",
       " 0.18872105828155553,\n",
       " 0.17807401105266413,\n",
       " 0.1674384042435018,\n",
       " 0.1568354841489629,\n",
       " 0.14627346816198103,\n",
       " 0.1357582215448607,\n",
       " 0.12529567044554485,\n",
       " 0.11489170336632272,\n",
       " 0.10455021525169125,\n",
       " 0.09426863823556426,\n",
       " 0.08405208228345475,\n",
       " 0.07390691353511289,\n",
       " 0.06383946888448681,\n",
       " 0.05385641687455061,\n",
       " 0.04402472145292385,\n",
       " 0.03444850734177523,\n",
       " 0.02519366900396886,\n",
       " 0.01641768092335681,\n",
       " 0.008394408450421985,\n",
       " 0.0018741554333653653,\n",
       " 2.2970567507988958e-05,\n",
       " 0.0010121165398447852,\n",
       " 0.006109611193112725,\n",
       " 0.01370514074439058,\n",
       " 0.02313370007675729,\n",
       " 0.033273072998687155,\n",
       " 0.043554159817668955,\n",
       " 0.05401079260060604,\n",
       " 0.06465236042520224,\n",
       " 0.07535526340628695,\n",
       " 0.08608920394286107,\n",
       " 0.09683862156249405,\n",
       " 0.1075640834160009,\n",
       " 0.11827990320504907,\n",
       " 0.12900109801263504,\n",
       " 0.1397275413876923,\n",
       " 0.15045912393206448,\n",
       " 0.16119570367181466,\n",
       " 0.1719371470350259,\n",
       " 0.18268333777319265,\n",
       " 0.19343414035525783,\n",
       " 0.20418941445045344,\n",
       " 0.21494903817324487,\n",
       " 0.22571288181702087,\n",
       " 0.23648079949722617,\n",
       " 0.24725266486144776,\n",
       " 0.2580283532532397,\n",
       " 0.26880878299795696,\n",
       " 0.27960090914862235,\n",
       " 0.29039876325877434,\n",
       " 0.30118517119208765,\n",
       " 0.31195849941390125,\n",
       " 0.3227212182905306,\n",
       " 0.33350459745566546,\n",
       " 0.3442818886937216,\n",
       " 0.35502739882219564,\n",
       " 0.3657429098807812,\n",
       " 0.3764298962845432,\n",
       " 0.38711445637963526,\n",
       " 0.39783322997139103,\n",
       " 0.40858605488135663,\n",
       " 0.4193721426334202,\n",
       " 0.43017830900415494,\n",
       " 0.4409573305010514,\n",
       " 0.45172790663824214,\n",
       " 0.46250517390586726,\n",
       " 0.47328951502343525,\n",
       " 0.48408111554715966,\n",
       " 0.49487490872098133,\n",
       " 0.5056685294183247,\n",
       " 0.5164619191878449,\n",
       " 0.5272550185700763,\n",
       " 0.538047769807962,\n",
       " 0.5488401145514649,\n",
       " 0.5596319934391053,\n",
       " 0.5704233485439291,\n",
       " 0.5812141216204684,\n",
       " 0.5920042534086314,\n",
       " 0.602793685807008,\n",
       " 0.6135840245366975,\n",
       " 0.6243805262405263,\n",
       " 0.6351834079411556,\n",
       " 0.6459835600989463,\n",
       " 0.65677669647181,\n",
       " 0.6675630645869804,\n",
       " 0.6783429348178385,\n",
       " 0.6891165056072681,\n",
       " 0.6998840054941735,\n",
       " 0.7106457134805344,\n",
       " 0.721401900718687,\n",
       " 0.7321555266379772,\n",
       " 0.742908177734797,\n",
       " 0.753659171989869,\n",
       " 0.7644078091358975,\n",
       " 0.7751534268769158,\n",
       " 0.7858953478579311,\n",
       " 0.7966328823502482,\n",
       " 0.8073653580322585,\n",
       " 0.8180921087583235,\n",
       " 0.8288124571452764,\n",
       " 0.8395257205105193,\n",
       " 0.8502312457088192,\n",
       " 0.8609283655624806,\n",
       " 0.8716163987619134,\n",
       " 0.8822961330805695,\n",
       " 0.892977083823789,\n",
       " 0.9036621294802024,\n",
       " 0.9143521874171552,\n",
       " 0.9250480828921622,\n",
       " 0.9357507389102209,\n",
       " 0.9464611457979726,\n",
       " 0.9571801200884288,\n",
       " 0.9679029829972787,\n",
       " 0.978615104744405,\n",
       " 0.9893163870696579,\n",
       " 1.0000077388872795]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['optimized'][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1631d22",
   "metadata": {},
   "source": [
    "# Step 3: Flatten columns and get X and Y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6feeae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_flatten(val):\n",
    "    \"\"\"\n",
    "    Recursively flatten a nested list/tuple into a single list of values.\n",
    "    E.g. [[x0, x1], [y0, y1]] -> [x0, x1, y0, y1].\n",
    "    \"\"\"\n",
    "    if not isinstance(val, (list, tuple)):\n",
    "        return [val]\n",
    "    else:\n",
    "        result = []\n",
    "        for item in val:\n",
    "            result.extend(recursive_flatten(item))\n",
    "        return result\n",
    "\n",
    "def flatten_list_columns_nested(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each column in `df` where the first row is a list,\n",
    "    recursively flatten that column and expand into multiple new numeric columns.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    new_cols_list = []\n",
    "    drop_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        first_value = df[col].iloc[0]\n",
    "        # Check if the first row is a list/tuple --> likely all rows are lists\n",
    "        if isinstance(first_value, (list, tuple)):\n",
    "            # Flatten each row, store in a temporary list\n",
    "            flattened_rows = [recursive_flatten(val) for val in df[col]]\n",
    "            # Ensure consistent length across rows\n",
    "            lengths = [len(row) for row in flattened_rows]\n",
    "            unique_lengths = set(lengths)\n",
    "            if len(unique_lengths) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Column '{col}' has rows of varying lengths {unique_lengths}. \"\n",
    "                    \"Cannot flatten consistently without special handling.\"\n",
    "                )\n",
    "            # Create new DataFrame with columns [col_0, col_1, ..., col_(n-1)]\n",
    "            n = lengths[0]\n",
    "            new_col_names = [f\"{col}_{i}\" for i in range(n)]\n",
    "            expanded_df = pd.DataFrame(flattened_rows, columns=new_col_names)\n",
    "            new_cols_list.append(expanded_df)\n",
    "            drop_cols.append(col)\n",
    "    \n",
    "    if new_cols_list:\n",
    "        # Drop original list-columns and concatenate new expanded columns\n",
    "        df = pd.concat(\n",
    "            [df.drop(columns=drop_cols).reset_index(drop=True)] + new_cols_list,\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Flatten the nested list columns\n",
    "df = flatten_list_columns_nested(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0c7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The flattened columns for \"optimized\" will now appear as \"optimized_0\", \"optimized_1\", etc.\n",
    "# Let's find all columns that start with \"optimized_\"\n",
    "opt_cols = [col for col in df.columns if col.startswith(\"optimized_\")]\n",
    "\n",
    "# Or you can do them all if you know the exact count/structure\n",
    "\n",
    "# Additional numeric columns we want\n",
    "other_input_cols = [\"mach\", \"reynolds\", \"alpha\"]\n",
    "\n",
    "# Combine them\n",
    "feature_cols = opt_cols + other_input_cols\n",
    "\n",
    "target_col = \"cl_val\"\n",
    "\n",
    "# Create X and y\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db970419",
   "metadata": {},
   "source": [
    "# Step 4: Split Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753e902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (448, 387), Val size: (150, 387), Test size: (150, 387)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppose X is shape [n_samples, n_features], y is shape [n_samples]\n",
    "# (Replace with your actual arrays from the flattened dataset.)\n",
    "\n",
    "# 1) Split off test data (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2) Split the remaining into train and validation (25% of X_temp = 20% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Val size: {X_val.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Optionally scale target as well\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a81a60",
   "metadata": {},
   "source": [
    "# Step 5: Define a Pytorch Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86f6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/100 - Train Loss: 0.7779, Val Loss: 0.6491\n",
      "Epoch 2/100 - Train Loss: 0.5426, Val Loss: 0.5571\n",
      "Epoch 3/100 - Train Loss: 0.4203, Val Loss: 0.4517\n",
      "Epoch 4/100 - Train Loss: 0.3566, Val Loss: 0.5126\n",
      "Epoch 5/100 - Train Loss: 0.2885, Val Loss: 0.3740\n",
      "Epoch 6/100 - Train Loss: 0.2311, Val Loss: 0.3203\n",
      "Epoch 7/100 - Train Loss: 0.2069, Val Loss: 0.3497\n",
      "Epoch 8/100 - Train Loss: 0.1813, Val Loss: 0.2814\n",
      "Epoch 9/100 - Train Loss: 0.1509, Val Loss: 0.2585\n",
      "Epoch 10/100 - Train Loss: 0.1446, Val Loss: 0.2606\n",
      "Epoch 11/100 - Train Loss: 0.1486, Val Loss: 0.2190\n",
      "Epoch 12/100 - Train Loss: 0.1236, Val Loss: 0.1765\n",
      "Epoch 13/100 - Train Loss: 0.1032, Val Loss: 0.2197\n",
      "Epoch 14/100 - Train Loss: 0.1139, Val Loss: 0.1706\n",
      "Epoch 15/100 - Train Loss: 0.0833, Val Loss: 0.1646\n",
      "Epoch 16/100 - Train Loss: 0.0799, Val Loss: 0.1505\n",
      "Epoch 17/100 - Train Loss: 0.0682, Val Loss: 0.1327\n",
      "Epoch 18/100 - Train Loss: 0.0685, Val Loss: 0.1336\n",
      "Epoch 19/100 - Train Loss: 0.0633, Val Loss: 0.1169\n",
      "Epoch 20/100 - Train Loss: 0.0549, Val Loss: 0.1110\n",
      "Epoch 21/100 - Train Loss: 0.0546, Val Loss: 0.1419\n",
      "Epoch 22/100 - Train Loss: 0.0667, Val Loss: 0.1434\n",
      "Epoch 23/100 - Train Loss: 0.0700, Val Loss: 0.1050\n",
      "Epoch 24/100 - Train Loss: 0.0441, Val Loss: 0.0900\n",
      "Epoch 25/100 - Train Loss: 0.0361, Val Loss: 0.1094\n",
      "Epoch 26/100 - Train Loss: 0.0391, Val Loss: 0.0928\n",
      "Epoch 27/100 - Train Loss: 0.0317, Val Loss: 0.0807\n",
      "Epoch 28/100 - Train Loss: 0.0405, Val Loss: 0.1051\n",
      "Epoch 29/100 - Train Loss: 0.0358, Val Loss: 0.0872\n",
      "Epoch 30/100 - Train Loss: 0.0278, Val Loss: 0.0788\n",
      "Epoch 31/100 - Train Loss: 0.0272, Val Loss: 0.0788\n",
      "Epoch 32/100 - Train Loss: 0.0232, Val Loss: 0.0762\n",
      "Epoch 33/100 - Train Loss: 0.0213, Val Loss: 0.0702\n",
      "Epoch 34/100 - Train Loss: 0.0191, Val Loss: 0.0701\n",
      "Epoch 35/100 - Train Loss: 0.0166, Val Loss: 0.0643\n",
      "Epoch 36/100 - Train Loss: 0.0165, Val Loss: 0.0657\n",
      "Epoch 37/100 - Train Loss: 0.0164, Val Loss: 0.0651\n",
      "Epoch 38/100 - Train Loss: 0.0169, Val Loss: 0.0697\n",
      "Epoch 39/100 - Train Loss: 0.0191, Val Loss: 0.0681\n",
      "Epoch 40/100 - Train Loss: 0.0218, Val Loss: 0.0757\n",
      "Epoch 41/100 - Train Loss: 0.0194, Val Loss: 0.0580\n",
      "Epoch 42/100 - Train Loss: 0.0285, Val Loss: 0.0841\n",
      "Epoch 43/100 - Train Loss: 0.0489, Val Loss: 0.0778\n",
      "Epoch 44/100 - Train Loss: 0.0446, Val Loss: 0.0890\n",
      "Epoch 45/100 - Train Loss: 0.0344, Val Loss: 0.0591\n",
      "Epoch 46/100 - Train Loss: 0.0262, Val Loss: 0.0795\n",
      "Epoch 47/100 - Train Loss: 0.0281, Val Loss: 0.0860\n",
      "Epoch 48/100 - Train Loss: 0.0721, Val Loss: 0.1671\n",
      "Epoch 49/100 - Train Loss: 0.0676, Val Loss: 0.1139\n",
      "Epoch 50/100 - Train Loss: 0.0605, Val Loss: 0.0747\n",
      "Epoch 51/100 - Train Loss: 0.0468, Val Loss: 0.0756\n",
      "Validation loss has not improved for 10 epochs. Stopping early.\n",
      "Best validation loss = 0.0580 at epoch 41.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TabularDataset(X_train_scaled, y_train_scaled)\n",
    "val_dataset   = TabularDataset(X_val_scaled,   y_val_scaled)\n",
    "test_dataset  = TabularDataset(X_test_scaled,  y_test_scaled)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(SimpleRegressor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]  # Number of features\n",
    "hidden_dim = 64\n",
    "model = SimpleRegressor(input_dim, hidden_dim, output_dim=1)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10  # Early stopping patience\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_model_weights = None\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch).squeeze(-1)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            predictions = model(X_batch).squeeze(-1)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            running_val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # Check for improvement\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "        # Store best weights\n",
    "        best_model_weights = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "          f\"- Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping condition\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Validation loss has not improved for {patience} epochs. Stopping early.\")\n",
    "        break\n",
    "\n",
    "# --- After training, restore the best weights ---\n",
    "print(f\"Best validation loss = {best_val_loss:.4f} at epoch {best_epoch+1}.\")\n",
    "if best_model_weights is not None:\n",
    "    model.load_state_dict(best_model_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cde5eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bf969",
   "metadata": {},
   "source": [
    "# Step 6: Predictions and Inverse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f9083b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  0.0031\n",
      "Test RMSE: 0.0561\n",
      "\n",
      "Sample of predictions vs true values:\n",
      "Predicted: 0.7420,  True: 0.7862,  Diff: -0.0443\n",
      "Predicted: 0.9102,  True: 0.9167,  Diff: -0.0065\n",
      "Predicted: 0.7486,  True: 0.7405,  Diff: 0.0082\n",
      "Predicted: 0.8529,  True: 0.8399,  Diff: 0.0130\n",
      "Predicted: 0.8227,  True: 0.8537,  Diff: -0.0310\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_preds_scaled = []\n",
    "test_targets_scaled = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        preds = model(X_batch).squeeze(-1)\n",
    "\n",
    "        test_preds_scaled.append(preds.cpu().numpy())\n",
    "        test_targets_scaled.append(y_batch.cpu().numpy())\n",
    "\n",
    "test_preds_scaled = np.concatenate(test_preds_scaled)\n",
    "test_targets_scaled = np.concatenate(test_targets_scaled)\n",
    "\n",
    "# Inverse transform if you scaled the target\n",
    "test_preds = scaler_y.inverse_transform(test_preds_scaled.reshape(-1, 1)).flatten()\n",
    "test_targets = scaler_y.inverse_transform(test_targets_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the final test MSE or RMSE\n",
    "mse_test = np.mean((test_preds - test_targets)**2)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print(f\"Test MSE:  {mse_test:.4f}\")\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# Display a small sample of true vs predicted\n",
    "print(\"\\nSample of predictions vs true values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {test_preds[i]:.4f},  True: {test_targets[i]:.4f},  Diff: {test_preds[i] - test_targets[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac8e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5273daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/engibench/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 748/748 [00:00<00:00, 25495.22 examples/s]\n",
      "Generating val split: 100%|██████████| 140/140 [00:00<00:00, 40948.57 examples/s]\n",
      "Generating test split: 100%|██████████| 47/47 [00:00<00:00, 21561.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./data/airfoil_data.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory and file path\n",
    "csv_directory = \"./data\"\n",
    "csv_file_name = \"airfoil_data.csv\"\n",
    "csv_path = os.path.join(csv_directory, csv_file_name)\n",
    "absolute_csv_path = os.path.abspath(csv_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(csv_directory, exist_ok=True)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"IDEALLab/airfoil_2d_v0\", split=\"train\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(absolute_csv_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved to {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41550d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         C1        C2        C3        C4        C5        C6        L1  \\\n",
      "0  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "1  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "2  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "3  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "4  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "\n",
      "         L2        L3   T1         g         r          e  \n",
      "0  0.000001  0.000001  0.1  0.440126  0.914354  -3.903844  \n",
      "1  0.000001  0.000001  0.2  0.410832  1.152453  -9.022484  \n",
      "2  0.000001  0.000001  0.3  0.353193  1.521469 -14.144002  \n",
      "3  0.000001  0.000001  0.4  0.256083  2.190335 -19.268402  \n",
      "4  0.000001  0.000001  0.5  0.147918  3.205281 -24.395419  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Hugging Face's raw CSV URL\n",
    "csv_url = \"https://huggingface.co/datasets/IDEALLab/power_electronics_v0/resolve/main/dataset_v0_1.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Save it locally if needed\n",
    "df.to_csv(\"./data/power_electronics_v0_1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da9982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"initial_design\"][0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4c1fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of df['initial_design'][0]: <class 'list'>\n",
      "Type of first element (x coords): <class 'list'>\n",
      "Type of second element (y coords): <class 'list'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the first entry for initial and optimal designs\n",
    "init_design = df[\"initial_design\"].iloc[0]\n",
    "opt_design  = df[\"optimal_design\"].iloc[0]\n",
    "\n",
    "# Verify the structure:\n",
    "print(\"Type of df['initial_design'][0]:\", type(init_design))\n",
    "print(\"Type of first element (x coords):\", type(init_design[0]))\n",
    "print(\"Type of second element (y coords):\", type(init_design[1]))\n",
    "\n",
    "# Pair x and y coordinates for initial and optimal designs\n",
    "init_coords = np.column_stack((init_design[0], init_design[1]))\n",
    "opt_coords  = np.column_stack((opt_design[0], opt_design[1]))\n",
    "\n",
    "# Plot the designs\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(init_coords[:,0], init_coords[:,1], 'bo-', label=\"Initial Airfoil\")\n",
    "plt.plot(opt_coords[:,0], opt_coords[:,1], 'go-', label=\"Optimal Airfoil\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Airfoil Design Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002c330",
   "metadata": {},
   "source": [
    "## Step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23110d90-27af-4110-aecf-15c70a6bf312",
   "metadata": {},
   "source": [
    "## Step 2: Load surrogate model from engiopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4462f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initial_design',\n",
       " 'optimal_design',\n",
       " 'mach',\n",
       " 'reynolds',\n",
       " 'cl_target',\n",
       " 'area_target',\n",
       " 'alpha',\n",
       " 'area_initial',\n",
       " 'cd_val',\n",
       " 'cl_val',\n",
       " 'cl_con',\n",
       " 'area_con']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb5a8571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         C1        C2        C3        C4        C5        C6        L1  \\\n",
      "0  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "1  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "2  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "3  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "4  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
      "\n",
      "         L2        L3   T1         g         r          e  \n",
      "0  0.000001  0.000001  0.1  0.440126  0.914354  -3.903844  \n",
      "1  0.000001  0.000001  0.2  0.410832  1.152453  -9.022484  \n",
      "2  0.000001  0.000001  0.3  0.353193  1.521469 -14.144002  \n",
      "3  0.000001  0.000001  0.4  0.256083  2.190335 -19.268402  \n",
      "4  0.000001  0.000001  0.5  0.147918  3.205281 -24.395419  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0e47215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Warning: Column '' not in DataFrame, skipping flatten.\n",
      "After flattening, df.columns: ['C1', ' C2', ' C3', ' C4', ' C5', ' C6', ' L1', ' L2', ' L3', ' T1', ' g', ' r', ' e']\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/smassoudi/Library/CloudStorage/OneDrive-ETHZurich/Code_Collaborative/EngiOpt/./engiopt/shape2shape_leastV_vae.py\", line 786, in <module>\n",
      "    main(args)\n",
      "  File \"/Users/smassoudi/Library/CloudStorage/OneDrive-ETHZurich/Code_Collaborative/EngiOpt/./engiopt/shape2shape_leastV_vae.py\", line 360, in main\n",
      "    raise ValueError(f\"Missing required target column: {args.target_col}\")\n",
      "ValueError: Missing required target column: g\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python ./engiopt/shape2shape_leastV_vae.py \\\n",
    "    --data_dir \"../EngiOpt/data\" \\\n",
    "    --data_input \"power_electronics_v0_1.csv\" \\\n",
    "    --init_col \"C1\" \\\n",
    "    --opt_col \"C1\" \\\n",
    "    --target_col \"g\" \\\n",
    "    --params_cols '[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"L1\",\"L2\",\"L3\",\"T1\"]' \\\n",
    "    --flatten_columns '[\"\"]' \\\n",
    "    --lambda_lv 1e-2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --gamma 1.0 \\\n",
    "    --no-structured \\\n",
    "    --hidden_layers 3 \\\n",
    "    --hidden_size 32 \\\n",
    "    --latent_dim 8 \\\n",
    "    --n_epochs 4000 \\\n",
    "    --batch_size 32 \\\n",
    "    --patience 500 \\\n",
    "    --scale_target \\\n",
    "    --no-track \\\n",
    "    --seed 18 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95eb27a2-25c1-49db-8494-bda712d135db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "After flattening, df.columns: ['mach', 'reynolds', 'cl_target', 'area_target', 'alpha', 'area_initial', 'cd_val', 'cl_val', 'cl_con', 'area_con', 'initial_design_0', 'initial_design_1', 'initial_design_2', 'initial_design_3', 'initial_design_4', 'initial_design_5', 'initial_design_6', 'initial_design_7', 'initial_design_8', 'initial_design_9', 'initial_design_10', 'initial_design_11', 'initial_design_12', 'initial_design_13', 'initial_design_14', 'initial_design_15', 'initial_design_16', 'initial_design_17', 'initial_design_18', 'initial_design_19', 'initial_design_20', 'initial_design_21', 'initial_design_22', 'initial_design_23', 'initial_design_24', 'initial_design_25', 'initial_design_26', 'initial_design_27', 'initial_design_28', 'initial_design_29', 'initial_design_30', 'initial_design_31', 'initial_design_32', 'initial_design_33', 'initial_design_34', 'initial_design_35', 'initial_design_36', 'initial_design_37', 'initial_design_38', 'initial_design_39', 'initial_design_40', 'initial_design_41', 'initial_design_42', 'initial_design_43', 'initial_design_44', 'initial_design_45', 'initial_design_46', 'initial_design_47', 'initial_design_48', 'initial_design_49', 'initial_design_50', 'initial_design_51', 'initial_design_52', 'initial_design_53', 'initial_design_54', 'initial_design_55', 'initial_design_56', 'initial_design_57', 'initial_design_58', 'initial_design_59', 'initial_design_60', 'initial_design_61', 'initial_design_62', 'initial_design_63', 'initial_design_64', 'initial_design_65', 'initial_design_66', 'initial_design_67', 'initial_design_68', 'initial_design_69', 'initial_design_70', 'initial_design_71', 'initial_design_72', 'initial_design_73', 'initial_design_74', 'initial_design_75', 'initial_design_76', 'initial_design_77', 'initial_design_78', 'initial_design_79', 'initial_design_80', 'initial_design_81', 'initial_design_82', 'initial_design_83', 'initial_design_84', 'initial_design_85', 'initial_design_86', 'initial_design_87', 'initial_design_88', 'initial_design_89', 'initial_design_90', 'initial_design_91', 'initial_design_92', 'initial_design_93', 'initial_design_94', 'initial_design_95', 'initial_design_96', 'initial_design_97', 'initial_design_98', 'initial_design_99', 'initial_design_100', 'initial_design_101', 'initial_design_102', 'initial_design_103', 'initial_design_104', 'initial_design_105', 'initial_design_106', 'initial_design_107', 'initial_design_108', 'initial_design_109', 'initial_design_110', 'initial_design_111', 'initial_design_112', 'initial_design_113', 'initial_design_114', 'initial_design_115', 'initial_design_116', 'initial_design_117', 'initial_design_118', 'initial_design_119', 'initial_design_120', 'initial_design_121', 'initial_design_122', 'initial_design_123', 'initial_design_124', 'initial_design_125', 'initial_design_126', 'initial_design_127', 'initial_design_128', 'initial_design_129', 'initial_design_130', 'initial_design_131', 'initial_design_132', 'initial_design_133', 'initial_design_134', 'initial_design_135', 'initial_design_136', 'initial_design_137', 'initial_design_138', 'initial_design_139', 'initial_design_140', 'initial_design_141', 'initial_design_142', 'initial_design_143', 'initial_design_144', 'initial_design_145', 'initial_design_146', 'initial_design_147', 'initial_design_148', 'initial_design_149', 'initial_design_150', 'initial_design_151', 'initial_design_152', 'initial_design_153', 'initial_design_154', 'initial_design_155', 'initial_design_156', 'initial_design_157', 'initial_design_158', 'initial_design_159', 'initial_design_160', 'initial_design_161', 'initial_design_162', 'initial_design_163', 'initial_design_164', 'initial_design_165', 'initial_design_166', 'initial_design_167', 'initial_design_168', 'initial_design_169', 'initial_design_170', 'initial_design_171', 'initial_design_172', 'initial_design_173', 'initial_design_174', 'initial_design_175', 'initial_design_176', 'initial_design_177', 'initial_design_178', 'initial_design_179', 'initial_design_180', 'initial_design_181', 'initial_design_182', 'initial_design_183', 'initial_design_184', 'initial_design_185', 'initial_design_186', 'initial_design_187', 'initial_design_188', 'initial_design_189', 'initial_design_190', 'initial_design_191', 'initial_design_192', 'initial_design_193', 'initial_design_194', 'initial_design_195', 'initial_design_196', 'initial_design_197', 'initial_design_198', 'initial_design_199', 'initial_design_200', 'initial_design_201', 'initial_design_202', 'initial_design_203', 'initial_design_204', 'initial_design_205', 'initial_design_206', 'initial_design_207', 'initial_design_208', 'initial_design_209', 'initial_design_210', 'initial_design_211', 'initial_design_212', 'initial_design_213', 'initial_design_214', 'initial_design_215', 'initial_design_216', 'initial_design_217', 'initial_design_218', 'initial_design_219', 'initial_design_220', 'initial_design_221', 'initial_design_222', 'initial_design_223', 'initial_design_224', 'initial_design_225', 'initial_design_226', 'initial_design_227', 'initial_design_228', 'initial_design_229', 'initial_design_230', 'initial_design_231', 'initial_design_232', 'initial_design_233', 'initial_design_234', 'initial_design_235', 'initial_design_236', 'initial_design_237', 'initial_design_238', 'initial_design_239', 'initial_design_240', 'initial_design_241', 'initial_design_242', 'initial_design_243', 'initial_design_244', 'initial_design_245', 'initial_design_246', 'initial_design_247', 'initial_design_248', 'initial_design_249', 'initial_design_250', 'initial_design_251', 'initial_design_252', 'initial_design_253', 'initial_design_254', 'initial_design_255', 'initial_design_256', 'initial_design_257', 'initial_design_258', 'initial_design_259', 'initial_design_260', 'initial_design_261', 'initial_design_262', 'initial_design_263', 'initial_design_264', 'initial_design_265', 'initial_design_266', 'initial_design_267', 'initial_design_268', 'initial_design_269', 'initial_design_270', 'initial_design_271', 'initial_design_272', 'initial_design_273', 'initial_design_274', 'initial_design_275', 'initial_design_276', 'initial_design_277', 'initial_design_278', 'initial_design_279', 'initial_design_280', 'initial_design_281', 'initial_design_282', 'initial_design_283', 'initial_design_284', 'initial_design_285', 'initial_design_286', 'initial_design_287', 'initial_design_288', 'initial_design_289', 'initial_design_290', 'initial_design_291', 'initial_design_292', 'initial_design_293', 'initial_design_294', 'initial_design_295', 'initial_design_296', 'initial_design_297', 'initial_design_298', 'initial_design_299', 'initial_design_300', 'initial_design_301', 'initial_design_302', 'initial_design_303', 'initial_design_304', 'initial_design_305', 'initial_design_306', 'initial_design_307', 'initial_design_308', 'initial_design_309', 'initial_design_310', 'initial_design_311', 'initial_design_312', 'initial_design_313', 'initial_design_314', 'initial_design_315', 'initial_design_316', 'initial_design_317', 'initial_design_318', 'initial_design_319', 'initial_design_320', 'initial_design_321', 'initial_design_322', 'initial_design_323', 'initial_design_324', 'initial_design_325', 'initial_design_326', 'initial_design_327', 'initial_design_328', 'initial_design_329', 'initial_design_330', 'initial_design_331', 'initial_design_332', 'initial_design_333', 'initial_design_334', 'initial_design_335', 'initial_design_336', 'initial_design_337', 'initial_design_338', 'initial_design_339', 'initial_design_340', 'initial_design_341', 'initial_design_342', 'initial_design_343', 'initial_design_344', 'initial_design_345', 'initial_design_346', 'initial_design_347', 'initial_design_348', 'initial_design_349', 'initial_design_350', 'initial_design_351', 'initial_design_352', 'initial_design_353', 'initial_design_354', 'initial_design_355', 'initial_design_356', 'initial_design_357', 'initial_design_358', 'initial_design_359', 'initial_design_360', 'initial_design_361', 'initial_design_362', 'initial_design_363', 'initial_design_364', 'initial_design_365', 'initial_design_366', 'initial_design_367', 'initial_design_368', 'initial_design_369', 'initial_design_370', 'initial_design_371', 'initial_design_372', 'initial_design_373', 'initial_design_374', 'initial_design_375', 'initial_design_376', 'initial_design_377', 'initial_design_378', 'initial_design_379', 'initial_design_380', 'initial_design_381', 'initial_design_382', 'initial_design_383', 'optimal_design_0', 'optimal_design_1', 'optimal_design_2', 'optimal_design_3', 'optimal_design_4', 'optimal_design_5', 'optimal_design_6', 'optimal_design_7', 'optimal_design_8', 'optimal_design_9', 'optimal_design_10', 'optimal_design_11', 'optimal_design_12', 'optimal_design_13', 'optimal_design_14', 'optimal_design_15', 'optimal_design_16', 'optimal_design_17', 'optimal_design_18', 'optimal_design_19', 'optimal_design_20', 'optimal_design_21', 'optimal_design_22', 'optimal_design_23', 'optimal_design_24', 'optimal_design_25', 'optimal_design_26', 'optimal_design_27', 'optimal_design_28', 'optimal_design_29', 'optimal_design_30', 'optimal_design_31', 'optimal_design_32', 'optimal_design_33', 'optimal_design_34', 'optimal_design_35', 'optimal_design_36', 'optimal_design_37', 'optimal_design_38', 'optimal_design_39', 'optimal_design_40', 'optimal_design_41', 'optimal_design_42', 'optimal_design_43', 'optimal_design_44', 'optimal_design_45', 'optimal_design_46', 'optimal_design_47', 'optimal_design_48', 'optimal_design_49', 'optimal_design_50', 'optimal_design_51', 'optimal_design_52', 'optimal_design_53', 'optimal_design_54', 'optimal_design_55', 'optimal_design_56', 'optimal_design_57', 'optimal_design_58', 'optimal_design_59', 'optimal_design_60', 'optimal_design_61', 'optimal_design_62', 'optimal_design_63', 'optimal_design_64', 'optimal_design_65', 'optimal_design_66', 'optimal_design_67', 'optimal_design_68', 'optimal_design_69', 'optimal_design_70', 'optimal_design_71', 'optimal_design_72', 'optimal_design_73', 'optimal_design_74', 'optimal_design_75', 'optimal_design_76', 'optimal_design_77', 'optimal_design_78', 'optimal_design_79', 'optimal_design_80', 'optimal_design_81', 'optimal_design_82', 'optimal_design_83', 'optimal_design_84', 'optimal_design_85', 'optimal_design_86', 'optimal_design_87', 'optimal_design_88', 'optimal_design_89', 'optimal_design_90', 'optimal_design_91', 'optimal_design_92', 'optimal_design_93', 'optimal_design_94', 'optimal_design_95', 'optimal_design_96', 'optimal_design_97', 'optimal_design_98', 'optimal_design_99', 'optimal_design_100', 'optimal_design_101', 'optimal_design_102', 'optimal_design_103', 'optimal_design_104', 'optimal_design_105', 'optimal_design_106', 'optimal_design_107', 'optimal_design_108', 'optimal_design_109', 'optimal_design_110', 'optimal_design_111', 'optimal_design_112', 'optimal_design_113', 'optimal_design_114', 'optimal_design_115', 'optimal_design_116', 'optimal_design_117', 'optimal_design_118', 'optimal_design_119', 'optimal_design_120', 'optimal_design_121', 'optimal_design_122', 'optimal_design_123', 'optimal_design_124', 'optimal_design_125', 'optimal_design_126', 'optimal_design_127', 'optimal_design_128', 'optimal_design_129', 'optimal_design_130', 'optimal_design_131', 'optimal_design_132', 'optimal_design_133', 'optimal_design_134', 'optimal_design_135', 'optimal_design_136', 'optimal_design_137', 'optimal_design_138', 'optimal_design_139', 'optimal_design_140', 'optimal_design_141', 'optimal_design_142', 'optimal_design_143', 'optimal_design_144', 'optimal_design_145', 'optimal_design_146', 'optimal_design_147', 'optimal_design_148', 'optimal_design_149', 'optimal_design_150', 'optimal_design_151', 'optimal_design_152', 'optimal_design_153', 'optimal_design_154', 'optimal_design_155', 'optimal_design_156', 'optimal_design_157', 'optimal_design_158', 'optimal_design_159', 'optimal_design_160', 'optimal_design_161', 'optimal_design_162', 'optimal_design_163', 'optimal_design_164', 'optimal_design_165', 'optimal_design_166', 'optimal_design_167', 'optimal_design_168', 'optimal_design_169', 'optimal_design_170', 'optimal_design_171', 'optimal_design_172', 'optimal_design_173', 'optimal_design_174', 'optimal_design_175', 'optimal_design_176', 'optimal_design_177', 'optimal_design_178', 'optimal_design_179', 'optimal_design_180', 'optimal_design_181', 'optimal_design_182', 'optimal_design_183', 'optimal_design_184', 'optimal_design_185', 'optimal_design_186', 'optimal_design_187', 'optimal_design_188', 'optimal_design_189', 'optimal_design_190', 'optimal_design_191', 'optimal_design_192', 'optimal_design_193', 'optimal_design_194', 'optimal_design_195', 'optimal_design_196', 'optimal_design_197', 'optimal_design_198', 'optimal_design_199', 'optimal_design_200', 'optimal_design_201', 'optimal_design_202', 'optimal_design_203', 'optimal_design_204', 'optimal_design_205', 'optimal_design_206', 'optimal_design_207', 'optimal_design_208', 'optimal_design_209', 'optimal_design_210', 'optimal_design_211', 'optimal_design_212', 'optimal_design_213', 'optimal_design_214', 'optimal_design_215', 'optimal_design_216', 'optimal_design_217', 'optimal_design_218', 'optimal_design_219', 'optimal_design_220', 'optimal_design_221', 'optimal_design_222', 'optimal_design_223', 'optimal_design_224', 'optimal_design_225', 'optimal_design_226', 'optimal_design_227', 'optimal_design_228', 'optimal_design_229', 'optimal_design_230', 'optimal_design_231', 'optimal_design_232', 'optimal_design_233', 'optimal_design_234', 'optimal_design_235', 'optimal_design_236', 'optimal_design_237', 'optimal_design_238', 'optimal_design_239', 'optimal_design_240', 'optimal_design_241', 'optimal_design_242', 'optimal_design_243', 'optimal_design_244', 'optimal_design_245', 'optimal_design_246', 'optimal_design_247', 'optimal_design_248', 'optimal_design_249', 'optimal_design_250', 'optimal_design_251', 'optimal_design_252', 'optimal_design_253', 'optimal_design_254', 'optimal_design_255', 'optimal_design_256', 'optimal_design_257', 'optimal_design_258', 'optimal_design_259', 'optimal_design_260', 'optimal_design_261', 'optimal_design_262', 'optimal_design_263', 'optimal_design_264', 'optimal_design_265', 'optimal_design_266', 'optimal_design_267', 'optimal_design_268', 'optimal_design_269', 'optimal_design_270', 'optimal_design_271', 'optimal_design_272', 'optimal_design_273', 'optimal_design_274', 'optimal_design_275', 'optimal_design_276', 'optimal_design_277', 'optimal_design_278', 'optimal_design_279', 'optimal_design_280', 'optimal_design_281', 'optimal_design_282', 'optimal_design_283', 'optimal_design_284', 'optimal_design_285', 'optimal_design_286', 'optimal_design_287', 'optimal_design_288', 'optimal_design_289', 'optimal_design_290', 'optimal_design_291', 'optimal_design_292', 'optimal_design_293', 'optimal_design_294', 'optimal_design_295', 'optimal_design_296', 'optimal_design_297', 'optimal_design_298', 'optimal_design_299', 'optimal_design_300', 'optimal_design_301', 'optimal_design_302', 'optimal_design_303', 'optimal_design_304', 'optimal_design_305', 'optimal_design_306', 'optimal_design_307', 'optimal_design_308', 'optimal_design_309', 'optimal_design_310', 'optimal_design_311', 'optimal_design_312', 'optimal_design_313', 'optimal_design_314', 'optimal_design_315', 'optimal_design_316', 'optimal_design_317', 'optimal_design_318', 'optimal_design_319', 'optimal_design_320', 'optimal_design_321', 'optimal_design_322', 'optimal_design_323', 'optimal_design_324', 'optimal_design_325', 'optimal_design_326', 'optimal_design_327', 'optimal_design_328', 'optimal_design_329', 'optimal_design_330', 'optimal_design_331', 'optimal_design_332', 'optimal_design_333', 'optimal_design_334', 'optimal_design_335', 'optimal_design_336', 'optimal_design_337', 'optimal_design_338', 'optimal_design_339', 'optimal_design_340', 'optimal_design_341', 'optimal_design_342', 'optimal_design_343', 'optimal_design_344', 'optimal_design_345', 'optimal_design_346', 'optimal_design_347', 'optimal_design_348', 'optimal_design_349', 'optimal_design_350', 'optimal_design_351', 'optimal_design_352', 'optimal_design_353', 'optimal_design_354', 'optimal_design_355', 'optimal_design_356', 'optimal_design_357', 'optimal_design_358', 'optimal_design_359', 'optimal_design_360', 'optimal_design_361', 'optimal_design_362', 'optimal_design_363', 'optimal_design_364', 'optimal_design_365', 'optimal_design_366', 'optimal_design_367', 'optimal_design_368', 'optimal_design_369', 'optimal_design_370', 'optimal_design_371', 'optimal_design_372', 'optimal_design_373', 'optimal_design_374', 'optimal_design_375', 'optimal_design_376', 'optimal_design_377', 'optimal_design_378', 'optimal_design_379', 'optimal_design_380', 'optimal_design_381', 'optimal_design_382', 'optimal_design_383']\n",
      "Epoch [1/4000] - Train Loss: 0.8453, Val Loss: 0.9335\n",
      "Epoch [2/4000] - Train Loss: 0.8411, Val Loss: 0.9317\n",
      "Epoch [3/4000] - Train Loss: 0.8395, Val Loss: 0.9275\n",
      "Epoch [4/4000] - Train Loss: 0.8371, Val Loss: 0.9229\n",
      "Epoch [5/4000] - Train Loss: 0.8343, Val Loss: 0.9185\n",
      "Epoch [6/4000] - Train Loss: 0.8259, Val Loss: 0.9049\n",
      "Epoch [7/4000] - Train Loss: 0.8133, Val Loss: 0.8920\n",
      "Epoch [8/4000] - Train Loss: 0.7942, Val Loss: 0.8610\n",
      "Epoch [9/4000] - Train Loss: 0.7745, Val Loss: 0.8242\n",
      "Epoch [10/4000] - Train Loss: 0.7425, Val Loss: 0.7805\n",
      "Epoch [11/4000] - Train Loss: 0.7070, Val Loss: 0.7423\n",
      "Epoch [12/4000] - Train Loss: 0.6862, Val Loss: 0.7023\n",
      "Epoch [13/4000] - Train Loss: 0.6650, Val Loss: 0.6868\n",
      "Epoch [14/4000] - Train Loss: 0.6420, Val Loss: 0.6695\n",
      "Epoch [15/4000] - Train Loss: 0.6205, Val Loss: 0.6505\n",
      "Epoch [16/4000] - Train Loss: 0.6023, Val Loss: 0.6319\n",
      "Epoch [17/4000] - Train Loss: 0.5807, Val Loss: 0.6038\n",
      "Epoch [18/4000] - Train Loss: 0.5596, Val Loss: 0.5845\n",
      "Epoch [19/4000] - Train Loss: 0.5352, Val Loss: 0.5736\n",
      "Epoch [20/4000] - Train Loss: 0.5172, Val Loss: 0.5564\n",
      "Epoch [21/4000] - Train Loss: 0.4960, Val Loss: 0.5378\n",
      "Epoch [22/4000] - Train Loss: 0.4760, Val Loss: 0.5279\n",
      "Epoch [23/4000] - Train Loss: 0.4619, Val Loss: 0.5136\n",
      "Epoch [24/4000] - Train Loss: 0.4458, Val Loss: 0.5069\n",
      "Epoch [25/4000] - Train Loss: 0.4310, Val Loss: 0.4992\n",
      "Epoch [26/4000] - Train Loss: 0.4152, Val Loss: 0.4804\n",
      "Epoch [27/4000] - Train Loss: 0.4019, Val Loss: 0.4714\n",
      "Epoch [28/4000] - Train Loss: 0.3868, Val Loss: 0.4533\n",
      "Epoch [29/4000] - Train Loss: 0.3797, Val Loss: 0.4497\n",
      "Epoch [30/4000] - Train Loss: 0.3631, Val Loss: 0.4352\n",
      "Epoch [31/4000] - Train Loss: 0.3546, Val Loss: 0.4323\n",
      "Epoch [32/4000] - Train Loss: 0.3435, Val Loss: 0.4163\n",
      "Epoch [33/4000] - Train Loss: 0.3269, Val Loss: 0.3992\n",
      "Epoch [34/4000] - Train Loss: 0.3275, Val Loss: 0.4032\n",
      "Epoch [35/4000] - Train Loss: 0.3233, Val Loss: 0.3965\n",
      "Epoch [36/4000] - Train Loss: 0.3077, Val Loss: 0.3790\n",
      "Epoch [37/4000] - Train Loss: 0.2968, Val Loss: 0.3782\n",
      "Epoch [38/4000] - Train Loss: 0.2877, Val Loss: 0.3712\n",
      "Epoch [39/4000] - Train Loss: 0.2802, Val Loss: 0.3699\n",
      "Epoch [40/4000] - Train Loss: 0.2730, Val Loss: 0.3485\n",
      "Epoch [41/4000] - Train Loss: 0.2637, Val Loss: 0.3506\n",
      "Epoch [42/4000] - Train Loss: 0.2561, Val Loss: 0.3445\n",
      "Epoch [43/4000] - Train Loss: 0.2479, Val Loss: 0.3359\n",
      "Epoch [44/4000] - Train Loss: 0.2409, Val Loss: 0.3311\n",
      "Epoch [45/4000] - Train Loss: 0.2371, Val Loss: 0.3287\n",
      "Epoch [46/4000] - Train Loss: 0.2329, Val Loss: 0.3285\n",
      "Epoch [47/4000] - Train Loss: 0.2347, Val Loss: 0.3251\n",
      "Epoch [48/4000] - Train Loss: 0.2273, Val Loss: 0.3244\n",
      "Epoch [49/4000] - Train Loss: 0.2239, Val Loss: 0.3206\n",
      "Epoch [50/4000] - Train Loss: 0.2202, Val Loss: 0.3159\n",
      "Epoch [51/4000] - Train Loss: 0.2217, Val Loss: 0.3172\n",
      "Epoch [52/4000] - Train Loss: 0.2165, Val Loss: 0.3090\n",
      "Epoch [53/4000] - Train Loss: 0.2121, Val Loss: 0.3069\n",
      "Epoch [54/4000] - Train Loss: 0.2083, Val Loss: 0.2940\n",
      "Epoch [55/4000] - Train Loss: 0.2065, Val Loss: 0.2943\n",
      "Epoch [56/4000] - Train Loss: 0.2053, Val Loss: 0.2970\n",
      "Epoch [57/4000] - Train Loss: 0.2047, Val Loss: 0.2971\n",
      "Epoch [58/4000] - Train Loss: 0.2025, Val Loss: 0.2898\n",
      "Epoch [59/4000] - Train Loss: 0.1987, Val Loss: 0.2863\n",
      "Epoch [60/4000] - Train Loss: 0.1965, Val Loss: 0.2821\n",
      "Epoch [61/4000] - Train Loss: 0.1936, Val Loss: 0.2792\n",
      "Epoch [62/4000] - Train Loss: 0.1921, Val Loss: 0.2808\n",
      "Epoch [63/4000] - Train Loss: 0.1873, Val Loss: 0.2751\n",
      "Epoch [64/4000] - Train Loss: 0.1835, Val Loss: 0.2691\n",
      "Epoch [65/4000] - Train Loss: 0.1827, Val Loss: 0.2655\n",
      "Epoch [66/4000] - Train Loss: 0.1842, Val Loss: 0.2689\n",
      "Epoch [67/4000] - Train Loss: 0.1821, Val Loss: 0.2645\n",
      "Epoch [68/4000] - Train Loss: 0.1770, Val Loss: 0.2604\n",
      "Epoch [69/4000] - Train Loss: 0.1736, Val Loss: 0.2570\n",
      "Epoch [70/4000] - Train Loss: 0.1719, Val Loss: 0.2519\n",
      "Epoch [71/4000] - Train Loss: 0.1689, Val Loss: 0.2492\n",
      "Epoch [72/4000] - Train Loss: 0.1675, Val Loss: 0.2468\n",
      "Epoch [73/4000] - Train Loss: 0.1656, Val Loss: 0.2452\n",
      "Epoch [74/4000] - Train Loss: 0.1645, Val Loss: 0.2440\n",
      "Epoch [75/4000] - Train Loss: 0.1609, Val Loss: 0.2398\n",
      "Epoch [76/4000] - Train Loss: 0.1610, Val Loss: 0.2440\n",
      "Epoch [77/4000] - Train Loss: 0.1620, Val Loss: 0.2443\n",
      "Epoch [78/4000] - Train Loss: 0.1585, Val Loss: 0.2343\n",
      "Epoch [79/4000] - Train Loss: 0.1538, Val Loss: 0.2314\n",
      "Epoch [80/4000] - Train Loss: 0.1621, Val Loss: 0.2360\n",
      "Epoch [81/4000] - Train Loss: 0.1542, Val Loss: 0.2265\n",
      "Epoch [82/4000] - Train Loss: 0.1519, Val Loss: 0.2262\n",
      "Epoch [83/4000] - Train Loss: 0.1515, Val Loss: 0.2255\n",
      "Epoch [84/4000] - Train Loss: 0.1488, Val Loss: 0.2211\n",
      "Epoch [85/4000] - Train Loss: 0.1481, Val Loss: 0.2213\n",
      "Epoch [86/4000] - Train Loss: 0.1472, Val Loss: 0.2165\n",
      "Epoch [87/4000] - Train Loss: 0.1440, Val Loss: 0.2153\n",
      "Epoch [88/4000] - Train Loss: 0.1438, Val Loss: 0.2188\n",
      "Epoch [89/4000] - Train Loss: 0.1474, Val Loss: 0.2165\n",
      "Epoch [90/4000] - Train Loss: 0.1431, Val Loss: 0.2144\n",
      "Epoch [91/4000] - Train Loss: 0.1423, Val Loss: 0.2124\n",
      "Epoch [92/4000] - Train Loss: 0.1402, Val Loss: 0.2110\n",
      "Epoch [93/4000] - Train Loss: 0.1397, Val Loss: 0.2108\n",
      "Epoch [94/4000] - Train Loss: 0.1415, Val Loss: 0.2131\n",
      "Epoch [95/4000] - Train Loss: 0.1399, Val Loss: 0.2082\n",
      "Epoch [96/4000] - Train Loss: 0.1370, Val Loss: 0.2072\n",
      "Epoch [97/4000] - Train Loss: 0.1368, Val Loss: 0.2050\n",
      "Epoch [98/4000] - Train Loss: 0.1370, Val Loss: 0.2085\n",
      "Epoch [99/4000] - Train Loss: 0.1351, Val Loss: 0.2039\n",
      "Epoch [100/4000] - Train Loss: 0.1341, Val Loss: 0.2046\n",
      "Epoch [101/4000] - Train Loss: 0.1356, Val Loss: 0.2043\n",
      "Epoch [102/4000] - Train Loss: 0.1337, Val Loss: 0.2048\n",
      "Epoch [103/4000] - Train Loss: 0.1332, Val Loss: 0.2025\n",
      "Epoch [104/4000] - Train Loss: 0.1314, Val Loss: 0.2004\n",
      "Epoch [105/4000] - Train Loss: 0.1316, Val Loss: 0.2022\n",
      "Epoch [106/4000] - Train Loss: 0.1305, Val Loss: 0.2006\n",
      "Epoch [107/4000] - Train Loss: 0.1309, Val Loss: 0.1993\n",
      "Epoch [108/4000] - Train Loss: 0.1325, Val Loss: 0.2032\n",
      "Epoch [109/4000] - Train Loss: 0.1302, Val Loss: 0.1992\n",
      "Epoch [110/4000] - Train Loss: 0.1300, Val Loss: 0.1993\n",
      "Epoch [111/4000] - Train Loss: 0.1300, Val Loss: 0.1995\n",
      "Epoch [112/4000] - Train Loss: 0.1275, Val Loss: 0.1966\n",
      "Epoch [113/4000] - Train Loss: 0.1261, Val Loss: 0.1965\n",
      "Epoch [114/4000] - Train Loss: 0.1252, Val Loss: 0.1959\n",
      "Epoch [115/4000] - Train Loss: 0.1249, Val Loss: 0.1946\n",
      "Epoch [116/4000] - Train Loss: 0.1258, Val Loss: 0.1968\n",
      "Epoch [117/4000] - Train Loss: 0.1253, Val Loss: 0.1953\n",
      "Epoch [118/4000] - Train Loss: 0.1243, Val Loss: 0.1948\n",
      "Epoch [119/4000] - Train Loss: 0.1254, Val Loss: 0.1929\n",
      "Epoch [120/4000] - Train Loss: 0.1251, Val Loss: 0.1939\n",
      "Epoch [121/4000] - Train Loss: 0.1227, Val Loss: 0.1937\n",
      "Epoch [122/4000] - Train Loss: 0.1226, Val Loss: 0.1938\n",
      "Epoch [123/4000] - Train Loss: 0.1238, Val Loss: 0.1941\n",
      "Epoch [124/4000] - Train Loss: 0.1225, Val Loss: 0.1915\n",
      "Epoch [125/4000] - Train Loss: 0.1216, Val Loss: 0.1942\n",
      "Epoch [126/4000] - Train Loss: 0.1220, Val Loss: 0.1905\n",
      "Epoch [127/4000] - Train Loss: 0.1208, Val Loss: 0.1910\n",
      "Epoch [128/4000] - Train Loss: 0.1208, Val Loss: 0.1912\n",
      "Epoch [129/4000] - Train Loss: 0.1207, Val Loss: 0.1909\n",
      "Epoch [130/4000] - Train Loss: 0.1197, Val Loss: 0.1915\n",
      "Epoch [131/4000] - Train Loss: 0.1196, Val Loss: 0.1890\n",
      "Epoch [132/4000] - Train Loss: 0.1200, Val Loss: 0.1904\n",
      "Epoch [133/4000] - Train Loss: 0.1192, Val Loss: 0.1910\n",
      "Epoch [134/4000] - Train Loss: 0.1185, Val Loss: 0.1891\n",
      "Epoch [135/4000] - Train Loss: 0.1173, Val Loss: 0.1870\n",
      "Epoch [136/4000] - Train Loss: 0.1164, Val Loss: 0.1873\n",
      "Epoch [137/4000] - Train Loss: 0.1164, Val Loss: 0.1864\n",
      "Epoch [138/4000] - Train Loss: 0.1166, Val Loss: 0.1863\n",
      "Epoch [139/4000] - Train Loss: 0.1157, Val Loss: 0.1869\n",
      "Epoch [140/4000] - Train Loss: 0.1152, Val Loss: 0.1865\n",
      "Epoch [141/4000] - Train Loss: 0.1144, Val Loss: 0.1856\n",
      "Epoch [142/4000] - Train Loss: 0.1143, Val Loss: 0.1854\n",
      "Epoch [143/4000] - Train Loss: 0.1141, Val Loss: 0.1843\n",
      "Epoch [144/4000] - Train Loss: 0.1138, Val Loss: 0.1849\n",
      "Epoch [145/4000] - Train Loss: 0.1138, Val Loss: 0.1840\n",
      "Epoch [146/4000] - Train Loss: 0.1134, Val Loss: 0.1848\n",
      "Epoch [147/4000] - Train Loss: 0.1142, Val Loss: 0.1848\n",
      "Epoch [148/4000] - Train Loss: 0.1125, Val Loss: 0.1828\n",
      "Epoch [149/4000] - Train Loss: 0.1120, Val Loss: 0.1824\n",
      "Epoch [150/4000] - Train Loss: 0.1119, Val Loss: 0.1826\n",
      "Epoch [151/4000] - Train Loss: 0.1116, Val Loss: 0.1836\n",
      "Epoch [152/4000] - Train Loss: 0.1126, Val Loss: 0.1842\n",
      "Epoch [153/4000] - Train Loss: 0.1119, Val Loss: 0.1837\n",
      "Epoch [154/4000] - Train Loss: 0.1113, Val Loss: 0.1806\n",
      "Epoch [155/4000] - Train Loss: 0.1100, Val Loss: 0.1813\n",
      "Epoch [156/4000] - Train Loss: 0.1092, Val Loss: 0.1797\n",
      "Epoch [157/4000] - Train Loss: 0.1095, Val Loss: 0.1798\n",
      "Epoch [158/4000] - Train Loss: 0.1091, Val Loss: 0.1844\n",
      "Epoch [159/4000] - Train Loss: 0.1093, Val Loss: 0.1790\n",
      "Epoch [160/4000] - Train Loss: 0.1082, Val Loss: 0.1794\n",
      "Epoch [161/4000] - Train Loss: 0.1077, Val Loss: 0.1802\n",
      "Epoch [162/4000] - Train Loss: 0.1076, Val Loss: 0.1788\n",
      "Epoch [163/4000] - Train Loss: 0.1073, Val Loss: 0.1796\n",
      "Epoch [164/4000] - Train Loss: 0.1071, Val Loss: 0.1767\n",
      "Epoch [165/4000] - Train Loss: 0.1070, Val Loss: 0.1791\n",
      "Epoch [166/4000] - Train Loss: 0.1076, Val Loss: 0.1773\n",
      "Epoch [167/4000] - Train Loss: 0.1063, Val Loss: 0.1758\n",
      "Epoch [168/4000] - Train Loss: 0.1051, Val Loss: 0.1756\n",
      "Epoch [169/4000] - Train Loss: 0.1054, Val Loss: 0.1799\n",
      "Epoch [170/4000] - Train Loss: 0.1064, Val Loss: 0.1757\n",
      "Epoch [171/4000] - Train Loss: 0.1047, Val Loss: 0.1768\n",
      "Epoch [172/4000] - Train Loss: 0.1041, Val Loss: 0.1741\n",
      "Epoch [173/4000] - Train Loss: 0.1038, Val Loss: 0.1725\n",
      "Epoch [174/4000] - Train Loss: 0.1038, Val Loss: 0.1729\n",
      "Epoch [175/4000] - Train Loss: 0.1027, Val Loss: 0.1730\n",
      "Epoch [176/4000] - Train Loss: 0.1025, Val Loss: 0.1740\n",
      "Epoch [177/4000] - Train Loss: 0.1024, Val Loss: 0.1708\n",
      "Epoch [178/4000] - Train Loss: 0.1007, Val Loss: 0.1706\n",
      "Epoch [179/4000] - Train Loss: 0.1004, Val Loss: 0.1687\n",
      "Epoch [180/4000] - Train Loss: 0.1021, Val Loss: 0.1726\n",
      "Epoch [181/4000] - Train Loss: 0.1028, Val Loss: 0.1703\n",
      "Epoch [182/4000] - Train Loss: 0.1010, Val Loss: 0.1720\n",
      "Epoch [183/4000] - Train Loss: 0.1012, Val Loss: 0.1710\n",
      "Epoch [184/4000] - Train Loss: 0.1007, Val Loss: 0.1737\n",
      "Epoch [185/4000] - Train Loss: 0.1006, Val Loss: 0.1676\n",
      "Epoch [186/4000] - Train Loss: 0.0991, Val Loss: 0.1680\n",
      "Epoch [187/4000] - Train Loss: 0.0989, Val Loss: 0.1674\n",
      "Epoch [188/4000] - Train Loss: 0.0993, Val Loss: 0.1669\n",
      "Epoch [189/4000] - Train Loss: 0.0986, Val Loss: 0.1776\n",
      "Epoch [190/4000] - Train Loss: 0.0991, Val Loss: 0.1666\n",
      "Epoch [191/4000] - Train Loss: 0.0973, Val Loss: 0.1676\n",
      "Epoch [192/4000] - Train Loss: 0.0969, Val Loss: 0.1651\n",
      "Epoch [193/4000] - Train Loss: 0.0966, Val Loss: 0.1643\n",
      "Epoch [194/4000] - Train Loss: 0.0954, Val Loss: 0.1638\n",
      "Epoch [195/4000] - Train Loss: 0.0948, Val Loss: 0.1636\n",
      "Epoch [196/4000] - Train Loss: 0.0950, Val Loss: 0.1643\n",
      "Epoch [197/4000] - Train Loss: 0.0943, Val Loss: 0.1612\n",
      "Epoch [198/4000] - Train Loss: 0.0943, Val Loss: 0.1614\n",
      "Epoch [199/4000] - Train Loss: 0.0940, Val Loss: 0.1629\n",
      "Epoch [200/4000] - Train Loss: 0.0946, Val Loss: 0.1625\n",
      "Epoch [201/4000] - Train Loss: 0.0951, Val Loss: 0.1626\n",
      "Epoch [202/4000] - Train Loss: 0.0949, Val Loss: 0.1605\n",
      "Epoch [203/4000] - Train Loss: 0.0946, Val Loss: 0.1635\n",
      "Epoch [204/4000] - Train Loss: 0.0930, Val Loss: 0.1594\n",
      "Epoch [205/4000] - Train Loss: 0.0925, Val Loss: 0.1623\n",
      "Epoch [206/4000] - Train Loss: 0.0927, Val Loss: 0.1594\n",
      "Epoch [207/4000] - Train Loss: 0.0921, Val Loss: 0.1595\n",
      "Epoch [208/4000] - Train Loss: 0.0910, Val Loss: 0.1590\n",
      "Epoch [209/4000] - Train Loss: 0.0912, Val Loss: 0.1577\n",
      "Epoch [210/4000] - Train Loss: 0.0908, Val Loss: 0.1586\n",
      "Epoch [211/4000] - Train Loss: 0.0905, Val Loss: 0.1575\n",
      "Epoch [212/4000] - Train Loss: 0.0906, Val Loss: 0.1583\n",
      "Epoch [213/4000] - Train Loss: 0.0899, Val Loss: 0.1554\n",
      "Epoch [214/4000] - Train Loss: 0.0885, Val Loss: 0.1568\n",
      "Epoch [215/4000] - Train Loss: 0.0885, Val Loss: 0.1552\n",
      "Epoch [216/4000] - Train Loss: 0.0890, Val Loss: 0.1569\n",
      "Epoch [217/4000] - Train Loss: 0.0881, Val Loss: 0.1547\n",
      "Epoch [218/4000] - Train Loss: 0.0875, Val Loss: 0.1541\n",
      "Epoch [219/4000] - Train Loss: 0.0871, Val Loss: 0.1544\n",
      "Epoch [220/4000] - Train Loss: 0.0878, Val Loss: 0.1542\n",
      "Epoch [221/4000] - Train Loss: 0.0882, Val Loss: 0.1543\n",
      "Epoch [222/4000] - Train Loss: 0.0873, Val Loss: 0.1554\n",
      "Epoch [223/4000] - Train Loss: 0.0874, Val Loss: 0.1522\n",
      "Epoch [224/4000] - Train Loss: 0.0875, Val Loss: 0.1531\n",
      "Epoch [225/4000] - Train Loss: 0.0866, Val Loss: 0.1578\n",
      "Epoch [226/4000] - Train Loss: 0.0875, Val Loss: 0.1517\n",
      "Epoch [227/4000] - Train Loss: 0.0861, Val Loss: 0.1511\n",
      "Epoch [228/4000] - Train Loss: 0.0853, Val Loss: 0.1505\n",
      "Epoch [229/4000] - Train Loss: 0.0858, Val Loss: 0.1528\n",
      "Epoch [230/4000] - Train Loss: 0.0855, Val Loss: 0.1508\n",
      "Epoch [231/4000] - Train Loss: 0.0852, Val Loss: 0.1502\n",
      "Epoch [232/4000] - Train Loss: 0.0849, Val Loss: 0.1513\n",
      "Epoch [233/4000] - Train Loss: 0.0838, Val Loss: 0.1502\n",
      "Epoch [234/4000] - Train Loss: 0.0842, Val Loss: 0.1497\n",
      "Epoch [235/4000] - Train Loss: 0.0844, Val Loss: 0.1497\n",
      "Epoch [236/4000] - Train Loss: 0.0835, Val Loss: 0.1482\n",
      "Epoch [237/4000] - Train Loss: 0.0838, Val Loss: 0.1486\n",
      "Epoch [238/4000] - Train Loss: 0.0834, Val Loss: 0.1481\n",
      "Epoch [239/4000] - Train Loss: 0.0838, Val Loss: 0.1491\n",
      "Epoch [240/4000] - Train Loss: 0.0830, Val Loss: 0.1477\n",
      "Epoch [241/4000] - Train Loss: 0.0823, Val Loss: 0.1474\n",
      "Epoch [242/4000] - Train Loss: 0.0821, Val Loss: 0.1474\n",
      "Epoch [243/4000] - Train Loss: 0.0821, Val Loss: 0.1481\n",
      "Epoch [244/4000] - Train Loss: 0.0817, Val Loss: 0.1471\n",
      "Epoch [245/4000] - Train Loss: 0.0817, Val Loss: 0.1468\n",
      "Epoch [246/4000] - Train Loss: 0.0815, Val Loss: 0.1453\n",
      "Epoch [247/4000] - Train Loss: 0.0813, Val Loss: 0.1465\n",
      "Epoch [248/4000] - Train Loss: 0.0813, Val Loss: 0.1464\n",
      "Epoch [249/4000] - Train Loss: 0.0812, Val Loss: 0.1453\n",
      "Epoch [250/4000] - Train Loss: 0.0818, Val Loss: 0.1451\n",
      "Epoch [251/4000] - Train Loss: 0.0813, Val Loss: 0.1461\n",
      "Epoch [252/4000] - Train Loss: 0.0809, Val Loss: 0.1474\n",
      "Epoch [253/4000] - Train Loss: 0.0818, Val Loss: 0.1461\n",
      "Epoch [254/4000] - Train Loss: 0.0814, Val Loss: 0.1436\n",
      "Epoch [255/4000] - Train Loss: 0.0800, Val Loss: 0.1448\n",
      "Epoch [256/4000] - Train Loss: 0.0799, Val Loss: 0.1451\n",
      "Epoch [257/4000] - Train Loss: 0.0795, Val Loss: 0.1451\n",
      "Epoch [258/4000] - Train Loss: 0.0795, Val Loss: 0.1434\n",
      "Epoch [259/4000] - Train Loss: 0.0790, Val Loss: 0.1432\n",
      "Epoch [260/4000] - Train Loss: 0.0795, Val Loss: 0.1439\n",
      "Epoch [261/4000] - Train Loss: 0.0792, Val Loss: 0.1455\n",
      "Epoch [262/4000] - Train Loss: 0.0793, Val Loss: 0.1436\n",
      "Epoch [263/4000] - Train Loss: 0.0799, Val Loss: 0.1467\n",
      "Epoch [264/4000] - Train Loss: 0.0801, Val Loss: 0.1423\n",
      "Epoch [265/4000] - Train Loss: 0.0785, Val Loss: 0.1432\n",
      "Epoch [266/4000] - Train Loss: 0.0786, Val Loss: 0.1445\n",
      "Epoch [267/4000] - Train Loss: 0.0786, Val Loss: 0.1453\n",
      "Epoch [268/4000] - Train Loss: 0.0787, Val Loss: 0.1419\n",
      "Epoch [269/4000] - Train Loss: 0.0780, Val Loss: 0.1406\n",
      "Epoch [270/4000] - Train Loss: 0.0775, Val Loss: 0.1406\n",
      "Epoch [271/4000] - Train Loss: 0.0799, Val Loss: 0.1515\n",
      "Epoch [272/4000] - Train Loss: 0.0796, Val Loss: 0.1431\n",
      "Epoch [273/4000] - Train Loss: 0.0786, Val Loss: 0.1423\n",
      "Epoch [274/4000] - Train Loss: 0.0779, Val Loss: 0.1408\n",
      "Epoch [275/4000] - Train Loss: 0.0778, Val Loss: 0.1419\n",
      "Epoch [276/4000] - Train Loss: 0.0772, Val Loss: 0.1428\n",
      "Epoch [277/4000] - Train Loss: 0.0774, Val Loss: 0.1415\n",
      "Epoch [278/4000] - Train Loss: 0.0768, Val Loss: 0.1416\n",
      "Epoch [279/4000] - Train Loss: 0.0761, Val Loss: 0.1411\n",
      "Epoch [280/4000] - Train Loss: 0.0757, Val Loss: 0.1391\n",
      "Epoch [281/4000] - Train Loss: 0.0755, Val Loss: 0.1410\n",
      "Epoch [282/4000] - Train Loss: 0.0758, Val Loss: 0.1416\n",
      "Epoch [283/4000] - Train Loss: 0.0764, Val Loss: 0.1412\n",
      "Epoch [284/4000] - Train Loss: 0.0762, Val Loss: 0.1405\n",
      "Epoch [285/4000] - Train Loss: 0.0762, Val Loss: 0.1416\n",
      "Epoch [286/4000] - Train Loss: 0.0759, Val Loss: 0.1404\n",
      "Epoch [287/4000] - Train Loss: 0.0758, Val Loss: 0.1392\n",
      "Epoch [288/4000] - Train Loss: 0.0757, Val Loss: 0.1409\n",
      "Epoch [289/4000] - Train Loss: 0.0773, Val Loss: 0.1398\n",
      "Epoch [290/4000] - Train Loss: 0.0758, Val Loss: 0.1398\n",
      "Epoch [291/4000] - Train Loss: 0.0761, Val Loss: 0.1419\n",
      "Epoch [292/4000] - Train Loss: 0.0760, Val Loss: 0.1406\n",
      "Epoch [293/4000] - Train Loss: 0.0750, Val Loss: 0.1392\n",
      "Epoch [294/4000] - Train Loss: 0.0747, Val Loss: 0.1405\n",
      "Epoch [295/4000] - Train Loss: 0.0749, Val Loss: 0.1403\n",
      "Epoch [296/4000] - Train Loss: 0.0752, Val Loss: 0.1389\n",
      "Epoch [297/4000] - Train Loss: 0.0750, Val Loss: 0.1393\n",
      "Epoch [298/4000] - Train Loss: 0.0745, Val Loss: 0.1401\n",
      "Epoch [299/4000] - Train Loss: 0.0747, Val Loss: 0.1421\n",
      "Epoch [300/4000] - Train Loss: 0.0750, Val Loss: 0.1400\n",
      "Epoch [301/4000] - Train Loss: 0.0749, Val Loss: 0.1400\n",
      "Epoch [302/4000] - Train Loss: 0.0756, Val Loss: 0.1384\n",
      "Epoch [303/4000] - Train Loss: 0.0755, Val Loss: 0.1397\n",
      "Epoch [304/4000] - Train Loss: 0.0744, Val Loss: 0.1385\n",
      "Epoch [305/4000] - Train Loss: 0.0736, Val Loss: 0.1392\n",
      "Epoch [306/4000] - Train Loss: 0.0735, Val Loss: 0.1397\n",
      "Epoch [307/4000] - Train Loss: 0.0739, Val Loss: 0.1388\n",
      "Epoch [308/4000] - Train Loss: 0.0732, Val Loss: 0.1383\n",
      "Epoch [309/4000] - Train Loss: 0.0737, Val Loss: 0.1407\n",
      "Epoch [310/4000] - Train Loss: 0.0736, Val Loss: 0.1385\n",
      "Epoch [311/4000] - Train Loss: 0.0735, Val Loss: 0.1396\n",
      "Epoch [312/4000] - Train Loss: 0.0729, Val Loss: 0.1374\n",
      "Epoch [313/4000] - Train Loss: 0.0724, Val Loss: 0.1385\n",
      "Epoch [314/4000] - Train Loss: 0.0727, Val Loss: 0.1380\n",
      "Epoch [315/4000] - Train Loss: 0.0730, Val Loss: 0.1397\n",
      "Epoch [316/4000] - Train Loss: 0.0726, Val Loss: 0.1393\n",
      "Epoch [317/4000] - Train Loss: 0.0727, Val Loss: 0.1395\n",
      "Epoch [318/4000] - Train Loss: 0.0727, Val Loss: 0.1381\n",
      "Epoch [319/4000] - Train Loss: 0.0726, Val Loss: 0.1377\n",
      "Epoch [320/4000] - Train Loss: 0.0723, Val Loss: 0.1383\n",
      "Epoch [321/4000] - Train Loss: 0.0718, Val Loss: 0.1376\n",
      "Epoch [322/4000] - Train Loss: 0.0717, Val Loss: 0.1381\n",
      "Epoch [323/4000] - Train Loss: 0.0715, Val Loss: 0.1377\n",
      "Epoch [324/4000] - Train Loss: 0.0712, Val Loss: 0.1382\n",
      "Epoch [325/4000] - Train Loss: 0.0714, Val Loss: 0.1381\n",
      "Epoch [326/4000] - Train Loss: 0.0710, Val Loss: 0.1383\n",
      "Epoch [327/4000] - Train Loss: 0.0709, Val Loss: 0.1384\n",
      "Epoch [328/4000] - Train Loss: 0.0709, Val Loss: 0.1373\n",
      "Epoch [329/4000] - Train Loss: 0.0707, Val Loss: 0.1369\n",
      "Epoch [330/4000] - Train Loss: 0.0709, Val Loss: 0.1369\n",
      "Epoch [331/4000] - Train Loss: 0.0713, Val Loss: 0.1365\n",
      "Epoch [332/4000] - Train Loss: 0.0715, Val Loss: 0.1378\n",
      "Epoch [333/4000] - Train Loss: 0.0711, Val Loss: 0.1380\n",
      "Epoch [334/4000] - Train Loss: 0.0709, Val Loss: 0.1383\n",
      "Epoch [335/4000] - Train Loss: 0.0710, Val Loss: 0.1384\n",
      "Epoch [336/4000] - Train Loss: 0.0698, Val Loss: 0.1370\n",
      "Epoch [337/4000] - Train Loss: 0.0710, Val Loss: 0.1373\n",
      "Epoch [338/4000] - Train Loss: 0.0705, Val Loss: 0.1383\n",
      "Epoch [339/4000] - Train Loss: 0.0709, Val Loss: 0.1391\n",
      "Epoch [340/4000] - Train Loss: 0.0717, Val Loss: 0.1385\n",
      "Epoch [341/4000] - Train Loss: 0.0714, Val Loss: 0.1382\n",
      "Epoch [342/4000] - Train Loss: 0.0713, Val Loss: 0.1366\n",
      "Epoch [343/4000] - Train Loss: 0.0699, Val Loss: 0.1373\n",
      "Epoch [344/4000] - Train Loss: 0.0696, Val Loss: 0.1360\n",
      "Epoch [345/4000] - Train Loss: 0.0700, Val Loss: 0.1364\n",
      "Epoch [346/4000] - Train Loss: 0.0697, Val Loss: 0.1374\n",
      "Epoch [347/4000] - Train Loss: 0.0697, Val Loss: 0.1376\n",
      "Epoch [348/4000] - Train Loss: 0.0694, Val Loss: 0.1363\n",
      "Epoch [349/4000] - Train Loss: 0.0688, Val Loss: 0.1352\n",
      "Epoch [350/4000] - Train Loss: 0.0691, Val Loss: 0.1361\n",
      "Epoch [351/4000] - Train Loss: 0.0694, Val Loss: 0.1354\n",
      "Epoch [352/4000] - Train Loss: 0.0703, Val Loss: 0.1364\n",
      "Epoch [353/4000] - Train Loss: 0.0696, Val Loss: 0.1363\n",
      "Epoch [354/4000] - Train Loss: 0.0690, Val Loss: 0.1357\n",
      "Epoch [355/4000] - Train Loss: 0.0687, Val Loss: 0.1363\n",
      "Epoch [356/4000] - Train Loss: 0.0689, Val Loss: 0.1369\n",
      "Epoch [357/4000] - Train Loss: 0.0690, Val Loss: 0.1373\n",
      "Epoch [358/4000] - Train Loss: 0.0687, Val Loss: 0.1363\n",
      "Epoch [359/4000] - Train Loss: 0.0687, Val Loss: 0.1358\n",
      "Epoch [360/4000] - Train Loss: 0.0684, Val Loss: 0.1354\n",
      "Epoch [361/4000] - Train Loss: 0.0683, Val Loss: 0.1366\n",
      "Epoch [362/4000] - Train Loss: 0.0688, Val Loss: 0.1367\n",
      "Epoch [363/4000] - Train Loss: 0.0689, Val Loss: 0.1367\n",
      "Epoch [364/4000] - Train Loss: 0.0690, Val Loss: 0.1362\n",
      "Epoch [365/4000] - Train Loss: 0.0685, Val Loss: 0.1366\n",
      "Epoch [366/4000] - Train Loss: 0.0692, Val Loss: 0.1365\n",
      "Epoch [367/4000] - Train Loss: 0.0681, Val Loss: 0.1363\n",
      "Epoch [368/4000] - Train Loss: 0.0680, Val Loss: 0.1354\n",
      "Epoch [369/4000] - Train Loss: 0.0685, Val Loss: 0.1364\n",
      "Epoch [370/4000] - Train Loss: 0.0688, Val Loss: 0.1395\n",
      "Epoch [371/4000] - Train Loss: 0.0684, Val Loss: 0.1361\n",
      "Epoch [372/4000] - Train Loss: 0.0677, Val Loss: 0.1361\n",
      "Epoch [373/4000] - Train Loss: 0.0676, Val Loss: 0.1345\n",
      "Epoch [374/4000] - Train Loss: 0.0673, Val Loss: 0.1356\n",
      "Epoch [375/4000] - Train Loss: 0.0669, Val Loss: 0.1344\n",
      "Epoch [376/4000] - Train Loss: 0.0673, Val Loss: 0.1351\n",
      "Epoch [377/4000] - Train Loss: 0.0685, Val Loss: 0.1363\n",
      "Epoch [378/4000] - Train Loss: 0.0683, Val Loss: 0.1351\n",
      "Epoch [379/4000] - Train Loss: 0.0683, Val Loss: 0.1368\n",
      "Epoch [380/4000] - Train Loss: 0.0677, Val Loss: 0.1346\n",
      "Epoch [381/4000] - Train Loss: 0.0679, Val Loss: 0.1353\n",
      "Epoch [382/4000] - Train Loss: 0.0684, Val Loss: 0.1348\n",
      "Epoch [383/4000] - Train Loss: 0.0673, Val Loss: 0.1352\n",
      "Epoch [384/4000] - Train Loss: 0.0663, Val Loss: 0.1350\n",
      "Epoch [385/4000] - Train Loss: 0.0663, Val Loss: 0.1362\n",
      "Epoch [386/4000] - Train Loss: 0.0675, Val Loss: 0.1373\n",
      "Epoch [387/4000] - Train Loss: 0.0684, Val Loss: 0.1358\n",
      "Epoch [388/4000] - Train Loss: 0.0661, Val Loss: 0.1345\n",
      "Epoch [389/4000] - Train Loss: 0.0659, Val Loss: 0.1349\n",
      "Epoch [390/4000] - Train Loss: 0.0666, Val Loss: 0.1350\n",
      "Epoch [391/4000] - Train Loss: 0.0664, Val Loss: 0.1356\n",
      "Epoch [392/4000] - Train Loss: 0.0668, Val Loss: 0.1351\n",
      "Epoch [393/4000] - Train Loss: 0.0664, Val Loss: 0.1347\n",
      "Epoch [394/4000] - Train Loss: 0.0663, Val Loss: 0.1336\n",
      "Epoch [395/4000] - Train Loss: 0.0653, Val Loss: 0.1355\n",
      "Epoch [396/4000] - Train Loss: 0.0656, Val Loss: 0.1339\n",
      "Epoch [397/4000] - Train Loss: 0.0653, Val Loss: 0.1343\n",
      "Epoch [398/4000] - Train Loss: 0.0658, Val Loss: 0.1356\n",
      "Epoch [399/4000] - Train Loss: 0.0659, Val Loss: 0.1338\n",
      "Epoch [400/4000] - Train Loss: 0.0666, Val Loss: 0.1347\n",
      "Epoch [401/4000] - Train Loss: 0.0664, Val Loss: 0.1347\n",
      "Epoch [402/4000] - Train Loss: 0.0667, Val Loss: 0.1349\n",
      "Epoch [403/4000] - Train Loss: 0.0657, Val Loss: 0.1334\n",
      "Epoch [404/4000] - Train Loss: 0.0653, Val Loss: 0.1340\n",
      "Epoch [405/4000] - Train Loss: 0.0658, Val Loss: 0.1338\n",
      "Epoch [406/4000] - Train Loss: 0.0659, Val Loss: 0.1349\n",
      "Epoch [407/4000] - Train Loss: 0.0646, Val Loss: 0.1333\n",
      "Epoch [408/4000] - Train Loss: 0.0642, Val Loss: 0.1352\n",
      "Epoch [409/4000] - Train Loss: 0.0650, Val Loss: 0.1343\n",
      "Epoch [410/4000] - Train Loss: 0.0658, Val Loss: 0.1324\n",
      "Epoch [411/4000] - Train Loss: 0.0667, Val Loss: 0.1338\n",
      "Epoch [412/4000] - Train Loss: 0.0658, Val Loss: 0.1371\n",
      "Epoch [413/4000] - Train Loss: 0.0655, Val Loss: 0.1344\n",
      "Epoch [414/4000] - Train Loss: 0.0647, Val Loss: 0.1324\n",
      "Epoch [415/4000] - Train Loss: 0.0645, Val Loss: 0.1359\n",
      "Epoch [416/4000] - Train Loss: 0.0649, Val Loss: 0.1363\n",
      "Epoch [417/4000] - Train Loss: 0.0653, Val Loss: 0.1336\n",
      "Epoch [418/4000] - Train Loss: 0.0642, Val Loss: 0.1320\n",
      "Epoch [419/4000] - Train Loss: 0.0639, Val Loss: 0.1345\n",
      "Epoch [420/4000] - Train Loss: 0.0643, Val Loss: 0.1340\n",
      "Epoch [421/4000] - Train Loss: 0.0639, Val Loss: 0.1319\n",
      "Epoch [422/4000] - Train Loss: 0.0635, Val Loss: 0.1350\n",
      "Epoch [423/4000] - Train Loss: 0.0649, Val Loss: 0.1333\n",
      "Epoch [424/4000] - Train Loss: 0.0646, Val Loss: 0.1327\n",
      "Epoch [425/4000] - Train Loss: 0.0639, Val Loss: 0.1336\n",
      "Epoch [426/4000] - Train Loss: 0.0639, Val Loss: 0.1334\n",
      "Epoch [427/4000] - Train Loss: 0.0642, Val Loss: 0.1326\n",
      "Epoch [428/4000] - Train Loss: 0.0642, Val Loss: 0.1332\n",
      "Epoch [429/4000] - Train Loss: 0.0640, Val Loss: 0.1343\n",
      "Epoch [430/4000] - Train Loss: 0.0644, Val Loss: 0.1329\n",
      "Epoch [431/4000] - Train Loss: 0.0635, Val Loss: 0.1335\n",
      "Epoch [432/4000] - Train Loss: 0.0634, Val Loss: 0.1330\n",
      "Epoch [433/4000] - Train Loss: 0.0635, Val Loss: 0.1317\n",
      "Epoch [434/4000] - Train Loss: 0.0641, Val Loss: 0.1314\n",
      "Epoch [435/4000] - Train Loss: 0.0635, Val Loss: 0.1323\n",
      "Epoch [436/4000] - Train Loss: 0.0630, Val Loss: 0.1311\n",
      "Epoch [437/4000] - Train Loss: 0.0628, Val Loss: 0.1318\n",
      "Epoch [438/4000] - Train Loss: 0.0629, Val Loss: 0.1323\n",
      "Epoch [439/4000] - Train Loss: 0.0636, Val Loss: 0.1335\n",
      "Epoch [440/4000] - Train Loss: 0.0632, Val Loss: 0.1328\n",
      "Epoch [441/4000] - Train Loss: 0.0628, Val Loss: 0.1309\n",
      "Epoch [442/4000] - Train Loss: 0.0624, Val Loss: 0.1311\n",
      "Epoch [443/4000] - Train Loss: 0.0626, Val Loss: 0.1306\n",
      "Epoch [444/4000] - Train Loss: 0.0617, Val Loss: 0.1317\n",
      "Epoch [445/4000] - Train Loss: 0.0620, Val Loss: 0.1308\n",
      "Epoch [446/4000] - Train Loss: 0.0625, Val Loss: 0.1319\n",
      "Epoch [447/4000] - Train Loss: 0.0628, Val Loss: 0.1319\n",
      "Epoch [448/4000] - Train Loss: 0.0627, Val Loss: 0.1333\n",
      "Epoch [449/4000] - Train Loss: 0.0632, Val Loss: 0.1327\n",
      "Epoch [450/4000] - Train Loss: 0.0627, Val Loss: 0.1319\n",
      "Epoch [451/4000] - Train Loss: 0.0619, Val Loss: 0.1314\n",
      "Epoch [452/4000] - Train Loss: 0.0626, Val Loss: 0.1312\n",
      "Epoch [453/4000] - Train Loss: 0.0624, Val Loss: 0.1306\n",
      "Epoch [454/4000] - Train Loss: 0.0619, Val Loss: 0.1305\n",
      "Epoch [455/4000] - Train Loss: 0.0617, Val Loss: 0.1321\n",
      "Epoch [456/4000] - Train Loss: 0.0616, Val Loss: 0.1317\n",
      "Epoch [457/4000] - Train Loss: 0.0614, Val Loss: 0.1305\n",
      "Epoch [458/4000] - Train Loss: 0.0620, Val Loss: 0.1335\n",
      "Epoch [459/4000] - Train Loss: 0.0628, Val Loss: 0.1355\n",
      "Epoch [460/4000] - Train Loss: 0.0637, Val Loss: 0.1317\n",
      "Epoch [461/4000] - Train Loss: 0.0634, Val Loss: 0.1321\n",
      "Epoch [462/4000] - Train Loss: 0.0630, Val Loss: 0.1323\n",
      "Epoch [463/4000] - Train Loss: 0.0625, Val Loss: 0.1312\n",
      "Epoch [464/4000] - Train Loss: 0.0627, Val Loss: 0.1312\n",
      "Epoch [465/4000] - Train Loss: 0.0619, Val Loss: 0.1311\n",
      "Epoch [466/4000] - Train Loss: 0.0613, Val Loss: 0.1310\n",
      "Epoch [467/4000] - Train Loss: 0.0612, Val Loss: 0.1311\n",
      "Epoch [468/4000] - Train Loss: 0.0613, Val Loss: 0.1308\n",
      "Epoch [469/4000] - Train Loss: 0.0621, Val Loss: 0.1301\n",
      "Epoch [470/4000] - Train Loss: 0.0612, Val Loss: 0.1305\n",
      "Epoch [471/4000] - Train Loss: 0.0609, Val Loss: 0.1304\n",
      "Epoch [472/4000] - Train Loss: 0.0613, Val Loss: 0.1309\n",
      "Epoch [473/4000] - Train Loss: 0.0617, Val Loss: 0.1296\n",
      "Epoch [474/4000] - Train Loss: 0.0616, Val Loss: 0.1305\n",
      "Epoch [475/4000] - Train Loss: 0.0609, Val Loss: 0.1302\n",
      "Epoch [476/4000] - Train Loss: 0.0607, Val Loss: 0.1318\n",
      "Epoch [477/4000] - Train Loss: 0.0621, Val Loss: 0.1306\n",
      "Epoch [478/4000] - Train Loss: 0.0608, Val Loss: 0.1302\n",
      "Epoch [479/4000] - Train Loss: 0.0611, Val Loss: 0.1304\n",
      "Epoch [480/4000] - Train Loss: 0.0604, Val Loss: 0.1300\n",
      "Epoch [481/4000] - Train Loss: 0.0611, Val Loss: 0.1298\n",
      "Epoch [482/4000] - Train Loss: 0.0606, Val Loss: 0.1295\n",
      "Epoch [483/4000] - Train Loss: 0.0607, Val Loss: 0.1302\n",
      "Epoch [484/4000] - Train Loss: 0.0608, Val Loss: 0.1313\n",
      "Epoch [485/4000] - Train Loss: 0.0604, Val Loss: 0.1287\n",
      "Epoch [486/4000] - Train Loss: 0.0604, Val Loss: 0.1290\n",
      "Epoch [487/4000] - Train Loss: 0.0598, Val Loss: 0.1297\n",
      "Epoch [488/4000] - Train Loss: 0.0607, Val Loss: 0.1292\n",
      "Epoch [489/4000] - Train Loss: 0.0597, Val Loss: 0.1298\n",
      "Epoch [490/4000] - Train Loss: 0.0602, Val Loss: 0.1308\n",
      "Epoch [491/4000] - Train Loss: 0.0605, Val Loss: 0.1312\n",
      "Epoch [492/4000] - Train Loss: 0.0597, Val Loss: 0.1306\n",
      "Epoch [493/4000] - Train Loss: 0.0613, Val Loss: 0.1306\n",
      "Epoch [494/4000] - Train Loss: 0.0602, Val Loss: 0.1295\n",
      "Epoch [495/4000] - Train Loss: 0.0607, Val Loss: 0.1306\n",
      "Epoch [496/4000] - Train Loss: 0.0597, Val Loss: 0.1296\n",
      "Epoch [497/4000] - Train Loss: 0.0599, Val Loss: 0.1295\n",
      "Epoch [498/4000] - Train Loss: 0.0597, Val Loss: 0.1289\n",
      "Epoch [499/4000] - Train Loss: 0.0595, Val Loss: 0.1292\n",
      "Epoch [500/4000] - Train Loss: 0.0593, Val Loss: 0.1292\n",
      "Epoch [501/4000] - Train Loss: 0.0591, Val Loss: 0.1283\n",
      "Epoch [502/4000] - Train Loss: 0.0590, Val Loss: 0.1301\n",
      "Epoch [503/4000] - Train Loss: 0.0591, Val Loss: 0.1285\n",
      "Epoch [504/4000] - Train Loss: 0.0592, Val Loss: 0.1292\n",
      "Epoch [505/4000] - Train Loss: 0.0597, Val Loss: 0.1292\n",
      "Epoch [506/4000] - Train Loss: 0.0600, Val Loss: 0.1306\n",
      "Epoch [507/4000] - Train Loss: 0.0593, Val Loss: 0.1282\n",
      "Epoch [508/4000] - Train Loss: 0.0587, Val Loss: 0.1284\n",
      "Epoch [509/4000] - Train Loss: 0.0595, Val Loss: 0.1285\n",
      "Epoch [510/4000] - Train Loss: 0.0591, Val Loss: 0.1296\n",
      "Epoch [511/4000] - Train Loss: 0.0600, Val Loss: 0.1280\n",
      "Epoch [512/4000] - Train Loss: 0.0600, Val Loss: 0.1292\n",
      "Epoch [513/4000] - Train Loss: 0.0591, Val Loss: 0.1285\n",
      "Epoch [514/4000] - Train Loss: 0.0583, Val Loss: 0.1293\n",
      "Epoch [515/4000] - Train Loss: 0.0590, Val Loss: 0.1272\n",
      "Epoch [516/4000] - Train Loss: 0.0589, Val Loss: 0.1284\n",
      "Epoch [517/4000] - Train Loss: 0.0586, Val Loss: 0.1277\n",
      "Epoch [518/4000] - Train Loss: 0.0579, Val Loss: 0.1284\n",
      "Epoch [519/4000] - Train Loss: 0.0585, Val Loss: 0.1309\n",
      "Epoch [520/4000] - Train Loss: 0.0583, Val Loss: 0.1266\n",
      "Epoch [521/4000] - Train Loss: 0.0584, Val Loss: 0.1283\n",
      "Epoch [522/4000] - Train Loss: 0.0582, Val Loss: 0.1272\n",
      "Epoch [523/4000] - Train Loss: 0.0583, Val Loss: 0.1278\n",
      "Epoch [524/4000] - Train Loss: 0.0582, Val Loss: 0.1276\n",
      "Epoch [525/4000] - Train Loss: 0.0580, Val Loss: 0.1264\n",
      "Epoch [526/4000] - Train Loss: 0.0575, Val Loss: 0.1269\n",
      "Epoch [527/4000] - Train Loss: 0.0579, Val Loss: 0.1289\n",
      "Epoch [528/4000] - Train Loss: 0.0590, Val Loss: 0.1300\n",
      "Epoch [529/4000] - Train Loss: 0.0584, Val Loss: 0.1274\n",
      "Epoch [530/4000] - Train Loss: 0.0576, Val Loss: 0.1274\n",
      "Epoch [531/4000] - Train Loss: 0.0570, Val Loss: 0.1269\n",
      "Epoch [532/4000] - Train Loss: 0.0581, Val Loss: 0.1273\n",
      "Epoch [533/4000] - Train Loss: 0.0582, Val Loss: 0.1287\n",
      "Epoch [534/4000] - Train Loss: 0.0579, Val Loss: 0.1264\n",
      "Epoch [535/4000] - Train Loss: 0.0574, Val Loss: 0.1268\n",
      "Epoch [536/4000] - Train Loss: 0.0567, Val Loss: 0.1279\n",
      "Epoch [537/4000] - Train Loss: 0.0572, Val Loss: 0.1265\n",
      "Epoch [538/4000] - Train Loss: 0.0572, Val Loss: 0.1260\n",
      "Epoch [539/4000] - Train Loss: 0.0573, Val Loss: 0.1274\n",
      "Epoch [540/4000] - Train Loss: 0.0566, Val Loss: 0.1268\n",
      "Epoch [541/4000] - Train Loss: 0.0566, Val Loss: 0.1271\n",
      "Epoch [542/4000] - Train Loss: 0.0563, Val Loss: 0.1270\n",
      "Epoch [543/4000] - Train Loss: 0.0569, Val Loss: 0.1258\n",
      "Epoch [544/4000] - Train Loss: 0.0579, Val Loss: 0.1268\n",
      "Epoch [545/4000] - Train Loss: 0.0570, Val Loss: 0.1264\n",
      "Epoch [546/4000] - Train Loss: 0.0564, Val Loss: 0.1265\n",
      "Epoch [547/4000] - Train Loss: 0.0563, Val Loss: 0.1280\n",
      "Epoch [548/4000] - Train Loss: 0.0568, Val Loss: 0.1283\n",
      "Epoch [549/4000] - Train Loss: 0.0570, Val Loss: 0.1278\n",
      "Epoch [550/4000] - Train Loss: 0.0563, Val Loss: 0.1262\n",
      "Epoch [551/4000] - Train Loss: 0.0567, Val Loss: 0.1248\n",
      "Epoch [552/4000] - Train Loss: 0.0567, Val Loss: 0.1256\n",
      "Epoch [553/4000] - Train Loss: 0.0567, Val Loss: 0.1253\n",
      "Epoch [554/4000] - Train Loss: 0.0568, Val Loss: 0.1256\n",
      "Epoch [555/4000] - Train Loss: 0.0562, Val Loss: 0.1279\n",
      "Epoch [556/4000] - Train Loss: 0.0565, Val Loss: 0.1251\n",
      "Epoch [557/4000] - Train Loss: 0.0555, Val Loss: 0.1249\n",
      "Epoch [558/4000] - Train Loss: 0.0561, Val Loss: 0.1249\n",
      "Epoch [559/4000] - Train Loss: 0.0556, Val Loss: 0.1245\n",
      "Epoch [560/4000] - Train Loss: 0.0559, Val Loss: 0.1254\n",
      "Epoch [561/4000] - Train Loss: 0.0562, Val Loss: 0.1255\n",
      "Epoch [562/4000] - Train Loss: 0.0559, Val Loss: 0.1247\n",
      "Epoch [563/4000] - Train Loss: 0.0566, Val Loss: 0.1258\n",
      "Epoch [564/4000] - Train Loss: 0.0551, Val Loss: 0.1259\n",
      "Epoch [565/4000] - Train Loss: 0.0563, Val Loss: 0.1267\n",
      "Epoch [566/4000] - Train Loss: 0.0564, Val Loss: 0.1258\n",
      "Epoch [567/4000] - Train Loss: 0.0554, Val Loss: 0.1233\n",
      "Epoch [568/4000] - Train Loss: 0.0548, Val Loss: 0.1240\n",
      "Epoch [569/4000] - Train Loss: 0.0556, Val Loss: 0.1250\n",
      "Epoch [570/4000] - Train Loss: 0.0557, Val Loss: 0.1255\n",
      "Epoch [571/4000] - Train Loss: 0.0567, Val Loss: 0.1255\n",
      "Epoch [572/4000] - Train Loss: 0.0562, Val Loss: 0.1242\n",
      "Epoch [573/4000] - Train Loss: 0.0553, Val Loss: 0.1266\n",
      "Epoch [574/4000] - Train Loss: 0.0552, Val Loss: 0.1242\n",
      "Epoch [575/4000] - Train Loss: 0.0550, Val Loss: 0.1242\n",
      "Epoch [576/4000] - Train Loss: 0.0553, Val Loss: 0.1239\n",
      "Epoch [577/4000] - Train Loss: 0.0552, Val Loss: 0.1252\n",
      "Epoch [578/4000] - Train Loss: 0.0584, Val Loss: 0.1258\n",
      "Epoch [579/4000] - Train Loss: 0.0574, Val Loss: 0.1242\n",
      "Epoch [580/4000] - Train Loss: 0.0580, Val Loss: 0.1255\n",
      "Epoch [581/4000] - Train Loss: 0.0587, Val Loss: 0.1300\n",
      "Epoch [582/4000] - Train Loss: 0.0578, Val Loss: 0.1285\n",
      "Epoch [583/4000] - Train Loss: 0.0565, Val Loss: 0.1229\n",
      "Epoch [584/4000] - Train Loss: 0.0553, Val Loss: 0.1234\n",
      "Epoch [585/4000] - Train Loss: 0.0549, Val Loss: 0.1240\n",
      "Epoch [586/4000] - Train Loss: 0.0547, Val Loss: 0.1222\n",
      "Epoch [587/4000] - Train Loss: 0.0546, Val Loss: 0.1226\n",
      "Epoch [588/4000] - Train Loss: 0.0546, Val Loss: 0.1244\n",
      "Epoch [589/4000] - Train Loss: 0.0547, Val Loss: 0.1245\n",
      "Epoch [590/4000] - Train Loss: 0.0548, Val Loss: 0.1236\n",
      "Epoch [591/4000] - Train Loss: 0.0545, Val Loss: 0.1233\n",
      "Epoch [592/4000] - Train Loss: 0.0541, Val Loss: 0.1241\n",
      "Epoch [593/4000] - Train Loss: 0.0538, Val Loss: 0.1234\n",
      "Epoch [594/4000] - Train Loss: 0.0540, Val Loss: 0.1222\n",
      "Epoch [595/4000] - Train Loss: 0.0541, Val Loss: 0.1222\n",
      "Epoch [596/4000] - Train Loss: 0.0537, Val Loss: 0.1221\n",
      "Epoch [597/4000] - Train Loss: 0.0538, Val Loss: 0.1233\n",
      "Epoch [598/4000] - Train Loss: 0.0542, Val Loss: 0.1233\n",
      "Epoch [599/4000] - Train Loss: 0.0533, Val Loss: 0.1224\n",
      "Epoch [600/4000] - Train Loss: 0.0535, Val Loss: 0.1225\n",
      "Epoch [601/4000] - Train Loss: 0.0550, Val Loss: 0.1235\n",
      "Epoch [602/4000] - Train Loss: 0.0541, Val Loss: 0.1233\n",
      "Epoch [603/4000] - Train Loss: 0.0536, Val Loss: 0.1228\n",
      "Epoch [604/4000] - Train Loss: 0.0535, Val Loss: 0.1238\n",
      "Epoch [605/4000] - Train Loss: 0.0547, Val Loss: 0.1236\n",
      "Epoch [606/4000] - Train Loss: 0.0540, Val Loss: 0.1249\n",
      "Epoch [607/4000] - Train Loss: 0.0547, Val Loss: 0.1236\n",
      "Epoch [608/4000] - Train Loss: 0.0541, Val Loss: 0.1246\n",
      "Epoch [609/4000] - Train Loss: 0.0542, Val Loss: 0.1240\n",
      "Epoch [610/4000] - Train Loss: 0.0544, Val Loss: 0.1218\n",
      "Epoch [611/4000] - Train Loss: 0.0533, Val Loss: 0.1229\n",
      "Epoch [612/4000] - Train Loss: 0.0539, Val Loss: 0.1270\n",
      "Epoch [613/4000] - Train Loss: 0.0557, Val Loss: 0.1241\n",
      "Epoch [614/4000] - Train Loss: 0.0550, Val Loss: 0.1225\n",
      "Epoch [615/4000] - Train Loss: 0.0561, Val Loss: 0.1242\n",
      "Epoch [616/4000] - Train Loss: 0.0534, Val Loss: 0.1239\n",
      "Epoch [617/4000] - Train Loss: 0.0535, Val Loss: 0.1244\n",
      "Epoch [618/4000] - Train Loss: 0.0542, Val Loss: 0.1225\n",
      "Epoch [619/4000] - Train Loss: 0.0539, Val Loss: 0.1249\n",
      "Epoch [620/4000] - Train Loss: 0.0533, Val Loss: 0.1218\n",
      "Epoch [621/4000] - Train Loss: 0.0531, Val Loss: 0.1229\n",
      "Epoch [622/4000] - Train Loss: 0.0532, Val Loss: 0.1220\n",
      "Epoch [623/4000] - Train Loss: 0.0534, Val Loss: 0.1222\n",
      "Epoch [624/4000] - Train Loss: 0.0530, Val Loss: 0.1212\n",
      "Epoch [625/4000] - Train Loss: 0.0532, Val Loss: 0.1212\n",
      "Epoch [626/4000] - Train Loss: 0.0530, Val Loss: 0.1238\n",
      "Epoch [627/4000] - Train Loss: 0.0533, Val Loss: 0.1228\n",
      "Epoch [628/4000] - Train Loss: 0.0535, Val Loss: 0.1225\n",
      "Epoch [629/4000] - Train Loss: 0.0533, Val Loss: 0.1217\n",
      "Epoch [630/4000] - Train Loss: 0.0528, Val Loss: 0.1221\n",
      "Epoch [631/4000] - Train Loss: 0.0531, Val Loss: 0.1206\n",
      "Epoch [632/4000] - Train Loss: 0.0526, Val Loss: 0.1214\n",
      "Epoch [633/4000] - Train Loss: 0.0540, Val Loss: 0.1230\n",
      "Epoch [634/4000] - Train Loss: 0.0550, Val Loss: 0.1249\n",
      "Epoch [635/4000] - Train Loss: 0.0540, Val Loss: 0.1238\n",
      "Epoch [636/4000] - Train Loss: 0.0537, Val Loss: 0.1233\n",
      "Epoch [637/4000] - Train Loss: 0.0542, Val Loss: 0.1209\n",
      "Epoch [638/4000] - Train Loss: 0.0546, Val Loss: 0.1208\n",
      "Epoch [639/4000] - Train Loss: 0.0549, Val Loss: 0.1257\n",
      "Epoch [640/4000] - Train Loss: 0.0540, Val Loss: 0.1214\n",
      "Epoch [641/4000] - Train Loss: 0.0527, Val Loss: 0.1199\n",
      "Epoch [642/4000] - Train Loss: 0.0524, Val Loss: 0.1216\n",
      "Epoch [643/4000] - Train Loss: 0.0523, Val Loss: 0.1217\n",
      "Epoch [644/4000] - Train Loss: 0.0521, Val Loss: 0.1219\n",
      "Epoch [645/4000] - Train Loss: 0.0517, Val Loss: 0.1206\n",
      "Epoch [646/4000] - Train Loss: 0.0519, Val Loss: 0.1216\n",
      "Epoch [647/4000] - Train Loss: 0.0528, Val Loss: 0.1218\n",
      "Epoch [648/4000] - Train Loss: 0.0524, Val Loss: 0.1212\n",
      "Epoch [649/4000] - Train Loss: 0.0524, Val Loss: 0.1228\n",
      "Epoch [650/4000] - Train Loss: 0.0525, Val Loss: 0.1203\n",
      "Epoch [651/4000] - Train Loss: 0.0518, Val Loss: 0.1200\n",
      "Epoch [652/4000] - Train Loss: 0.0517, Val Loss: 0.1194\n",
      "Epoch [653/4000] - Train Loss: 0.0515, Val Loss: 0.1216\n",
      "Epoch [654/4000] - Train Loss: 0.0519, Val Loss: 0.1213\n",
      "Epoch [655/4000] - Train Loss: 0.0530, Val Loss: 0.1224\n",
      "Epoch [656/4000] - Train Loss: 0.0520, Val Loss: 0.1208\n",
      "Epoch [657/4000] - Train Loss: 0.0514, Val Loss: 0.1202\n",
      "Epoch [658/4000] - Train Loss: 0.0513, Val Loss: 0.1209\n",
      "Epoch [659/4000] - Train Loss: 0.0523, Val Loss: 0.1208\n",
      "Epoch [660/4000] - Train Loss: 0.0518, Val Loss: 0.1216\n",
      "Epoch [661/4000] - Train Loss: 0.0516, Val Loss: 0.1210\n",
      "Epoch [662/4000] - Train Loss: 0.0512, Val Loss: 0.1200\n",
      "Epoch [663/4000] - Train Loss: 0.0518, Val Loss: 0.1211\n",
      "Epoch [664/4000] - Train Loss: 0.0511, Val Loss: 0.1216\n",
      "Epoch [665/4000] - Train Loss: 0.0510, Val Loss: 0.1210\n",
      "Epoch [666/4000] - Train Loss: 0.0518, Val Loss: 0.1203\n",
      "Epoch [667/4000] - Train Loss: 0.0515, Val Loss: 0.1208\n",
      "Epoch [668/4000] - Train Loss: 0.0532, Val Loss: 0.1223\n",
      "Epoch [669/4000] - Train Loss: 0.0530, Val Loss: 0.1202\n",
      "Epoch [670/4000] - Train Loss: 0.0516, Val Loss: 0.1207\n",
      "Epoch [671/4000] - Train Loss: 0.0514, Val Loss: 0.1201\n",
      "Epoch [672/4000] - Train Loss: 0.0510, Val Loss: 0.1204\n",
      "Epoch [673/4000] - Train Loss: 0.0510, Val Loss: 0.1211\n",
      "Epoch [674/4000] - Train Loss: 0.0514, Val Loss: 0.1203\n",
      "Epoch [675/4000] - Train Loss: 0.0505, Val Loss: 0.1202\n",
      "Epoch [676/4000] - Train Loss: 0.0510, Val Loss: 0.1199\n",
      "Epoch [677/4000] - Train Loss: 0.0514, Val Loss: 0.1208\n",
      "Epoch [678/4000] - Train Loss: 0.0506, Val Loss: 0.1200\n",
      "Epoch [679/4000] - Train Loss: 0.0508, Val Loss: 0.1188\n",
      "Epoch [680/4000] - Train Loss: 0.0507, Val Loss: 0.1193\n",
      "Epoch [681/4000] - Train Loss: 0.0506, Val Loss: 0.1189\n",
      "Epoch [682/4000] - Train Loss: 0.0507, Val Loss: 0.1200\n",
      "Epoch [683/4000] - Train Loss: 0.0512, Val Loss: 0.1203\n",
      "Epoch [684/4000] - Train Loss: 0.0510, Val Loss: 0.1199\n",
      "Epoch [685/4000] - Train Loss: 0.0517, Val Loss: 0.1197\n",
      "Epoch [686/4000] - Train Loss: 0.0505, Val Loss: 0.1219\n",
      "Epoch [687/4000] - Train Loss: 0.0513, Val Loss: 0.1215\n",
      "Epoch [688/4000] - Train Loss: 0.0516, Val Loss: 0.1207\n",
      "Epoch [689/4000] - Train Loss: 0.0509, Val Loss: 0.1202\n",
      "Epoch [690/4000] - Train Loss: 0.0512, Val Loss: 0.1199\n",
      "Epoch [691/4000] - Train Loss: 0.0503, Val Loss: 0.1193\n",
      "Epoch [692/4000] - Train Loss: 0.0506, Val Loss: 0.1194\n",
      "Epoch [693/4000] - Train Loss: 0.0498, Val Loss: 0.1187\n",
      "Epoch [694/4000] - Train Loss: 0.0509, Val Loss: 0.1201\n",
      "Epoch [695/4000] - Train Loss: 0.0507, Val Loss: 0.1194\n",
      "Epoch [696/4000] - Train Loss: 0.0510, Val Loss: 0.1197\n",
      "Epoch [697/4000] - Train Loss: 0.0503, Val Loss: 0.1184\n",
      "Epoch [698/4000] - Train Loss: 0.0509, Val Loss: 0.1190\n",
      "Epoch [699/4000] - Train Loss: 0.0511, Val Loss: 0.1197\n",
      "Epoch [700/4000] - Train Loss: 0.0525, Val Loss: 0.1205\n",
      "Epoch [701/4000] - Train Loss: 0.0518, Val Loss: 0.1198\n",
      "Epoch [702/4000] - Train Loss: 0.0504, Val Loss: 0.1182\n",
      "Epoch [703/4000] - Train Loss: 0.0506, Val Loss: 0.1191\n",
      "Epoch [704/4000] - Train Loss: 0.0506, Val Loss: 0.1185\n",
      "Epoch [705/4000] - Train Loss: 0.0506, Val Loss: 0.1204\n",
      "Epoch [706/4000] - Train Loss: 0.0509, Val Loss: 0.1205\n",
      "Epoch [707/4000] - Train Loss: 0.0507, Val Loss: 0.1204\n",
      "Epoch [708/4000] - Train Loss: 0.0504, Val Loss: 0.1190\n",
      "Epoch [709/4000] - Train Loss: 0.0504, Val Loss: 0.1208\n",
      "Epoch [710/4000] - Train Loss: 0.0512, Val Loss: 0.1228\n",
      "Epoch [711/4000] - Train Loss: 0.0512, Val Loss: 0.1197\n",
      "Epoch [712/4000] - Train Loss: 0.0503, Val Loss: 0.1187\n",
      "Epoch [713/4000] - Train Loss: 0.0499, Val Loss: 0.1186\n",
      "Epoch [714/4000] - Train Loss: 0.0501, Val Loss: 0.1184\n",
      "Epoch [715/4000] - Train Loss: 0.0500, Val Loss: 0.1176\n",
      "Epoch [716/4000] - Train Loss: 0.0501, Val Loss: 0.1188\n",
      "Epoch [717/4000] - Train Loss: 0.0499, Val Loss: 0.1182\n",
      "Epoch [718/4000] - Train Loss: 0.0499, Val Loss: 0.1180\n",
      "Epoch [719/4000] - Train Loss: 0.0503, Val Loss: 0.1179\n",
      "Epoch [720/4000] - Train Loss: 0.0496, Val Loss: 0.1181\n",
      "Epoch [721/4000] - Train Loss: 0.0498, Val Loss: 0.1187\n",
      "Epoch [722/4000] - Train Loss: 0.0504, Val Loss: 0.1192\n",
      "Epoch [723/4000] - Train Loss: 0.0506, Val Loss: 0.1188\n",
      "Epoch [724/4000] - Train Loss: 0.0505, Val Loss: 0.1176\n",
      "Epoch [725/4000] - Train Loss: 0.0494, Val Loss: 0.1193\n",
      "Epoch [726/4000] - Train Loss: 0.0499, Val Loss: 0.1181\n",
      "Epoch [727/4000] - Train Loss: 0.0500, Val Loss: 0.1181\n",
      "Epoch [728/4000] - Train Loss: 0.0514, Val Loss: 0.1182\n",
      "Epoch [729/4000] - Train Loss: 0.0502, Val Loss: 0.1180\n",
      "Epoch [730/4000] - Train Loss: 0.0497, Val Loss: 0.1190\n",
      "Epoch [731/4000] - Train Loss: 0.0502, Val Loss: 0.1191\n",
      "Epoch [732/4000] - Train Loss: 0.0509, Val Loss: 0.1183\n",
      "Epoch [733/4000] - Train Loss: 0.0514, Val Loss: 0.1174\n",
      "Epoch [734/4000] - Train Loss: 0.0508, Val Loss: 0.1172\n",
      "Epoch [735/4000] - Train Loss: 0.0506, Val Loss: 0.1184\n",
      "Epoch [736/4000] - Train Loss: 0.0499, Val Loss: 0.1202\n",
      "Epoch [737/4000] - Train Loss: 0.0507, Val Loss: 0.1203\n",
      "Epoch [738/4000] - Train Loss: 0.0503, Val Loss: 0.1181\n",
      "Epoch [739/4000] - Train Loss: 0.0496, Val Loss: 0.1188\n",
      "Epoch [740/4000] - Train Loss: 0.0492, Val Loss: 0.1177\n",
      "Epoch [741/4000] - Train Loss: 0.0506, Val Loss: 0.1185\n",
      "Epoch [742/4000] - Train Loss: 0.0508, Val Loss: 0.1178\n",
      "Epoch [743/4000] - Train Loss: 0.0502, Val Loss: 0.1181\n",
      "Epoch [744/4000] - Train Loss: 0.0496, Val Loss: 0.1167\n",
      "Epoch [745/4000] - Train Loss: 0.0492, Val Loss: 0.1173\n",
      "Epoch [746/4000] - Train Loss: 0.0495, Val Loss: 0.1178\n",
      "Epoch [747/4000] - Train Loss: 0.0488, Val Loss: 0.1170\n",
      "Epoch [748/4000] - Train Loss: 0.0497, Val Loss: 0.1172\n",
      "Epoch [749/4000] - Train Loss: 0.0495, Val Loss: 0.1178\n",
      "Epoch [750/4000] - Train Loss: 0.0495, Val Loss: 0.1174\n",
      "Epoch [751/4000] - Train Loss: 0.0498, Val Loss: 0.1171\n",
      "Epoch [752/4000] - Train Loss: 0.0497, Val Loss: 0.1167\n",
      "Epoch [753/4000] - Train Loss: 0.0495, Val Loss: 0.1170\n",
      "Epoch [754/4000] - Train Loss: 0.0495, Val Loss: 0.1183\n",
      "Epoch [755/4000] - Train Loss: 0.0485, Val Loss: 0.1184\n",
      "Epoch [756/4000] - Train Loss: 0.0484, Val Loss: 0.1171\n",
      "Epoch [757/4000] - Train Loss: 0.0485, Val Loss: 0.1179\n",
      "Epoch [758/4000] - Train Loss: 0.0491, Val Loss: 0.1171\n",
      "Epoch [759/4000] - Train Loss: 0.0500, Val Loss: 0.1177\n",
      "Epoch [760/4000] - Train Loss: 0.0493, Val Loss: 0.1181\n",
      "Epoch [761/4000] - Train Loss: 0.0491, Val Loss: 0.1178\n",
      "Epoch [762/4000] - Train Loss: 0.0496, Val Loss: 0.1173\n",
      "Epoch [763/4000] - Train Loss: 0.0487, Val Loss: 0.1169\n",
      "Epoch [764/4000] - Train Loss: 0.0504, Val Loss: 0.1174\n",
      "Epoch [765/4000] - Train Loss: 0.0491, Val Loss: 0.1176\n",
      "Epoch [766/4000] - Train Loss: 0.0495, Val Loss: 0.1182\n",
      "Epoch [767/4000] - Train Loss: 0.0500, Val Loss: 0.1179\n",
      "Epoch [768/4000] - Train Loss: 0.0489, Val Loss: 0.1172\n",
      "Epoch [769/4000] - Train Loss: 0.0490, Val Loss: 0.1175\n",
      "Epoch [770/4000] - Train Loss: 0.0492, Val Loss: 0.1163\n",
      "Epoch [771/4000] - Train Loss: 0.0487, Val Loss: 0.1166\n",
      "Epoch [772/4000] - Train Loss: 0.0497, Val Loss: 0.1177\n",
      "Epoch [773/4000] - Train Loss: 0.0510, Val Loss: 0.1210\n",
      "Epoch [774/4000] - Train Loss: 0.0574, Val Loss: 0.1168\n",
      "Epoch [775/4000] - Train Loss: 0.0524, Val Loss: 0.1226\n",
      "Epoch [776/4000] - Train Loss: 0.0499, Val Loss: 0.1183\n",
      "Epoch [777/4000] - Train Loss: 0.0497, Val Loss: 0.1159\n",
      "Epoch [778/4000] - Train Loss: 0.0484, Val Loss: 0.1165\n",
      "Epoch [779/4000] - Train Loss: 0.0484, Val Loss: 0.1166\n",
      "Epoch [780/4000] - Train Loss: 0.0479, Val Loss: 0.1168\n",
      "Epoch [781/4000] - Train Loss: 0.0480, Val Loss: 0.1172\n",
      "Epoch [782/4000] - Train Loss: 0.0483, Val Loss: 0.1167\n",
      "Epoch [783/4000] - Train Loss: 0.0477, Val Loss: 0.1170\n",
      "Epoch [784/4000] - Train Loss: 0.0475, Val Loss: 0.1162\n",
      "Epoch [785/4000] - Train Loss: 0.0480, Val Loss: 0.1165\n",
      "Epoch [786/4000] - Train Loss: 0.0483, Val Loss: 0.1167\n",
      "Epoch [787/4000] - Train Loss: 0.0475, Val Loss: 0.1167\n",
      "Epoch [788/4000] - Train Loss: 0.0480, Val Loss: 0.1171\n",
      "Epoch [789/4000] - Train Loss: 0.0487, Val Loss: 0.1158\n",
      "Epoch [790/4000] - Train Loss: 0.0479, Val Loss: 0.1170\n",
      "Epoch [791/4000] - Train Loss: 0.0487, Val Loss: 0.1184\n",
      "Epoch [792/4000] - Train Loss: 0.0483, Val Loss: 0.1175\n",
      "Epoch [793/4000] - Train Loss: 0.0490, Val Loss: 0.1168\n",
      "Epoch [794/4000] - Train Loss: 0.0474, Val Loss: 0.1175\n",
      "Epoch [795/4000] - Train Loss: 0.0475, Val Loss: 0.1159\n",
      "Epoch [796/4000] - Train Loss: 0.0475, Val Loss: 0.1165\n",
      "Epoch [797/4000] - Train Loss: 0.0479, Val Loss: 0.1163\n",
      "Epoch [798/4000] - Train Loss: 0.0480, Val Loss: 0.1166\n",
      "Epoch [799/4000] - Train Loss: 0.0475, Val Loss: 0.1159\n",
      "Epoch [800/4000] - Train Loss: 0.0484, Val Loss: 0.1166\n",
      "Epoch [801/4000] - Train Loss: 0.0488, Val Loss: 0.1166\n",
      "Epoch [802/4000] - Train Loss: 0.0492, Val Loss: 0.1159\n",
      "Epoch [803/4000] - Train Loss: 0.0481, Val Loss: 0.1153\n",
      "Epoch [804/4000] - Train Loss: 0.0485, Val Loss: 0.1156\n",
      "Epoch [805/4000] - Train Loss: 0.0478, Val Loss: 0.1177\n",
      "Epoch [806/4000] - Train Loss: 0.0482, Val Loss: 0.1180\n",
      "Epoch [807/4000] - Train Loss: 0.0498, Val Loss: 0.1169\n",
      "Epoch [808/4000] - Train Loss: 0.0490, Val Loss: 0.1194\n",
      "Epoch [809/4000] - Train Loss: 0.0496, Val Loss: 0.1189\n",
      "Epoch [810/4000] - Train Loss: 0.0510, Val Loss: 0.1188\n",
      "Epoch [811/4000] - Train Loss: 0.0490, Val Loss: 0.1176\n",
      "Epoch [812/4000] - Train Loss: 0.0487, Val Loss: 0.1156\n",
      "Epoch [813/4000] - Train Loss: 0.0485, Val Loss: 0.1157\n",
      "Epoch [814/4000] - Train Loss: 0.0482, Val Loss: 0.1154\n",
      "Epoch [815/4000] - Train Loss: 0.0478, Val Loss: 0.1159\n",
      "Epoch [816/4000] - Train Loss: 0.0476, Val Loss: 0.1165\n",
      "Epoch [817/4000] - Train Loss: 0.0483, Val Loss: 0.1175\n",
      "Epoch [818/4000] - Train Loss: 0.0479, Val Loss: 0.1162\n",
      "Epoch [819/4000] - Train Loss: 0.0488, Val Loss: 0.1155\n",
      "Epoch [820/4000] - Train Loss: 0.0481, Val Loss: 0.1151\n",
      "Epoch [821/4000] - Train Loss: 0.0479, Val Loss: 0.1163\n",
      "Epoch [822/4000] - Train Loss: 0.0484, Val Loss: 0.1162\n",
      "Epoch [823/4000] - Train Loss: 0.0486, Val Loss: 0.1165\n",
      "Epoch [824/4000] - Train Loss: 0.0493, Val Loss: 0.1171\n",
      "Epoch [825/4000] - Train Loss: 0.0488, Val Loss: 0.1157\n",
      "Epoch [826/4000] - Train Loss: 0.0479, Val Loss: 0.1152\n",
      "Epoch [827/4000] - Train Loss: 0.0483, Val Loss: 0.1165\n",
      "Epoch [828/4000] - Train Loss: 0.0475, Val Loss: 0.1158\n",
      "Epoch [829/4000] - Train Loss: 0.0467, Val Loss: 0.1165\n",
      "Epoch [830/4000] - Train Loss: 0.0471, Val Loss: 0.1173\n",
      "Epoch [831/4000] - Train Loss: 0.0476, Val Loss: 0.1169\n",
      "Epoch [832/4000] - Train Loss: 0.0478, Val Loss: 0.1171\n",
      "Epoch [833/4000] - Train Loss: 0.0478, Val Loss: 0.1163\n",
      "Epoch [834/4000] - Train Loss: 0.0478, Val Loss: 0.1153\n",
      "Epoch [835/4000] - Train Loss: 0.0478, Val Loss: 0.1163\n",
      "Epoch [836/4000] - Train Loss: 0.0473, Val Loss: 0.1163\n",
      "Epoch [837/4000] - Train Loss: 0.0470, Val Loss: 0.1168\n",
      "Epoch [838/4000] - Train Loss: 0.0472, Val Loss: 0.1160\n",
      "Epoch [839/4000] - Train Loss: 0.0479, Val Loss: 0.1168\n",
      "Epoch [840/4000] - Train Loss: 0.0472, Val Loss: 0.1174\n",
      "Epoch [841/4000] - Train Loss: 0.0478, Val Loss: 0.1169\n",
      "Epoch [842/4000] - Train Loss: 0.0474, Val Loss: 0.1171\n",
      "Epoch [843/4000] - Train Loss: 0.0475, Val Loss: 0.1177\n",
      "Epoch [844/4000] - Train Loss: 0.0477, Val Loss: 0.1185\n",
      "Epoch [845/4000] - Train Loss: 0.0478, Val Loss: 0.1168\n",
      "Epoch [846/4000] - Train Loss: 0.0479, Val Loss: 0.1172\n",
      "Epoch [847/4000] - Train Loss: 0.0491, Val Loss: 0.1164\n",
      "Epoch [848/4000] - Train Loss: 0.0486, Val Loss: 0.1170\n",
      "Epoch [849/4000] - Train Loss: 0.0476, Val Loss: 0.1171\n",
      "Epoch [850/4000] - Train Loss: 0.0479, Val Loss: 0.1150\n",
      "Epoch [851/4000] - Train Loss: 0.0474, Val Loss: 0.1161\n",
      "Epoch [852/4000] - Train Loss: 0.0474, Val Loss: 0.1160\n",
      "Epoch [853/4000] - Train Loss: 0.0473, Val Loss: 0.1149\n",
      "Epoch [854/4000] - Train Loss: 0.0474, Val Loss: 0.1162\n",
      "Epoch [855/4000] - Train Loss: 0.0468, Val Loss: 0.1157\n",
      "Epoch [856/4000] - Train Loss: 0.0476, Val Loss: 0.1157\n",
      "Epoch [857/4000] - Train Loss: 0.0472, Val Loss: 0.1164\n",
      "Epoch [858/4000] - Train Loss: 0.0476, Val Loss: 0.1181\n",
      "Epoch [859/4000] - Train Loss: 0.0484, Val Loss: 0.1190\n",
      "Epoch [860/4000] - Train Loss: 0.0495, Val Loss: 0.1182\n",
      "Epoch [861/4000] - Train Loss: 0.0502, Val Loss: 0.1166\n",
      "Epoch [862/4000] - Train Loss: 0.0498, Val Loss: 0.1158\n",
      "Epoch [863/4000] - Train Loss: 0.0483, Val Loss: 0.1181\n",
      "Epoch [864/4000] - Train Loss: 0.0485, Val Loss: 0.1151\n",
      "Epoch [865/4000] - Train Loss: 0.0480, Val Loss: 0.1154\n",
      "Epoch [866/4000] - Train Loss: 0.0475, Val Loss: 0.1144\n",
      "Epoch [867/4000] - Train Loss: 0.0468, Val Loss: 0.1142\n",
      "Epoch [868/4000] - Train Loss: 0.0469, Val Loss: 0.1146\n",
      "Epoch [869/4000] - Train Loss: 0.0473, Val Loss: 0.1154\n",
      "Epoch [870/4000] - Train Loss: 0.0466, Val Loss: 0.1151\n",
      "Epoch [871/4000] - Train Loss: 0.0471, Val Loss: 0.1161\n",
      "Epoch [872/4000] - Train Loss: 0.0467, Val Loss: 0.1158\n",
      "Epoch [873/4000] - Train Loss: 0.0465, Val Loss: 0.1161\n",
      "Epoch [874/4000] - Train Loss: 0.0467, Val Loss: 0.1151\n",
      "Epoch [875/4000] - Train Loss: 0.0465, Val Loss: 0.1144\n",
      "Epoch [876/4000] - Train Loss: 0.0471, Val Loss: 0.1146\n",
      "Epoch [877/4000] - Train Loss: 0.0480, Val Loss: 0.1147\n",
      "Epoch [878/4000] - Train Loss: 0.0470, Val Loss: 0.1147\n",
      "Epoch [879/4000] - Train Loss: 0.0472, Val Loss: 0.1149\n",
      "Epoch [880/4000] - Train Loss: 0.0482, Val Loss: 0.1153\n",
      "Epoch [881/4000] - Train Loss: 0.0478, Val Loss: 0.1145\n",
      "Epoch [882/4000] - Train Loss: 0.0475, Val Loss: 0.1159\n",
      "Epoch [883/4000] - Train Loss: 0.0473, Val Loss: 0.1151\n",
      "Epoch [884/4000] - Train Loss: 0.0474, Val Loss: 0.1147\n",
      "Epoch [885/4000] - Train Loss: 0.0467, Val Loss: 0.1161\n",
      "Epoch [886/4000] - Train Loss: 0.0468, Val Loss: 0.1152\n",
      "Epoch [887/4000] - Train Loss: 0.0471, Val Loss: 0.1150\n",
      "Epoch [888/4000] - Train Loss: 0.0479, Val Loss: 0.1157\n",
      "Epoch [889/4000] - Train Loss: 0.0473, Val Loss: 0.1148\n",
      "Epoch [890/4000] - Train Loss: 0.0471, Val Loss: 0.1148\n",
      "Epoch [891/4000] - Train Loss: 0.0465, Val Loss: 0.1152\n",
      "Epoch [892/4000] - Train Loss: 0.0468, Val Loss: 0.1146\n",
      "Epoch [893/4000] - Train Loss: 0.0464, Val Loss: 0.1144\n",
      "Epoch [894/4000] - Train Loss: 0.0460, Val Loss: 0.1148\n",
      "Epoch [895/4000] - Train Loss: 0.0469, Val Loss: 0.1139\n",
      "Epoch [896/4000] - Train Loss: 0.0465, Val Loss: 0.1145\n",
      "Epoch [897/4000] - Train Loss: 0.0472, Val Loss: 0.1152\n",
      "Epoch [898/4000] - Train Loss: 0.0476, Val Loss: 0.1168\n",
      "Epoch [899/4000] - Train Loss: 0.0475, Val Loss: 0.1181\n",
      "Epoch [900/4000] - Train Loss: 0.0479, Val Loss: 0.1154\n",
      "Epoch [901/4000] - Train Loss: 0.0484, Val Loss: 0.1140\n",
      "Epoch [902/4000] - Train Loss: 0.0472, Val Loss: 0.1149\n",
      "Epoch [903/4000] - Train Loss: 0.0469, Val Loss: 0.1158\n",
      "Epoch [904/4000] - Train Loss: 0.0462, Val Loss: 0.1154\n",
      "Epoch [905/4000] - Train Loss: 0.0463, Val Loss: 0.1147\n",
      "Epoch [906/4000] - Train Loss: 0.0462, Val Loss: 0.1153\n",
      "Epoch [907/4000] - Train Loss: 0.0465, Val Loss: 0.1157\n",
      "Epoch [908/4000] - Train Loss: 0.0468, Val Loss: 0.1157\n",
      "Epoch [909/4000] - Train Loss: 0.0471, Val Loss: 0.1157\n",
      "Epoch [910/4000] - Train Loss: 0.0463, Val Loss: 0.1159\n",
      "Epoch [911/4000] - Train Loss: 0.0467, Val Loss: 0.1152\n",
      "Epoch [912/4000] - Train Loss: 0.0467, Val Loss: 0.1138\n",
      "Epoch [913/4000] - Train Loss: 0.0463, Val Loss: 0.1145\n",
      "Epoch [914/4000] - Train Loss: 0.0462, Val Loss: 0.1140\n",
      "Epoch [915/4000] - Train Loss: 0.0467, Val Loss: 0.1163\n",
      "Epoch [916/4000] - Train Loss: 0.0474, Val Loss: 0.1150\n",
      "Epoch [917/4000] - Train Loss: 0.0476, Val Loss: 0.1161\n",
      "Epoch [918/4000] - Train Loss: 0.0464, Val Loss: 0.1177\n",
      "Epoch [919/4000] - Train Loss: 0.0459, Val Loss: 0.1156\n",
      "Epoch [920/4000] - Train Loss: 0.0474, Val Loss: 0.1157\n",
      "Epoch [921/4000] - Train Loss: 0.0462, Val Loss: 0.1158\n",
      "Epoch [922/4000] - Train Loss: 0.0466, Val Loss: 0.1154\n",
      "Epoch [923/4000] - Train Loss: 0.0463, Val Loss: 0.1143\n",
      "Epoch [924/4000] - Train Loss: 0.0467, Val Loss: 0.1143\n",
      "Epoch [925/4000] - Train Loss: 0.0468, Val Loss: 0.1149\n",
      "Epoch [926/4000] - Train Loss: 0.0477, Val Loss: 0.1154\n",
      "Epoch [927/4000] - Train Loss: 0.0476, Val Loss: 0.1156\n",
      "Epoch [928/4000] - Train Loss: 0.0478, Val Loss: 0.1153\n",
      "Epoch [929/4000] - Train Loss: 0.0471, Val Loss: 0.1146\n",
      "Epoch [930/4000] - Train Loss: 0.0463, Val Loss: 0.1158\n",
      "Epoch [931/4000] - Train Loss: 0.0472, Val Loss: 0.1151\n",
      "Epoch [932/4000] - Train Loss: 0.0467, Val Loss: 0.1136\n",
      "Epoch [933/4000] - Train Loss: 0.0453, Val Loss: 0.1150\n",
      "Epoch [934/4000] - Train Loss: 0.0458, Val Loss: 0.1143\n",
      "Epoch [935/4000] - Train Loss: 0.0463, Val Loss: 0.1150\n",
      "Epoch [936/4000] - Train Loss: 0.0461, Val Loss: 0.1157\n",
      "Epoch [937/4000] - Train Loss: 0.0467, Val Loss: 0.1181\n",
      "Epoch [938/4000] - Train Loss: 0.0475, Val Loss: 0.1166\n",
      "Epoch [939/4000] - Train Loss: 0.0473, Val Loss: 0.1168\n",
      "Epoch [940/4000] - Train Loss: 0.0472, Val Loss: 0.1136\n",
      "Epoch [941/4000] - Train Loss: 0.0460, Val Loss: 0.1147\n",
      "Epoch [942/4000] - Train Loss: 0.0460, Val Loss: 0.1144\n",
      "Epoch [943/4000] - Train Loss: 0.0456, Val Loss: 0.1158\n",
      "Epoch [944/4000] - Train Loss: 0.0460, Val Loss: 0.1151\n",
      "Epoch [945/4000] - Train Loss: 0.0464, Val Loss: 0.1156\n",
      "Epoch [946/4000] - Train Loss: 0.0479, Val Loss: 0.1158\n",
      "Epoch [947/4000] - Train Loss: 0.0470, Val Loss: 0.1153\n",
      "Epoch [948/4000] - Train Loss: 0.0461, Val Loss: 0.1144\n",
      "Epoch [949/4000] - Train Loss: 0.0454, Val Loss: 0.1160\n",
      "Epoch [950/4000] - Train Loss: 0.0454, Val Loss: 0.1143\n",
      "Epoch [951/4000] - Train Loss: 0.0461, Val Loss: 0.1153\n",
      "Epoch [952/4000] - Train Loss: 0.0461, Val Loss: 0.1149\n",
      "Epoch [953/4000] - Train Loss: 0.0458, Val Loss: 0.1158\n",
      "Epoch [954/4000] - Train Loss: 0.0467, Val Loss: 0.1138\n",
      "Epoch [955/4000] - Train Loss: 0.0463, Val Loss: 0.1141\n",
      "Epoch [956/4000] - Train Loss: 0.0477, Val Loss: 0.1176\n",
      "Epoch [957/4000] - Train Loss: 0.0477, Val Loss: 0.1166\n",
      "Epoch [958/4000] - Train Loss: 0.0455, Val Loss: 0.1176\n",
      "Epoch [959/4000] - Train Loss: 0.0478, Val Loss: 0.1151\n",
      "Epoch [960/4000] - Train Loss: 0.0482, Val Loss: 0.1173\n",
      "Epoch [961/4000] - Train Loss: 0.0476, Val Loss: 0.1166\n",
      "Epoch [962/4000] - Train Loss: 0.0470, Val Loss: 0.1150\n",
      "Epoch [963/4000] - Train Loss: 0.0466, Val Loss: 0.1157\n",
      "Epoch [964/4000] - Train Loss: 0.0465, Val Loss: 0.1146\n",
      "Epoch [965/4000] - Train Loss: 0.0452, Val Loss: 0.1154\n",
      "Epoch [966/4000] - Train Loss: 0.0451, Val Loss: 0.1173\n",
      "Epoch [967/4000] - Train Loss: 0.0473, Val Loss: 0.1204\n",
      "Epoch [968/4000] - Train Loss: 0.0463, Val Loss: 0.1167\n",
      "Epoch [969/4000] - Train Loss: 0.0467, Val Loss: 0.1144\n",
      "Epoch [970/4000] - Train Loss: 0.0461, Val Loss: 0.1147\n",
      "Epoch [971/4000] - Train Loss: 0.0454, Val Loss: 0.1155\n",
      "Epoch [972/4000] - Train Loss: 0.0458, Val Loss: 0.1157\n",
      "Epoch [973/4000] - Train Loss: 0.0456, Val Loss: 0.1153\n",
      "Epoch [974/4000] - Train Loss: 0.0460, Val Loss: 0.1161\n",
      "Epoch [975/4000] - Train Loss: 0.0455, Val Loss: 0.1151\n",
      "Epoch [976/4000] - Train Loss: 0.0460, Val Loss: 0.1151\n",
      "Epoch [977/4000] - Train Loss: 0.0462, Val Loss: 0.1145\n",
      "Epoch [978/4000] - Train Loss: 0.0459, Val Loss: 0.1138\n",
      "Epoch [979/4000] - Train Loss: 0.0457, Val Loss: 0.1143\n",
      "Epoch [980/4000] - Train Loss: 0.0467, Val Loss: 0.1145\n",
      "Epoch [981/4000] - Train Loss: 0.0513, Val Loss: 0.1195\n",
      "Epoch [982/4000] - Train Loss: 0.0502, Val Loss: 0.1186\n",
      "Epoch [983/4000] - Train Loss: 0.0485, Val Loss: 0.1190\n",
      "Epoch [984/4000] - Train Loss: 0.0491, Val Loss: 0.1172\n",
      "Epoch [985/4000] - Train Loss: 0.0471, Val Loss: 0.1128\n",
      "Epoch [986/4000] - Train Loss: 0.0465, Val Loss: 0.1143\n",
      "Epoch [987/4000] - Train Loss: 0.0456, Val Loss: 0.1149\n",
      "Epoch [988/4000] - Train Loss: 0.0458, Val Loss: 0.1154\n",
      "Epoch [989/4000] - Train Loss: 0.0457, Val Loss: 0.1165\n",
      "Epoch [990/4000] - Train Loss: 0.0458, Val Loss: 0.1150\n",
      "Epoch [991/4000] - Train Loss: 0.0461, Val Loss: 0.1144\n",
      "Epoch [992/4000] - Train Loss: 0.0454, Val Loss: 0.1146\n",
      "Epoch [993/4000] - Train Loss: 0.0452, Val Loss: 0.1143\n",
      "Epoch [994/4000] - Train Loss: 0.0448, Val Loss: 0.1139\n",
      "Epoch [995/4000] - Train Loss: 0.0447, Val Loss: 0.1138\n",
      "Epoch [996/4000] - Train Loss: 0.0466, Val Loss: 0.1144\n",
      "Epoch [997/4000] - Train Loss: 0.0461, Val Loss: 0.1141\n",
      "Epoch [998/4000] - Train Loss: 0.0452, Val Loss: 0.1148\n",
      "Epoch [999/4000] - Train Loss: 0.0445, Val Loss: 0.1152\n",
      "Epoch [1000/4000] - Train Loss: 0.0447, Val Loss: 0.1140\n",
      "Epoch [1001/4000] - Train Loss: 0.0446, Val Loss: 0.1140\n",
      "Epoch [1002/4000] - Train Loss: 0.0447, Val Loss: 0.1145\n",
      "Epoch [1003/4000] - Train Loss: 0.0451, Val Loss: 0.1146\n",
      "Epoch [1004/4000] - Train Loss: 0.0450, Val Loss: 0.1151\n",
      "Epoch [1005/4000] - Train Loss: 0.0458, Val Loss: 0.1144\n",
      "Epoch [1006/4000] - Train Loss: 0.0446, Val Loss: 0.1136\n",
      "Epoch [1007/4000] - Train Loss: 0.0445, Val Loss: 0.1152\n",
      "Epoch [1008/4000] - Train Loss: 0.0449, Val Loss: 0.1150\n",
      "Epoch [1009/4000] - Train Loss: 0.0455, Val Loss: 0.1144\n",
      "Epoch [1010/4000] - Train Loss: 0.0449, Val Loss: 0.1145\n",
      "Epoch [1011/4000] - Train Loss: 0.0444, Val Loss: 0.1159\n",
      "Epoch [1012/4000] - Train Loss: 0.0448, Val Loss: 0.1156\n",
      "Epoch [1013/4000] - Train Loss: 0.0455, Val Loss: 0.1150\n",
      "Epoch [1014/4000] - Train Loss: 0.0450, Val Loss: 0.1153\n",
      "Epoch [1015/4000] - Train Loss: 0.0452, Val Loss: 0.1143\n",
      "Epoch [1016/4000] - Train Loss: 0.0452, Val Loss: 0.1154\n",
      "Epoch [1017/4000] - Train Loss: 0.0454, Val Loss: 0.1154\n",
      "Epoch [1018/4000] - Train Loss: 0.0445, Val Loss: 0.1142\n",
      "Epoch [1019/4000] - Train Loss: 0.0450, Val Loss: 0.1152\n",
      "Epoch [1020/4000] - Train Loss: 0.0450, Val Loss: 0.1139\n",
      "Epoch [1021/4000] - Train Loss: 0.0452, Val Loss: 0.1139\n",
      "Epoch [1022/4000] - Train Loss: 0.0444, Val Loss: 0.1157\n",
      "Epoch [1023/4000] - Train Loss: 0.0455, Val Loss: 0.1148\n",
      "Epoch [1024/4000] - Train Loss: 0.0453, Val Loss: 0.1155\n",
      "Epoch [1025/4000] - Train Loss: 0.0456, Val Loss: 0.1153\n",
      "Epoch [1026/4000] - Train Loss: 0.0449, Val Loss: 0.1148\n",
      "Epoch [1027/4000] - Train Loss: 0.0451, Val Loss: 0.1162\n",
      "Epoch [1028/4000] - Train Loss: 0.0447, Val Loss: 0.1164\n",
      "Epoch [1029/4000] - Train Loss: 0.0444, Val Loss: 0.1146\n",
      "Epoch [1030/4000] - Train Loss: 0.0443, Val Loss: 0.1145\n",
      "Epoch [1031/4000] - Train Loss: 0.0478, Val Loss: 0.1153\n",
      "Epoch [1032/4000] - Train Loss: 0.0459, Val Loss: 0.1149\n",
      "Epoch [1033/4000] - Train Loss: 0.0455, Val Loss: 0.1141\n",
      "Epoch [1034/4000] - Train Loss: 0.0454, Val Loss: 0.1150\n",
      "Epoch [1035/4000] - Train Loss: 0.0452, Val Loss: 0.1143\n",
      "Epoch [1036/4000] - Train Loss: 0.0442, Val Loss: 0.1140\n",
      "Epoch [1037/4000] - Train Loss: 0.0453, Val Loss: 0.1139\n",
      "Epoch [1038/4000] - Train Loss: 0.0452, Val Loss: 0.1156\n",
      "Epoch [1039/4000] - Train Loss: 0.0445, Val Loss: 0.1152\n",
      "Epoch [1040/4000] - Train Loss: 0.0448, Val Loss: 0.1158\n",
      "Epoch [1041/4000] - Train Loss: 0.0450, Val Loss: 0.1140\n",
      "Epoch [1042/4000] - Train Loss: 0.0451, Val Loss: 0.1138\n",
      "Epoch [1043/4000] - Train Loss: 0.0462, Val Loss: 0.1144\n",
      "Epoch [1044/4000] - Train Loss: 0.0449, Val Loss: 0.1153\n",
      "Epoch [1045/4000] - Train Loss: 0.0451, Val Loss: 0.1147\n",
      "Epoch [1046/4000] - Train Loss: 0.0467, Val Loss: 0.1140\n",
      "Epoch [1047/4000] - Train Loss: 0.0455, Val Loss: 0.1154\n",
      "Epoch [1048/4000] - Train Loss: 0.0455, Val Loss: 0.1152\n",
      "Epoch [1049/4000] - Train Loss: 0.0463, Val Loss: 0.1145\n",
      "Epoch [1050/4000] - Train Loss: 0.0459, Val Loss: 0.1164\n",
      "Epoch [1051/4000] - Train Loss: 0.0459, Val Loss: 0.1188\n",
      "Epoch [1052/4000] - Train Loss: 0.0461, Val Loss: 0.1212\n",
      "Epoch [1053/4000] - Train Loss: 0.0513, Val Loss: 0.1169\n",
      "Epoch [1054/4000] - Train Loss: 0.0475, Val Loss: 0.1125\n",
      "Epoch [1055/4000] - Train Loss: 0.0464, Val Loss: 0.1130\n",
      "Epoch [1056/4000] - Train Loss: 0.0449, Val Loss: 0.1163\n",
      "Epoch [1057/4000] - Train Loss: 0.0442, Val Loss: 0.1144\n",
      "Epoch [1058/4000] - Train Loss: 0.0443, Val Loss: 0.1148\n",
      "Epoch [1059/4000] - Train Loss: 0.0441, Val Loss: 0.1140\n",
      "Epoch [1060/4000] - Train Loss: 0.0445, Val Loss: 0.1136\n",
      "Epoch [1061/4000] - Train Loss: 0.0448, Val Loss: 0.1137\n",
      "Epoch [1062/4000] - Train Loss: 0.0441, Val Loss: 0.1150\n",
      "Epoch [1063/4000] - Train Loss: 0.0447, Val Loss: 0.1152\n",
      "Epoch [1064/4000] - Train Loss: 0.0440, Val Loss: 0.1148\n",
      "Epoch [1065/4000] - Train Loss: 0.0443, Val Loss: 0.1147\n",
      "Epoch [1066/4000] - Train Loss: 0.0438, Val Loss: 0.1147\n",
      "Epoch [1067/4000] - Train Loss: 0.0449, Val Loss: 0.1167\n",
      "Epoch [1068/4000] - Train Loss: 0.0458, Val Loss: 0.1144\n",
      "Epoch [1069/4000] - Train Loss: 0.0445, Val Loss: 0.1161\n",
      "Epoch [1070/4000] - Train Loss: 0.0449, Val Loss: 0.1143\n",
      "Epoch [1071/4000] - Train Loss: 0.0466, Val Loss: 0.1150\n",
      "Epoch [1072/4000] - Train Loss: 0.0457, Val Loss: 0.1151\n",
      "Epoch [1073/4000] - Train Loss: 0.0453, Val Loss: 0.1138\n",
      "Epoch [1074/4000] - Train Loss: 0.0446, Val Loss: 0.1155\n",
      "Epoch [1075/4000] - Train Loss: 0.0445, Val Loss: 0.1143\n",
      "Epoch [1076/4000] - Train Loss: 0.0442, Val Loss: 0.1154\n",
      "Epoch [1077/4000] - Train Loss: 0.0441, Val Loss: 0.1144\n",
      "Epoch [1078/4000] - Train Loss: 0.0441, Val Loss: 0.1136\n",
      "Epoch [1079/4000] - Train Loss: 0.0443, Val Loss: 0.1150\n",
      "Epoch [1080/4000] - Train Loss: 0.0440, Val Loss: 0.1143\n",
      "Epoch [1081/4000] - Train Loss: 0.0450, Val Loss: 0.1157\n",
      "Epoch [1082/4000] - Train Loss: 0.0455, Val Loss: 0.1149\n",
      "Epoch [1083/4000] - Train Loss: 0.0447, Val Loss: 0.1152\n",
      "Epoch [1084/4000] - Train Loss: 0.0441, Val Loss: 0.1147\n",
      "Epoch [1085/4000] - Train Loss: 0.0440, Val Loss: 0.1140\n",
      "Epoch [1086/4000] - Train Loss: 0.0451, Val Loss: 0.1154\n",
      "Epoch [1087/4000] - Train Loss: 0.0448, Val Loss: 0.1141\n",
      "Epoch [1088/4000] - Train Loss: 0.0444, Val Loss: 0.1148\n",
      "Epoch [1089/4000] - Train Loss: 0.0451, Val Loss: 0.1141\n",
      "Epoch [1090/4000] - Train Loss: 0.0449, Val Loss: 0.1148\n",
      "Epoch [1091/4000] - Train Loss: 0.0450, Val Loss: 0.1145\n",
      "Epoch [1092/4000] - Train Loss: 0.0439, Val Loss: 0.1144\n",
      "Epoch [1093/4000] - Train Loss: 0.0436, Val Loss: 0.1145\n",
      "Epoch [1094/4000] - Train Loss: 0.0434, Val Loss: 0.1138\n",
      "Epoch [1095/4000] - Train Loss: 0.0442, Val Loss: 0.1144\n",
      "Epoch [1096/4000] - Train Loss: 0.0443, Val Loss: 0.1147\n",
      "Epoch [1097/4000] - Train Loss: 0.0434, Val Loss: 0.1139\n",
      "Epoch [1098/4000] - Train Loss: 0.0435, Val Loss: 0.1149\n",
      "Epoch [1099/4000] - Train Loss: 0.0433, Val Loss: 0.1134\n",
      "Epoch [1100/4000] - Train Loss: 0.0442, Val Loss: 0.1141\n",
      "Epoch [1101/4000] - Train Loss: 0.0443, Val Loss: 0.1131\n",
      "Epoch [1102/4000] - Train Loss: 0.0437, Val Loss: 0.1125\n",
      "Epoch [1103/4000] - Train Loss: 0.0445, Val Loss: 0.1144\n",
      "Epoch [1104/4000] - Train Loss: 0.0443, Val Loss: 0.1140\n",
      "Epoch [1105/4000] - Train Loss: 0.0446, Val Loss: 0.1143\n",
      "Epoch [1106/4000] - Train Loss: 0.0429, Val Loss: 0.1145\n",
      "Epoch [1107/4000] - Train Loss: 0.0438, Val Loss: 0.1148\n",
      "Epoch [1108/4000] - Train Loss: 0.0438, Val Loss: 0.1160\n",
      "Epoch [1109/4000] - Train Loss: 0.0443, Val Loss: 0.1141\n",
      "Epoch [1110/4000] - Train Loss: 0.0453, Val Loss: 0.1135\n",
      "Epoch [1111/4000] - Train Loss: 0.0445, Val Loss: 0.1137\n",
      "Epoch [1112/4000] - Train Loss: 0.0435, Val Loss: 0.1140\n",
      "Epoch [1113/4000] - Train Loss: 0.0436, Val Loss: 0.1150\n",
      "Epoch [1114/4000] - Train Loss: 0.0436, Val Loss: 0.1147\n",
      "Epoch [1115/4000] - Train Loss: 0.0439, Val Loss: 0.1142\n",
      "Epoch [1116/4000] - Train Loss: 0.0439, Val Loss: 0.1148\n",
      "Epoch [1117/4000] - Train Loss: 0.0432, Val Loss: 0.1147\n",
      "Epoch [1118/4000] - Train Loss: 0.0430, Val Loss: 0.1148\n",
      "Epoch [1119/4000] - Train Loss: 0.0436, Val Loss: 0.1150\n",
      "Epoch [1120/4000] - Train Loss: 0.0438, Val Loss: 0.1145\n",
      "Epoch [1121/4000] - Train Loss: 0.0439, Val Loss: 0.1124\n",
      "Epoch [1122/4000] - Train Loss: 0.0432, Val Loss: 0.1143\n",
      "Epoch [1123/4000] - Train Loss: 0.0442, Val Loss: 0.1146\n",
      "Epoch [1124/4000] - Train Loss: 0.0447, Val Loss: 0.1163\n",
      "Epoch [1125/4000] - Train Loss: 0.0436, Val Loss: 0.1148\n",
      "Epoch [1126/4000] - Train Loss: 0.0432, Val Loss: 0.1162\n",
      "Epoch [1127/4000] - Train Loss: 0.0439, Val Loss: 0.1153\n",
      "Epoch [1128/4000] - Train Loss: 0.0449, Val Loss: 0.1158\n",
      "Epoch [1129/4000] - Train Loss: 0.0453, Val Loss: 0.1168\n",
      "Epoch [1130/4000] - Train Loss: 0.0450, Val Loss: 0.1155\n",
      "Epoch [1131/4000] - Train Loss: 0.0441, Val Loss: 0.1145\n",
      "Epoch [1132/4000] - Train Loss: 0.0447, Val Loss: 0.1137\n",
      "Epoch [1133/4000] - Train Loss: 0.0439, Val Loss: 0.1140\n",
      "Epoch [1134/4000] - Train Loss: 0.0434, Val Loss: 0.1139\n",
      "Epoch [1135/4000] - Train Loss: 0.0428, Val Loss: 0.1139\n",
      "Epoch [1136/4000] - Train Loss: 0.0435, Val Loss: 0.1141\n",
      "Epoch [1137/4000] - Train Loss: 0.0435, Val Loss: 0.1150\n",
      "Epoch [1138/4000] - Train Loss: 0.0428, Val Loss: 0.1146\n",
      "Epoch [1139/4000] - Train Loss: 0.0429, Val Loss: 0.1137\n",
      "Epoch [1140/4000] - Train Loss: 0.0435, Val Loss: 0.1139\n",
      "Epoch [1141/4000] - Train Loss: 0.0430, Val Loss: 0.1145\n",
      "Epoch [1142/4000] - Train Loss: 0.0436, Val Loss: 0.1152\n",
      "Epoch [1143/4000] - Train Loss: 0.0445, Val Loss: 0.1163\n",
      "Epoch [1144/4000] - Train Loss: 0.0437, Val Loss: 0.1174\n",
      "Epoch [1145/4000] - Train Loss: 0.0441, Val Loss: 0.1173\n",
      "Epoch [1146/4000] - Train Loss: 0.0450, Val Loss: 0.1165\n",
      "Epoch [1147/4000] - Train Loss: 0.0445, Val Loss: 0.1146\n",
      "Epoch [1148/4000] - Train Loss: 0.0437, Val Loss: 0.1139\n",
      "Epoch [1149/4000] - Train Loss: 0.0430, Val Loss: 0.1160\n",
      "Epoch [1150/4000] - Train Loss: 0.0429, Val Loss: 0.1146\n",
      "Epoch [1151/4000] - Train Loss: 0.0433, Val Loss: 0.1156\n",
      "Epoch [1152/4000] - Train Loss: 0.0440, Val Loss: 0.1158\n",
      "Epoch [1153/4000] - Train Loss: 0.0446, Val Loss: 0.1147\n",
      "Epoch [1154/4000] - Train Loss: 0.0468, Val Loss: 0.1187\n",
      "Epoch [1155/4000] - Train Loss: 0.0444, Val Loss: 0.1138\n",
      "Epoch [1156/4000] - Train Loss: 0.0442, Val Loss: 0.1155\n",
      "Epoch [1157/4000] - Train Loss: 0.0441, Val Loss: 0.1162\n",
      "Epoch [1158/4000] - Train Loss: 0.0434, Val Loss: 0.1151\n",
      "Epoch [1159/4000] - Train Loss: 0.0431, Val Loss: 0.1150\n",
      "Epoch [1160/4000] - Train Loss: 0.0437, Val Loss: 0.1143\n",
      "Epoch [1161/4000] - Train Loss: 0.0445, Val Loss: 0.1154\n",
      "Epoch [1162/4000] - Train Loss: 0.0446, Val Loss: 0.1160\n",
      "Epoch [1163/4000] - Train Loss: 0.0472, Val Loss: 0.1178\n",
      "Epoch [1164/4000] - Train Loss: 0.0469, Val Loss: 0.1163\n",
      "Epoch [1165/4000] - Train Loss: 0.0445, Val Loss: 0.1186\n",
      "Epoch [1166/4000] - Train Loss: 0.0433, Val Loss: 0.1161\n",
      "Epoch [1167/4000] - Train Loss: 0.0436, Val Loss: 0.1151\n",
      "Epoch [1168/4000] - Train Loss: 0.0439, Val Loss: 0.1169\n",
      "Epoch [1169/4000] - Train Loss: 0.0432, Val Loss: 0.1144\n",
      "Epoch [1170/4000] - Train Loss: 0.0436, Val Loss: 0.1166\n",
      "Epoch [1171/4000] - Train Loss: 0.0443, Val Loss: 0.1160\n",
      "Epoch [1172/4000] - Train Loss: 0.0430, Val Loss: 0.1146\n",
      "Epoch [1173/4000] - Train Loss: 0.0437, Val Loss: 0.1148\n",
      "Epoch [1174/4000] - Train Loss: 0.0437, Val Loss: 0.1145\n",
      "Epoch [1175/4000] - Train Loss: 0.0438, Val Loss: 0.1158\n",
      "Epoch [1176/4000] - Train Loss: 0.0433, Val Loss: 0.1145\n",
      "Epoch [1177/4000] - Train Loss: 0.0439, Val Loss: 0.1156\n",
      "Epoch [1178/4000] - Train Loss: 0.0428, Val Loss: 0.1143\n",
      "Epoch [1179/4000] - Train Loss: 0.0433, Val Loss: 0.1147\n",
      "Epoch [1180/4000] - Train Loss: 0.0425, Val Loss: 0.1152\n",
      "Epoch [1181/4000] - Train Loss: 0.0427, Val Loss: 0.1157\n",
      "Epoch [1182/4000] - Train Loss: 0.0434, Val Loss: 0.1136\n",
      "Epoch [1183/4000] - Train Loss: 0.0428, Val Loss: 0.1142\n",
      "Epoch [1184/4000] - Train Loss: 0.0427, Val Loss: 0.1142\n",
      "Epoch [1185/4000] - Train Loss: 0.0428, Val Loss: 0.1149\n",
      "Epoch [1186/4000] - Train Loss: 0.0430, Val Loss: 0.1153\n",
      "Epoch [1187/4000] - Train Loss: 0.0435, Val Loss: 0.1148\n",
      "Epoch [1188/4000] - Train Loss: 0.0438, Val Loss: 0.1134\n",
      "Epoch [1189/4000] - Train Loss: 0.0443, Val Loss: 0.1133\n",
      "Epoch [1190/4000] - Train Loss: 0.0433, Val Loss: 0.1162\n",
      "Epoch [1191/4000] - Train Loss: 0.0428, Val Loss: 0.1144\n",
      "Epoch [1192/4000] - Train Loss: 0.0431, Val Loss: 0.1143\n",
      "Epoch [1193/4000] - Train Loss: 0.0434, Val Loss: 0.1155\n",
      "Epoch [1194/4000] - Train Loss: 0.0435, Val Loss: 0.1158\n",
      "Epoch [1195/4000] - Train Loss: 0.0443, Val Loss: 0.1147\n",
      "Epoch [1196/4000] - Train Loss: 0.0455, Val Loss: 0.1154\n",
      "Epoch [1197/4000] - Train Loss: 0.0468, Val Loss: 0.1157\n",
      "Epoch [1198/4000] - Train Loss: 0.0454, Val Loss: 0.1159\n",
      "Epoch [1199/4000] - Train Loss: 0.0439, Val Loss: 0.1147\n",
      "Epoch [1200/4000] - Train Loss: 0.0434, Val Loss: 0.1157\n",
      "Epoch [1201/4000] - Train Loss: 0.0446, Val Loss: 0.1177\n",
      "Epoch [1202/4000] - Train Loss: 0.0438, Val Loss: 0.1146\n",
      "Epoch [1203/4000] - Train Loss: 0.0448, Val Loss: 0.1147\n",
      "Epoch [1204/4000] - Train Loss: 0.0437, Val Loss: 0.1162\n",
      "Epoch [1205/4000] - Train Loss: 0.0427, Val Loss: 0.1154\n",
      "Epoch [1206/4000] - Train Loss: 0.0425, Val Loss: 0.1136\n",
      "Epoch [1207/4000] - Train Loss: 0.0425, Val Loss: 0.1149\n",
      "Epoch [1208/4000] - Train Loss: 0.0434, Val Loss: 0.1166\n",
      "Epoch [1209/4000] - Train Loss: 0.0430, Val Loss: 0.1135\n",
      "Epoch [1210/4000] - Train Loss: 0.0434, Val Loss: 0.1159\n",
      "Epoch [1211/4000] - Train Loss: 0.0432, Val Loss: 0.1133\n",
      "Epoch [1212/4000] - Train Loss: 0.0430, Val Loss: 0.1140\n",
      "Epoch [1213/4000] - Train Loss: 0.0437, Val Loss: 0.1150\n",
      "Epoch [1214/4000] - Train Loss: 0.0437, Val Loss: 0.1128\n",
      "Epoch [1215/4000] - Train Loss: 0.0426, Val Loss: 0.1135\n",
      "Epoch [1216/4000] - Train Loss: 0.0428, Val Loss: 0.1146\n",
      "Epoch [1217/4000] - Train Loss: 0.0428, Val Loss: 0.1134\n",
      "Epoch [1218/4000] - Train Loss: 0.0423, Val Loss: 0.1138\n",
      "Epoch [1219/4000] - Train Loss: 0.0430, Val Loss: 0.1136\n",
      "Epoch [1220/4000] - Train Loss: 0.0429, Val Loss: 0.1129\n",
      "Epoch [1221/4000] - Train Loss: 0.0426, Val Loss: 0.1144\n",
      "Epoch [1222/4000] - Train Loss: 0.0432, Val Loss: 0.1137\n",
      "Epoch [1223/4000] - Train Loss: 0.0426, Val Loss: 0.1138\n",
      "Epoch [1224/4000] - Train Loss: 0.0418, Val Loss: 0.1143\n",
      "Epoch [1225/4000] - Train Loss: 0.0422, Val Loss: 0.1135\n",
      "Epoch [1226/4000] - Train Loss: 0.0421, Val Loss: 0.1143\n",
      "Epoch [1227/4000] - Train Loss: 0.0420, Val Loss: 0.1146\n",
      "Epoch [1228/4000] - Train Loss: 0.0413, Val Loss: 0.1144\n",
      "Epoch [1229/4000] - Train Loss: 0.0418, Val Loss: 0.1143\n",
      "Epoch [1230/4000] - Train Loss: 0.0425, Val Loss: 0.1139\n",
      "Epoch [1231/4000] - Train Loss: 0.0421, Val Loss: 0.1140\n",
      "Epoch [1232/4000] - Train Loss: 0.0418, Val Loss: 0.1124\n",
      "Epoch [1233/4000] - Train Loss: 0.0426, Val Loss: 0.1152\n",
      "Epoch [1234/4000] - Train Loss: 0.0436, Val Loss: 0.1134\n",
      "Epoch [1235/4000] - Train Loss: 0.0428, Val Loss: 0.1135\n",
      "Epoch [1236/4000] - Train Loss: 0.0436, Val Loss: 0.1142\n",
      "Epoch [1237/4000] - Train Loss: 0.0431, Val Loss: 0.1127\n",
      "Epoch [1238/4000] - Train Loss: 0.0426, Val Loss: 0.1147\n",
      "Epoch [1239/4000] - Train Loss: 0.0426, Val Loss: 0.1158\n",
      "Epoch [1240/4000] - Train Loss: 0.0428, Val Loss: 0.1136\n",
      "Epoch [1241/4000] - Train Loss: 0.0428, Val Loss: 0.1125\n",
      "Epoch [1242/4000] - Train Loss: 0.0430, Val Loss: 0.1157\n",
      "Epoch [1243/4000] - Train Loss: 0.0431, Val Loss: 0.1139\n",
      "Epoch [1244/4000] - Train Loss: 0.0434, Val Loss: 0.1127\n",
      "Epoch [1245/4000] - Train Loss: 0.0427, Val Loss: 0.1131\n",
      "Epoch [1246/4000] - Train Loss: 0.0434, Val Loss: 0.1138\n",
      "Epoch [1247/4000] - Train Loss: 0.0423, Val Loss: 0.1139\n",
      "Epoch [1248/4000] - Train Loss: 0.0421, Val Loss: 0.1127\n",
      "Epoch [1249/4000] - Train Loss: 0.0421, Val Loss: 0.1130\n",
      "Epoch [1250/4000] - Train Loss: 0.0432, Val Loss: 0.1140\n",
      "Epoch [1251/4000] - Train Loss: 0.0439, Val Loss: 0.1160\n",
      "Epoch [1252/4000] - Train Loss: 0.0437, Val Loss: 0.1144\n",
      "Epoch [1253/4000] - Train Loss: 0.0429, Val Loss: 0.1124\n",
      "Epoch [1254/4000] - Train Loss: 0.0430, Val Loss: 0.1151\n",
      "Epoch [1255/4000] - Train Loss: 0.0428, Val Loss: 0.1154\n",
      "Epoch [1256/4000] - Train Loss: 0.0419, Val Loss: 0.1145\n",
      "Epoch [1257/4000] - Train Loss: 0.0422, Val Loss: 0.1150\n",
      "Epoch [1258/4000] - Train Loss: 0.0423, Val Loss: 0.1149\n",
      "Epoch [1259/4000] - Train Loss: 0.0417, Val Loss: 0.1143\n",
      "Epoch [1260/4000] - Train Loss: 0.0422, Val Loss: 0.1147\n",
      "Epoch [1261/4000] - Train Loss: 0.0427, Val Loss: 0.1144\n",
      "Epoch [1262/4000] - Train Loss: 0.0435, Val Loss: 0.1142\n",
      "Epoch [1263/4000] - Train Loss: 0.0418, Val Loss: 0.1148\n",
      "Epoch [1264/4000] - Train Loss: 0.0422, Val Loss: 0.1140\n",
      "Epoch [1265/4000] - Train Loss: 0.0423, Val Loss: 0.1149\n",
      "Epoch [1266/4000] - Train Loss: 0.0424, Val Loss: 0.1126\n",
      "Epoch [1267/4000] - Train Loss: 0.0413, Val Loss: 0.1139\n",
      "Epoch [1268/4000] - Train Loss: 0.0423, Val Loss: 0.1139\n",
      "Epoch [1269/4000] - Train Loss: 0.0433, Val Loss: 0.1135\n",
      "Epoch [1270/4000] - Train Loss: 0.0423, Val Loss: 0.1145\n",
      "Epoch [1271/4000] - Train Loss: 0.0418, Val Loss: 0.1142\n",
      "Epoch [1272/4000] - Train Loss: 0.0424, Val Loss: 0.1155\n",
      "Epoch [1273/4000] - Train Loss: 0.0428, Val Loss: 0.1175\n",
      "Epoch [1274/4000] - Train Loss: 0.0441, Val Loss: 0.1149\n",
      "Epoch [1275/4000] - Train Loss: 0.0441, Val Loss: 0.1143\n",
      "Epoch [1276/4000] - Train Loss: 0.0428, Val Loss: 0.1148\n",
      "Epoch [1277/4000] - Train Loss: 0.0436, Val Loss: 0.1154\n",
      "Epoch [1278/4000] - Train Loss: 0.0430, Val Loss: 0.1152\n",
      "Epoch [1279/4000] - Train Loss: 0.0421, Val Loss: 0.1143\n",
      "Epoch [1280/4000] - Train Loss: 0.0417, Val Loss: 0.1139\n",
      "Epoch [1281/4000] - Train Loss: 0.0421, Val Loss: 0.1147\n",
      "Epoch [1282/4000] - Train Loss: 0.0416, Val Loss: 0.1140\n",
      "Epoch [1283/4000] - Train Loss: 0.0418, Val Loss: 0.1137\n",
      "Epoch [1284/4000] - Train Loss: 0.0419, Val Loss: 0.1130\n",
      "Epoch [1285/4000] - Train Loss: 0.0424, Val Loss: 0.1130\n",
      "Epoch [1286/4000] - Train Loss: 0.0421, Val Loss: 0.1130\n",
      "Epoch [1287/4000] - Train Loss: 0.0418, Val Loss: 0.1117\n",
      "Epoch [1288/4000] - Train Loss: 0.0434, Val Loss: 0.1144\n",
      "Epoch [1289/4000] - Train Loss: 0.0442, Val Loss: 0.1154\n",
      "Epoch [1290/4000] - Train Loss: 0.0464, Val Loss: 0.1151\n",
      "Epoch [1291/4000] - Train Loss: 0.0458, Val Loss: 0.1148\n",
      "Epoch [1292/4000] - Train Loss: 0.0450, Val Loss: 0.1145\n",
      "Epoch [1293/4000] - Train Loss: 0.0430, Val Loss: 0.1143\n",
      "Epoch [1294/4000] - Train Loss: 0.0423, Val Loss: 0.1139\n",
      "Epoch [1295/4000] - Train Loss: 0.0419, Val Loss: 0.1134\n",
      "Epoch [1296/4000] - Train Loss: 0.0421, Val Loss: 0.1151\n",
      "Epoch [1297/4000] - Train Loss: 0.0419, Val Loss: 0.1147\n",
      "Epoch [1298/4000] - Train Loss: 0.0412, Val Loss: 0.1143\n",
      "Epoch [1299/4000] - Train Loss: 0.0412, Val Loss: 0.1138\n",
      "Epoch [1300/4000] - Train Loss: 0.0412, Val Loss: 0.1141\n",
      "Epoch [1301/4000] - Train Loss: 0.0415, Val Loss: 0.1134\n",
      "Epoch [1302/4000] - Train Loss: 0.0412, Val Loss: 0.1142\n",
      "Epoch [1303/4000] - Train Loss: 0.0412, Val Loss: 0.1129\n",
      "Epoch [1304/4000] - Train Loss: 0.0418, Val Loss: 0.1140\n",
      "Epoch [1305/4000] - Train Loss: 0.0411, Val Loss: 0.1147\n",
      "Epoch [1306/4000] - Train Loss: 0.0411, Val Loss: 0.1147\n",
      "Epoch [1307/4000] - Train Loss: 0.0420, Val Loss: 0.1139\n",
      "Epoch [1308/4000] - Train Loss: 0.0419, Val Loss: 0.1146\n",
      "Epoch [1309/4000] - Train Loss: 0.0421, Val Loss: 0.1149\n",
      "Epoch [1310/4000] - Train Loss: 0.0417, Val Loss: 0.1147\n",
      "Epoch [1311/4000] - Train Loss: 0.0415, Val Loss: 0.1152\n",
      "Epoch [1312/4000] - Train Loss: 0.0410, Val Loss: 0.1149\n",
      "Epoch [1313/4000] - Train Loss: 0.0418, Val Loss: 0.1151\n",
      "Epoch [1314/4000] - Train Loss: 0.0412, Val Loss: 0.1148\n",
      "Epoch [1315/4000] - Train Loss: 0.0410, Val Loss: 0.1150\n",
      "Epoch [1316/4000] - Train Loss: 0.0415, Val Loss: 0.1138\n",
      "Epoch [1317/4000] - Train Loss: 0.0416, Val Loss: 0.1148\n",
      "Epoch [1318/4000] - Train Loss: 0.0413, Val Loss: 0.1145\n",
      "Epoch [1319/4000] - Train Loss: 0.0416, Val Loss: 0.1142\n",
      "Epoch [1320/4000] - Train Loss: 0.0410, Val Loss: 0.1142\n",
      "Epoch [1321/4000] - Train Loss: 0.0420, Val Loss: 0.1137\n",
      "Epoch [1322/4000] - Train Loss: 0.0418, Val Loss: 0.1159\n",
      "Epoch [1323/4000] - Train Loss: 0.0411, Val Loss: 0.1161\n",
      "Epoch [1324/4000] - Train Loss: 0.0410, Val Loss: 0.1145\n",
      "Epoch [1325/4000] - Train Loss: 0.0412, Val Loss: 0.1152\n",
      "Epoch [1326/4000] - Train Loss: 0.0417, Val Loss: 0.1162\n",
      "Epoch [1327/4000] - Train Loss: 0.0416, Val Loss: 0.1137\n",
      "Epoch [1328/4000] - Train Loss: 0.0414, Val Loss: 0.1151\n",
      "Epoch [1329/4000] - Train Loss: 0.0411, Val Loss: 0.1139\n",
      "Epoch [1330/4000] - Train Loss: 0.0416, Val Loss: 0.1125\n",
      "Epoch [1331/4000] - Train Loss: 0.0418, Val Loss: 0.1149\n",
      "Epoch [1332/4000] - Train Loss: 0.0413, Val Loss: 0.1130\n",
      "Epoch [1333/4000] - Train Loss: 0.0427, Val Loss: 0.1128\n",
      "Epoch [1334/4000] - Train Loss: 0.0421, Val Loss: 0.1140\n",
      "Epoch [1335/4000] - Train Loss: 0.0415, Val Loss: 0.1145\n",
      "Epoch [1336/4000] - Train Loss: 0.0411, Val Loss: 0.1144\n",
      "Epoch [1337/4000] - Train Loss: 0.0418, Val Loss: 0.1138\n",
      "Epoch [1338/4000] - Train Loss: 0.0424, Val Loss: 0.1144\n",
      "Epoch [1339/4000] - Train Loss: 0.0417, Val Loss: 0.1141\n",
      "Epoch [1340/4000] - Train Loss: 0.0410, Val Loss: 0.1145\n",
      "Epoch [1341/4000] - Train Loss: 0.0411, Val Loss: 0.1147\n",
      "Epoch [1342/4000] - Train Loss: 0.0415, Val Loss: 0.1156\n",
      "Epoch [1343/4000] - Train Loss: 0.0412, Val Loss: 0.1138\n",
      "Epoch [1344/4000] - Train Loss: 0.0411, Val Loss: 0.1146\n",
      "Epoch [1345/4000] - Train Loss: 0.0413, Val Loss: 0.1146\n",
      "Epoch [1346/4000] - Train Loss: 0.0416, Val Loss: 0.1151\n",
      "Epoch [1347/4000] - Train Loss: 0.0418, Val Loss: 0.1146\n",
      "Epoch [1348/4000] - Train Loss: 0.0409, Val Loss: 0.1141\n",
      "Epoch [1349/4000] - Train Loss: 0.0426, Val Loss: 0.1140\n",
      "Epoch [1350/4000] - Train Loss: 0.0423, Val Loss: 0.1131\n",
      "Epoch [1351/4000] - Train Loss: 0.0420, Val Loss: 0.1142\n",
      "Epoch [1352/4000] - Train Loss: 0.0422, Val Loss: 0.1152\n",
      "Epoch [1353/4000] - Train Loss: 0.0414, Val Loss: 0.1149\n",
      "Epoch [1354/4000] - Train Loss: 0.0416, Val Loss: 0.1156\n",
      "Epoch [1355/4000] - Train Loss: 0.0413, Val Loss: 0.1141\n",
      "Epoch [1356/4000] - Train Loss: 0.0411, Val Loss: 0.1153\n",
      "Epoch [1357/4000] - Train Loss: 0.0415, Val Loss: 0.1164\n",
      "Epoch [1358/4000] - Train Loss: 0.0418, Val Loss: 0.1136\n",
      "Epoch [1359/4000] - Train Loss: 0.0409, Val Loss: 0.1135\n",
      "Epoch [1360/4000] - Train Loss: 0.0411, Val Loss: 0.1147\n",
      "Epoch [1361/4000] - Train Loss: 0.0407, Val Loss: 0.1145\n",
      "Epoch [1362/4000] - Train Loss: 0.0415, Val Loss: 0.1153\n",
      "Epoch [1363/4000] - Train Loss: 0.0419, Val Loss: 0.1149\n",
      "Epoch [1364/4000] - Train Loss: 0.0408, Val Loss: 0.1148\n",
      "Epoch [1365/4000] - Train Loss: 0.0409, Val Loss: 0.1139\n",
      "Epoch [1366/4000] - Train Loss: 0.0401, Val Loss: 0.1144\n",
      "Epoch [1367/4000] - Train Loss: 0.0413, Val Loss: 0.1137\n",
      "Epoch [1368/4000] - Train Loss: 0.0403, Val Loss: 0.1156\n",
      "Epoch [1369/4000] - Train Loss: 0.0407, Val Loss: 0.1143\n",
      "Epoch [1370/4000] - Train Loss: 0.0412, Val Loss: 0.1140\n",
      "Epoch [1371/4000] - Train Loss: 0.0411, Val Loss: 0.1143\n",
      "Epoch [1372/4000] - Train Loss: 0.0412, Val Loss: 0.1153\n",
      "Epoch [1373/4000] - Train Loss: 0.0406, Val Loss: 0.1139\n",
      "Epoch [1374/4000] - Train Loss: 0.0403, Val Loss: 0.1161\n",
      "Epoch [1375/4000] - Train Loss: 0.0417, Val Loss: 0.1179\n",
      "Epoch [1376/4000] - Train Loss: 0.0409, Val Loss: 0.1164\n",
      "Epoch [1377/4000] - Train Loss: 0.0431, Val Loss: 0.1151\n",
      "Epoch [1378/4000] - Train Loss: 0.0432, Val Loss: 0.1133\n",
      "Epoch [1379/4000] - Train Loss: 0.0418, Val Loss: 0.1170\n",
      "Epoch [1380/4000] - Train Loss: 0.0424, Val Loss: 0.1152\n",
      "Epoch [1381/4000] - Train Loss: 0.0418, Val Loss: 0.1146\n",
      "Epoch [1382/4000] - Train Loss: 0.0421, Val Loss: 0.1180\n",
      "Epoch [1383/4000] - Train Loss: 0.0414, Val Loss: 0.1146\n",
      "Epoch [1384/4000] - Train Loss: 0.0414, Val Loss: 0.1149\n",
      "Epoch [1385/4000] - Train Loss: 0.0414, Val Loss: 0.1154\n",
      "Epoch [1386/4000] - Train Loss: 0.0417, Val Loss: 0.1152\n",
      "Epoch [1387/4000] - Train Loss: 0.0415, Val Loss: 0.1158\n",
      "Epoch [1388/4000] - Train Loss: 0.0408, Val Loss: 0.1147\n",
      "Epoch [1389/4000] - Train Loss: 0.0418, Val Loss: 0.1142\n",
      "Epoch [1390/4000] - Train Loss: 0.0413, Val Loss: 0.1131\n",
      "Epoch [1391/4000] - Train Loss: 0.0420, Val Loss: 0.1151\n",
      "Epoch [1392/4000] - Train Loss: 0.0421, Val Loss: 0.1180\n",
      "Epoch [1393/4000] - Train Loss: 0.0420, Val Loss: 0.1182\n",
      "Epoch [1394/4000] - Train Loss: 0.0425, Val Loss: 0.1195\n",
      "Epoch [1395/4000] - Train Loss: 0.0428, Val Loss: 0.1155\n",
      "Epoch [1396/4000] - Train Loss: 0.0417, Val Loss: 0.1140\n",
      "Epoch [1397/4000] - Train Loss: 0.0407, Val Loss: 0.1151\n",
      "Epoch [1398/4000] - Train Loss: 0.0406, Val Loss: 0.1145\n",
      "Epoch [1399/4000] - Train Loss: 0.0413, Val Loss: 0.1156\n",
      "Epoch [1400/4000] - Train Loss: 0.0413, Val Loss: 0.1151\n",
      "Epoch [1401/4000] - Train Loss: 0.0404, Val Loss: 0.1167\n",
      "Epoch [1402/4000] - Train Loss: 0.0409, Val Loss: 0.1140\n",
      "Epoch [1403/4000] - Train Loss: 0.0410, Val Loss: 0.1162\n",
      "Epoch [1404/4000] - Train Loss: 0.0409, Val Loss: 0.1146\n",
      "Epoch [1405/4000] - Train Loss: 0.0409, Val Loss: 0.1156\n",
      "Epoch [1406/4000] - Train Loss: 0.0400, Val Loss: 0.1144\n",
      "Epoch [1407/4000] - Train Loss: 0.0404, Val Loss: 0.1138\n",
      "Epoch [1408/4000] - Train Loss: 0.0409, Val Loss: 0.1137\n",
      "Epoch [1409/4000] - Train Loss: 0.0404, Val Loss: 0.1159\n",
      "Epoch [1410/4000] - Train Loss: 0.0412, Val Loss: 0.1138\n",
      "Epoch [1411/4000] - Train Loss: 0.0410, Val Loss: 0.1136\n",
      "Epoch [1412/4000] - Train Loss: 0.0413, Val Loss: 0.1153\n",
      "Epoch [1413/4000] - Train Loss: 0.0414, Val Loss: 0.1155\n",
      "Epoch [1414/4000] - Train Loss: 0.0408, Val Loss: 0.1158\n",
      "Epoch [1415/4000] - Train Loss: 0.0415, Val Loss: 0.1156\n",
      "Epoch [1416/4000] - Train Loss: 0.0415, Val Loss: 0.1153\n",
      "Epoch [1417/4000] - Train Loss: 0.0415, Val Loss: 0.1196\n",
      "Epoch [1418/4000] - Train Loss: 0.0428, Val Loss: 0.1160\n",
      "Epoch [1419/4000] - Train Loss: 0.0423, Val Loss: 0.1170\n",
      "Epoch [1420/4000] - Train Loss: 0.0417, Val Loss: 0.1168\n",
      "Epoch [1421/4000] - Train Loss: 0.0413, Val Loss: 0.1149\n",
      "Epoch [1422/4000] - Train Loss: 0.0427, Val Loss: 0.1150\n",
      "Epoch [1423/4000] - Train Loss: 0.0419, Val Loss: 0.1165\n",
      "Epoch [1424/4000] - Train Loss: 0.0406, Val Loss: 0.1157\n",
      "Epoch [1425/4000] - Train Loss: 0.0415, Val Loss: 0.1163\n",
      "Epoch [1426/4000] - Train Loss: 0.0424, Val Loss: 0.1134\n",
      "Epoch [1427/4000] - Train Loss: 0.0415, Val Loss: 0.1153\n",
      "Epoch [1428/4000] - Train Loss: 0.0414, Val Loss: 0.1165\n",
      "Epoch [1429/4000] - Train Loss: 0.0413, Val Loss: 0.1149\n",
      "Epoch [1430/4000] - Train Loss: 0.0407, Val Loss: 0.1143\n",
      "Epoch [1431/4000] - Train Loss: 0.0411, Val Loss: 0.1173\n",
      "Epoch [1432/4000] - Train Loss: 0.0415, Val Loss: 0.1148\n",
      "Epoch [1433/4000] - Train Loss: 0.0419, Val Loss: 0.1150\n",
      "Epoch [1434/4000] - Train Loss: 0.0407, Val Loss: 0.1176\n",
      "Epoch [1435/4000] - Train Loss: 0.0410, Val Loss: 0.1157\n",
      "Epoch [1436/4000] - Train Loss: 0.0410, Val Loss: 0.1139\n",
      "Epoch [1437/4000] - Train Loss: 0.0404, Val Loss: 0.1150\n",
      "Epoch [1438/4000] - Train Loss: 0.0403, Val Loss: 0.1158\n",
      "Epoch [1439/4000] - Train Loss: 0.0412, Val Loss: 0.1145\n",
      "Epoch [1440/4000] - Train Loss: 0.0404, Val Loss: 0.1152\n",
      "Epoch [1441/4000] - Train Loss: 0.0399, Val Loss: 0.1149\n",
      "Epoch [1442/4000] - Train Loss: 0.0394, Val Loss: 0.1157\n",
      "Epoch [1443/4000] - Train Loss: 0.0391, Val Loss: 0.1149\n",
      "Epoch [1444/4000] - Train Loss: 0.0399, Val Loss: 0.1162\n",
      "Epoch [1445/4000] - Train Loss: 0.0405, Val Loss: 0.1151\n",
      "Epoch [1446/4000] - Train Loss: 0.0401, Val Loss: 0.1158\n",
      "Epoch [1447/4000] - Train Loss: 0.0401, Val Loss: 0.1158\n",
      "Epoch [1448/4000] - Train Loss: 0.0402, Val Loss: 0.1169\n",
      "Epoch [1449/4000] - Train Loss: 0.0406, Val Loss: 0.1164\n",
      "Epoch [1450/4000] - Train Loss: 0.0427, Val Loss: 0.1244\n",
      "Epoch [1451/4000] - Train Loss: 0.0448, Val Loss: 0.1176\n",
      "Epoch [1452/4000] - Train Loss: 0.0489, Val Loss: 0.1200\n",
      "Epoch [1453/4000] - Train Loss: 0.0447, Val Loss: 0.1161\n",
      "Epoch [1454/4000] - Train Loss: 0.0435, Val Loss: 0.1172\n",
      "Epoch [1455/4000] - Train Loss: 0.0426, Val Loss: 0.1167\n",
      "Epoch [1456/4000] - Train Loss: 0.0411, Val Loss: 0.1146\n",
      "Epoch [1457/4000] - Train Loss: 0.0400, Val Loss: 0.1166\n",
      "Epoch [1458/4000] - Train Loss: 0.0403, Val Loss: 0.1162\n",
      "Epoch [1459/4000] - Train Loss: 0.0412, Val Loss: 0.1149\n",
      "Epoch [1460/4000] - Train Loss: 0.0407, Val Loss: 0.1146\n",
      "Epoch [1461/4000] - Train Loss: 0.0403, Val Loss: 0.1152\n",
      "Epoch [1462/4000] - Train Loss: 0.0402, Val Loss: 0.1150\n",
      "Epoch [1463/4000] - Train Loss: 0.0398, Val Loss: 0.1151\n",
      "Epoch [1464/4000] - Train Loss: 0.0405, Val Loss: 0.1152\n",
      "Epoch [1465/4000] - Train Loss: 0.0398, Val Loss: 0.1148\n",
      "Epoch [1466/4000] - Train Loss: 0.0397, Val Loss: 0.1150\n",
      "Epoch [1467/4000] - Train Loss: 0.0401, Val Loss: 0.1143\n",
      "Epoch [1468/4000] - Train Loss: 0.0397, Val Loss: 0.1153\n",
      "Epoch [1469/4000] - Train Loss: 0.0402, Val Loss: 0.1158\n",
      "Epoch [1470/4000] - Train Loss: 0.0406, Val Loss: 0.1154\n",
      "Epoch [1471/4000] - Train Loss: 0.0401, Val Loss: 0.1153\n",
      "Epoch [1472/4000] - Train Loss: 0.0408, Val Loss: 0.1153\n",
      "Epoch [1473/4000] - Train Loss: 0.0406, Val Loss: 0.1152\n",
      "Epoch [1474/4000] - Train Loss: 0.0398, Val Loss: 0.1158\n",
      "Epoch [1475/4000] - Train Loss: 0.0396, Val Loss: 0.1155\n",
      "Epoch [1476/4000] - Train Loss: 0.0394, Val Loss: 0.1161\n",
      "Epoch [1477/4000] - Train Loss: 0.0401, Val Loss: 0.1159\n",
      "Epoch [1478/4000] - Train Loss: 0.0415, Val Loss: 0.1175\n",
      "Epoch [1479/4000] - Train Loss: 0.0409, Val Loss: 0.1162\n",
      "Epoch [1480/4000] - Train Loss: 0.0403, Val Loss: 0.1159\n",
      "Epoch [1481/4000] - Train Loss: 0.0404, Val Loss: 0.1164\n",
      "Epoch [1482/4000] - Train Loss: 0.0400, Val Loss: 0.1152\n",
      "Epoch [1483/4000] - Train Loss: 0.0394, Val Loss: 0.1157\n",
      "Epoch [1484/4000] - Train Loss: 0.0394, Val Loss: 0.1144\n",
      "Epoch [1485/4000] - Train Loss: 0.0401, Val Loss: 0.1144\n",
      "Epoch [1486/4000] - Train Loss: 0.0408, Val Loss: 0.1136\n",
      "Epoch [1487/4000] - Train Loss: 0.0420, Val Loss: 0.1162\n",
      "Epoch [1488/4000] - Train Loss: 0.0418, Val Loss: 0.1148\n",
      "Epoch [1489/4000] - Train Loss: 0.0407, Val Loss: 0.1171\n",
      "Epoch [1490/4000] - Train Loss: 0.0403, Val Loss: 0.1169\n",
      "Epoch [1491/4000] - Train Loss: 0.0416, Val Loss: 0.1185\n",
      "Epoch [1492/4000] - Train Loss: 0.0406, Val Loss: 0.1190\n",
      "Epoch [1493/4000] - Train Loss: 0.0405, Val Loss: 0.1151\n",
      "Epoch [1494/4000] - Train Loss: 0.0411, Val Loss: 0.1179\n",
      "Epoch [1495/4000] - Train Loss: 0.0408, Val Loss: 0.1144\n",
      "Epoch [1496/4000] - Train Loss: 0.0401, Val Loss: 0.1152\n",
      "Epoch [1497/4000] - Train Loss: 0.0397, Val Loss: 0.1139\n",
      "Epoch [1498/4000] - Train Loss: 0.0394, Val Loss: 0.1153\n",
      "Epoch [1499/4000] - Train Loss: 0.0392, Val Loss: 0.1142\n",
      "Epoch [1500/4000] - Train Loss: 0.0385, Val Loss: 0.1155\n",
      "Epoch [1501/4000] - Train Loss: 0.0394, Val Loss: 0.1164\n",
      "Epoch [1502/4000] - Train Loss: 0.0394, Val Loss: 0.1159\n",
      "Epoch [1503/4000] - Train Loss: 0.0401, Val Loss: 0.1176\n",
      "Epoch [1504/4000] - Train Loss: 0.0405, Val Loss: 0.1169\n",
      "Epoch [1505/4000] - Train Loss: 0.0411, Val Loss: 0.1169\n",
      "Epoch [1506/4000] - Train Loss: 0.0409, Val Loss: 0.1171\n",
      "Epoch [1507/4000] - Train Loss: 0.0410, Val Loss: 0.1170\n",
      "Epoch [1508/4000] - Train Loss: 0.0411, Val Loss: 0.1165\n",
      "Epoch [1509/4000] - Train Loss: 0.0407, Val Loss: 0.1192\n",
      "Epoch [1510/4000] - Train Loss: 0.0406, Val Loss: 0.1161\n",
      "Epoch [1511/4000] - Train Loss: 0.0398, Val Loss: 0.1173\n",
      "Epoch [1512/4000] - Train Loss: 0.0399, Val Loss: 0.1163\n",
      "Epoch [1513/4000] - Train Loss: 0.0394, Val Loss: 0.1148\n",
      "Epoch [1514/4000] - Train Loss: 0.0392, Val Loss: 0.1148\n",
      "Epoch [1515/4000] - Train Loss: 0.0393, Val Loss: 0.1156\n",
      "Epoch [1516/4000] - Train Loss: 0.0395, Val Loss: 0.1139\n",
      "Epoch [1517/4000] - Train Loss: 0.0394, Val Loss: 0.1167\n",
      "Epoch [1518/4000] - Train Loss: 0.0391, Val Loss: 0.1145\n",
      "Epoch [1519/4000] - Train Loss: 0.0394, Val Loss: 0.1143\n",
      "Epoch [1520/4000] - Train Loss: 0.0398, Val Loss: 0.1152\n",
      "Epoch [1521/4000] - Train Loss: 0.0399, Val Loss: 0.1153\n",
      "Epoch [1522/4000] - Train Loss: 0.0395, Val Loss: 0.1156\n",
      "Epoch [1523/4000] - Train Loss: 0.0393, Val Loss: 0.1151\n",
      "Epoch [1524/4000] - Train Loss: 0.0392, Val Loss: 0.1155\n",
      "Epoch [1525/4000] - Train Loss: 0.0391, Val Loss: 0.1149\n",
      "Epoch [1526/4000] - Train Loss: 0.0396, Val Loss: 0.1176\n",
      "Epoch [1527/4000] - Train Loss: 0.0411, Val Loss: 0.1179\n",
      "Epoch [1528/4000] - Train Loss: 0.0407, Val Loss: 0.1195\n",
      "Epoch [1529/4000] - Train Loss: 0.0416, Val Loss: 0.1193\n",
      "Epoch [1530/4000] - Train Loss: 0.0409, Val Loss: 0.1189\n",
      "Epoch [1531/4000] - Train Loss: 0.0399, Val Loss: 0.1169\n",
      "Epoch [1532/4000] - Train Loss: 0.0396, Val Loss: 0.1157\n",
      "Epoch [1533/4000] - Train Loss: 0.0391, Val Loss: 0.1169\n",
      "Epoch [1534/4000] - Train Loss: 0.0397, Val Loss: 0.1165\n",
      "Epoch [1535/4000] - Train Loss: 0.0410, Val Loss: 0.1163\n",
      "Epoch [1536/4000] - Train Loss: 0.0406, Val Loss: 0.1144\n",
      "Epoch [1537/4000] - Train Loss: 0.0411, Val Loss: 0.1135\n",
      "Epoch [1538/4000] - Train Loss: 0.0398, Val Loss: 0.1147\n",
      "Epoch [1539/4000] - Train Loss: 0.0397, Val Loss: 0.1145\n",
      "Epoch [1540/4000] - Train Loss: 0.0402, Val Loss: 0.1172\n",
      "Epoch [1541/4000] - Train Loss: 0.0401, Val Loss: 0.1182\n",
      "Epoch [1542/4000] - Train Loss: 0.0404, Val Loss: 0.1200\n",
      "Epoch [1543/4000] - Train Loss: 0.0411, Val Loss: 0.1196\n",
      "Epoch [1544/4000] - Train Loss: 0.0416, Val Loss: 0.1231\n",
      "Epoch [1545/4000] - Train Loss: 0.0410, Val Loss: 0.1157\n",
      "Epoch [1546/4000] - Train Loss: 0.0407, Val Loss: 0.1160\n",
      "Epoch [1547/4000] - Train Loss: 0.0405, Val Loss: 0.1180\n",
      "Epoch [1548/4000] - Train Loss: 0.0397, Val Loss: 0.1163\n",
      "Epoch [1549/4000] - Train Loss: 0.0388, Val Loss: 0.1147\n",
      "Epoch [1550/4000] - Train Loss: 0.0388, Val Loss: 0.1155\n",
      "Epoch [1551/4000] - Train Loss: 0.0395, Val Loss: 0.1158\n",
      "Epoch [1552/4000] - Train Loss: 0.0390, Val Loss: 0.1158\n",
      "Epoch [1553/4000] - Train Loss: 0.0387, Val Loss: 0.1157\n",
      "Epoch [1554/4000] - Train Loss: 0.0385, Val Loss: 0.1174\n",
      "Epoch [1555/4000] - Train Loss: 0.0393, Val Loss: 0.1158\n",
      "Epoch [1556/4000] - Train Loss: 0.0389, Val Loss: 0.1158\n",
      "Epoch [1557/4000] - Train Loss: 0.0385, Val Loss: 0.1164\n",
      "Epoch [1558/4000] - Train Loss: 0.0390, Val Loss: 0.1166\n",
      "Epoch [1559/4000] - Train Loss: 0.0391, Val Loss: 0.1159\n",
      "Epoch [1560/4000] - Train Loss: 0.0397, Val Loss: 0.1158\n",
      "Epoch [1561/4000] - Train Loss: 0.0392, Val Loss: 0.1149\n",
      "Epoch [1562/4000] - Train Loss: 0.0394, Val Loss: 0.1169\n",
      "Epoch [1563/4000] - Train Loss: 0.0398, Val Loss: 0.1157\n",
      "Epoch [1564/4000] - Train Loss: 0.0392, Val Loss: 0.1148\n",
      "Epoch [1565/4000] - Train Loss: 0.0385, Val Loss: 0.1174\n",
      "Epoch [1566/4000] - Train Loss: 0.0396, Val Loss: 0.1169\n",
      "Epoch [1567/4000] - Train Loss: 0.0389, Val Loss: 0.1169\n",
      "Epoch [1568/4000] - Train Loss: 0.0392, Val Loss: 0.1157\n",
      "Epoch [1569/4000] - Train Loss: 0.0386, Val Loss: 0.1168\n",
      "Epoch [1570/4000] - Train Loss: 0.0394, Val Loss: 0.1153\n",
      "Epoch [1571/4000] - Train Loss: 0.0392, Val Loss: 0.1175\n",
      "Epoch [1572/4000] - Train Loss: 0.0399, Val Loss: 0.1181\n",
      "Epoch [1573/4000] - Train Loss: 0.0398, Val Loss: 0.1158\n",
      "Epoch [1574/4000] - Train Loss: 0.0394, Val Loss: 0.1167\n",
      "Epoch [1575/4000] - Train Loss: 0.0396, Val Loss: 0.1165\n",
      "Epoch [1576/4000] - Train Loss: 0.0389, Val Loss: 0.1159\n",
      "Epoch [1577/4000] - Train Loss: 0.0386, Val Loss: 0.1164\n",
      "Epoch [1578/4000] - Train Loss: 0.0386, Val Loss: 0.1157\n",
      "Epoch [1579/4000] - Train Loss: 0.0386, Val Loss: 0.1159\n",
      "Epoch [1580/4000] - Train Loss: 0.0385, Val Loss: 0.1163\n",
      "Epoch [1581/4000] - Train Loss: 0.0390, Val Loss: 0.1178\n",
      "Epoch [1582/4000] - Train Loss: 0.0392, Val Loss: 0.1171\n",
      "Epoch [1583/4000] - Train Loss: 0.0384, Val Loss: 0.1165\n",
      "Epoch [1584/4000] - Train Loss: 0.0382, Val Loss: 0.1157\n",
      "Epoch [1585/4000] - Train Loss: 0.0385, Val Loss: 0.1158\n",
      "Epoch [1586/4000] - Train Loss: 0.0387, Val Loss: 0.1158\n",
      "Epoch [1587/4000] - Train Loss: 0.0395, Val Loss: 0.1157\n",
      "Epoch [1588/4000] - Train Loss: 0.0381, Val Loss: 0.1166\n",
      "Epoch [1589/4000] - Train Loss: 0.0389, Val Loss: 0.1184\n",
      "Epoch [1590/4000] - Train Loss: 0.0384, Val Loss: 0.1156\n",
      "Epoch [1591/4000] - Train Loss: 0.0383, Val Loss: 0.1151\n",
      "Epoch [1592/4000] - Train Loss: 0.0391, Val Loss: 0.1165\n",
      "Epoch [1593/4000] - Train Loss: 0.0395, Val Loss: 0.1154\n",
      "Epoch [1594/4000] - Train Loss: 0.0393, Val Loss: 0.1149\n",
      "Epoch [1595/4000] - Train Loss: 0.0391, Val Loss: 0.1173\n",
      "Epoch [1596/4000] - Train Loss: 0.0395, Val Loss: 0.1154\n",
      "Epoch [1597/4000] - Train Loss: 0.0395, Val Loss: 0.1202\n",
      "Epoch [1598/4000] - Train Loss: 0.0406, Val Loss: 0.1149\n",
      "Epoch [1599/4000] - Train Loss: 0.0399, Val Loss: 0.1171\n",
      "Epoch [1600/4000] - Train Loss: 0.0398, Val Loss: 0.1141\n",
      "Epoch [1601/4000] - Train Loss: 0.0390, Val Loss: 0.1183\n",
      "Epoch [1602/4000] - Train Loss: 0.0411, Val Loss: 0.1156\n",
      "Epoch [1603/4000] - Train Loss: 0.0394, Val Loss: 0.1166\n",
      "Epoch [1604/4000] - Train Loss: 0.0388, Val Loss: 0.1164\n",
      "Epoch [1605/4000] - Train Loss: 0.0386, Val Loss: 0.1176\n",
      "Epoch [1606/4000] - Train Loss: 0.0392, Val Loss: 0.1170\n",
      "Epoch [1607/4000] - Train Loss: 0.0388, Val Loss: 0.1176\n",
      "Epoch [1608/4000] - Train Loss: 0.0393, Val Loss: 0.1159\n",
      "Epoch [1609/4000] - Train Loss: 0.0385, Val Loss: 0.1166\n",
      "Epoch [1610/4000] - Train Loss: 0.0383, Val Loss: 0.1161\n",
      "Epoch [1611/4000] - Train Loss: 0.0382, Val Loss: 0.1159\n",
      "Epoch [1612/4000] - Train Loss: 0.0395, Val Loss: 0.1155\n",
      "Epoch [1613/4000] - Train Loss: 0.0385, Val Loss: 0.1162\n",
      "Epoch [1614/4000] - Train Loss: 0.0387, Val Loss: 0.1187\n",
      "Epoch [1615/4000] - Train Loss: 0.0394, Val Loss: 0.1163\n",
      "Epoch [1616/4000] - Train Loss: 0.0392, Val Loss: 0.1174\n",
      "Epoch [1617/4000] - Train Loss: 0.0384, Val Loss: 0.1155\n",
      "Epoch [1618/4000] - Train Loss: 0.0383, Val Loss: 0.1168\n",
      "Epoch [1619/4000] - Train Loss: 0.0386, Val Loss: 0.1172\n",
      "Epoch [1620/4000] - Train Loss: 0.0389, Val Loss: 0.1172\n",
      "Epoch [1621/4000] - Train Loss: 0.0389, Val Loss: 0.1166\n",
      "Epoch [1622/4000] - Train Loss: 0.0390, Val Loss: 0.1159\n",
      "Epoch [1623/4000] - Train Loss: 0.0389, Val Loss: 0.1157\n",
      "Epoch [1624/4000] - Train Loss: 0.0383, Val Loss: 0.1149\n",
      "Epoch [1625/4000] - Train Loss: 0.0378, Val Loss: 0.1156\n",
      "Epoch [1626/4000] - Train Loss: 0.0381, Val Loss: 0.1168\n",
      "Epoch [1627/4000] - Train Loss: 0.0386, Val Loss: 0.1171\n",
      "Epoch [1628/4000] - Train Loss: 0.0404, Val Loss: 0.1181\n",
      "Epoch [1629/4000] - Train Loss: 0.0419, Val Loss: 0.1206\n",
      "Epoch [1630/4000] - Train Loss: 0.0398, Val Loss: 0.1160\n",
      "Epoch [1631/4000] - Train Loss: 0.0395, Val Loss: 0.1139\n",
      "Epoch [1632/4000] - Train Loss: 0.0384, Val Loss: 0.1150\n",
      "Epoch [1633/4000] - Train Loss: 0.0380, Val Loss: 0.1166\n",
      "Epoch [1634/4000] - Train Loss: 0.0390, Val Loss: 0.1204\n",
      "Epoch [1635/4000] - Train Loss: 0.0414, Val Loss: 0.1191\n",
      "Epoch [1636/4000] - Train Loss: 0.0407, Val Loss: 0.1181\n",
      "Epoch [1637/4000] - Train Loss: 0.0398, Val Loss: 0.1154\n",
      "Epoch [1638/4000] - Train Loss: 0.0393, Val Loss: 0.1171\n",
      "Epoch [1639/4000] - Train Loss: 0.0390, Val Loss: 0.1178\n",
      "Epoch [1640/4000] - Train Loss: 0.0384, Val Loss: 0.1195\n",
      "Epoch [1641/4000] - Train Loss: 0.0395, Val Loss: 0.1173\n",
      "Epoch [1642/4000] - Train Loss: 0.0387, Val Loss: 0.1165\n",
      "Epoch [1643/4000] - Train Loss: 0.0381, Val Loss: 0.1179\n",
      "Epoch [1644/4000] - Train Loss: 0.0389, Val Loss: 0.1169\n",
      "Epoch [1645/4000] - Train Loss: 0.0385, Val Loss: 0.1156\n",
      "Epoch [1646/4000] - Train Loss: 0.0387, Val Loss: 0.1167\n",
      "Epoch [1647/4000] - Train Loss: 0.0386, Val Loss: 0.1155\n",
      "Epoch [1648/4000] - Train Loss: 0.0382, Val Loss: 0.1177\n",
      "Epoch [1649/4000] - Train Loss: 0.0380, Val Loss: 0.1163\n",
      "Epoch [1650/4000] - Train Loss: 0.0389, Val Loss: 0.1169\n",
      "Epoch [1651/4000] - Train Loss: 0.0384, Val Loss: 0.1172\n",
      "Epoch [1652/4000] - Train Loss: 0.0380, Val Loss: 0.1167\n",
      "Epoch [1653/4000] - Train Loss: 0.0393, Val Loss: 0.1161\n",
      "Epoch [1654/4000] - Train Loss: 0.0387, Val Loss: 0.1189\n",
      "Epoch [1655/4000] - Train Loss: 0.0387, Val Loss: 0.1185\n",
      "Epoch [1656/4000] - Train Loss: 0.0397, Val Loss: 0.1168\n",
      "Epoch [1657/4000] - Train Loss: 0.0399, Val Loss: 0.1189\n",
      "Epoch [1658/4000] - Train Loss: 0.0391, Val Loss: 0.1174\n",
      "Epoch [1659/4000] - Train Loss: 0.0387, Val Loss: 0.1176\n",
      "Epoch [1660/4000] - Train Loss: 0.0402, Val Loss: 0.1232\n",
      "Epoch [1661/4000] - Train Loss: 0.0407, Val Loss: 0.1166\n",
      "Epoch [1662/4000] - Train Loss: 0.0391, Val Loss: 0.1178\n",
      "Epoch [1663/4000] - Train Loss: 0.0382, Val Loss: 0.1150\n",
      "Epoch [1664/4000] - Train Loss: 0.0385, Val Loss: 0.1154\n",
      "Epoch [1665/4000] - Train Loss: 0.0386, Val Loss: 0.1189\n",
      "Epoch [1666/4000] - Train Loss: 0.0398, Val Loss: 0.1174\n",
      "Epoch [1667/4000] - Train Loss: 0.0385, Val Loss: 0.1166\n",
      "Epoch [1668/4000] - Train Loss: 0.0383, Val Loss: 0.1179\n",
      "Epoch [1669/4000] - Train Loss: 0.0378, Val Loss: 0.1192\n",
      "Epoch [1670/4000] - Train Loss: 0.0386, Val Loss: 0.1186\n",
      "Epoch [1671/4000] - Train Loss: 0.0378, Val Loss: 0.1161\n",
      "Epoch [1672/4000] - Train Loss: 0.0375, Val Loss: 0.1149\n",
      "Epoch [1673/4000] - Train Loss: 0.0381, Val Loss: 0.1164\n",
      "Epoch [1674/4000] - Train Loss: 0.0391, Val Loss: 0.1164\n",
      "Epoch [1675/4000] - Train Loss: 0.0390, Val Loss: 0.1167\n",
      "Epoch [1676/4000] - Train Loss: 0.0377, Val Loss: 0.1155\n",
      "Epoch [1677/4000] - Train Loss: 0.0380, Val Loss: 0.1158\n",
      "Epoch [1678/4000] - Train Loss: 0.0383, Val Loss: 0.1179\n",
      "Epoch [1679/4000] - Train Loss: 0.0377, Val Loss: 0.1158\n",
      "Epoch [1680/4000] - Train Loss: 0.0378, Val Loss: 0.1184\n",
      "Epoch [1681/4000] - Train Loss: 0.0369, Val Loss: 0.1170\n",
      "Epoch [1682/4000] - Train Loss: 0.0400, Val Loss: 0.1174\n",
      "Epoch [1683/4000] - Train Loss: 0.0379, Val Loss: 0.1164\n",
      "Epoch [1684/4000] - Train Loss: 0.0373, Val Loss: 0.1181\n",
      "Epoch [1685/4000] - Train Loss: 0.0371, Val Loss: 0.1173\n",
      "Epoch [1686/4000] - Train Loss: 0.0375, Val Loss: 0.1179\n",
      "Epoch [1687/4000] - Train Loss: 0.0375, Val Loss: 0.1173\n",
      "Epoch [1688/4000] - Train Loss: 0.0383, Val Loss: 0.1165\n",
      "Epoch [1689/4000] - Train Loss: 0.0381, Val Loss: 0.1191\n",
      "Epoch [1690/4000] - Train Loss: 0.0383, Val Loss: 0.1165\n",
      "Epoch [1691/4000] - Train Loss: 0.0377, Val Loss: 0.1162\n",
      "Epoch [1692/4000] - Train Loss: 0.0374, Val Loss: 0.1180\n",
      "Epoch [1693/4000] - Train Loss: 0.0384, Val Loss: 0.1182\n",
      "Epoch [1694/4000] - Train Loss: 0.0379, Val Loss: 0.1174\n",
      "Epoch [1695/4000] - Train Loss: 0.0386, Val Loss: 0.1207\n",
      "Epoch [1696/4000] - Train Loss: 0.0410, Val Loss: 0.1197\n",
      "Epoch [1697/4000] - Train Loss: 0.0536, Val Loss: 0.1195\n",
      "Epoch [1698/4000] - Train Loss: 0.0470, Val Loss: 0.1187\n",
      "Epoch [1699/4000] - Train Loss: 0.0438, Val Loss: 0.1351\n",
      "Epoch [1700/4000] - Train Loss: 0.0431, Val Loss: 0.1186\n",
      "Epoch [1701/4000] - Train Loss: 0.0409, Val Loss: 0.1146\n",
      "Epoch [1702/4000] - Train Loss: 0.0398, Val Loss: 0.1206\n",
      "Epoch [1703/4000] - Train Loss: 0.0397, Val Loss: 0.1176\n",
      "Epoch [1704/4000] - Train Loss: 0.0393, Val Loss: 0.1128\n",
      "Epoch [1705/4000] - Train Loss: 0.0390, Val Loss: 0.1157\n",
      "Epoch [1706/4000] - Train Loss: 0.0383, Val Loss: 0.1172\n",
      "Epoch [1707/4000] - Train Loss: 0.0373, Val Loss: 0.1161\n",
      "Epoch [1708/4000] - Train Loss: 0.0376, Val Loss: 0.1171\n",
      "Epoch [1709/4000] - Train Loss: 0.0380, Val Loss: 0.1155\n",
      "Epoch [1710/4000] - Train Loss: 0.0378, Val Loss: 0.1153\n",
      "Epoch [1711/4000] - Train Loss: 0.0384, Val Loss: 0.1153\n",
      "Epoch [1712/4000] - Train Loss: 0.0374, Val Loss: 0.1156\n",
      "Epoch [1713/4000] - Train Loss: 0.0379, Val Loss: 0.1159\n",
      "Epoch [1714/4000] - Train Loss: 0.0373, Val Loss: 0.1163\n",
      "Epoch [1715/4000] - Train Loss: 0.0369, Val Loss: 0.1161\n",
      "Epoch [1716/4000] - Train Loss: 0.0369, Val Loss: 0.1162\n",
      "Epoch [1717/4000] - Train Loss: 0.0372, Val Loss: 0.1165\n",
      "Epoch [1718/4000] - Train Loss: 0.0375, Val Loss: 0.1160\n",
      "Epoch [1719/4000] - Train Loss: 0.0371, Val Loss: 0.1156\n",
      "Epoch [1720/4000] - Train Loss: 0.0369, Val Loss: 0.1162\n",
      "Epoch [1721/4000] - Train Loss: 0.0369, Val Loss: 0.1152\n",
      "Epoch [1722/4000] - Train Loss: 0.0370, Val Loss: 0.1163\n",
      "Epoch [1723/4000] - Train Loss: 0.0369, Val Loss: 0.1167\n",
      "Epoch [1724/4000] - Train Loss: 0.0368, Val Loss: 0.1157\n",
      "Epoch [1725/4000] - Train Loss: 0.0372, Val Loss: 0.1169\n",
      "Epoch [1726/4000] - Train Loss: 0.0371, Val Loss: 0.1162\n",
      "Epoch [1727/4000] - Train Loss: 0.0369, Val Loss: 0.1173\n",
      "Epoch [1728/4000] - Train Loss: 0.0369, Val Loss: 0.1166\n",
      "Epoch [1729/4000] - Train Loss: 0.0377, Val Loss: 0.1166\n",
      "Epoch [1730/4000] - Train Loss: 0.0374, Val Loss: 0.1162\n",
      "Epoch [1731/4000] - Train Loss: 0.0370, Val Loss: 0.1178\n",
      "Epoch [1732/4000] - Train Loss: 0.0375, Val Loss: 0.1161\n",
      "Epoch [1733/4000] - Train Loss: 0.0376, Val Loss: 0.1155\n",
      "Epoch [1734/4000] - Train Loss: 0.0368, Val Loss: 0.1159\n",
      "Epoch [1735/4000] - Train Loss: 0.0377, Val Loss: 0.1165\n",
      "Epoch [1736/4000] - Train Loss: 0.0370, Val Loss: 0.1174\n",
      "Epoch [1737/4000] - Train Loss: 0.0370, Val Loss: 0.1161\n",
      "Epoch [1738/4000] - Train Loss: 0.0370, Val Loss: 0.1161\n",
      "Epoch [1739/4000] - Train Loss: 0.0370, Val Loss: 0.1169\n",
      "Epoch [1740/4000] - Train Loss: 0.0380, Val Loss: 0.1183\n",
      "Epoch [1741/4000] - Train Loss: 0.0375, Val Loss: 0.1167\n",
      "Epoch [1742/4000] - Train Loss: 0.0375, Val Loss: 0.1160\n",
      "Epoch [1743/4000] - Train Loss: 0.0377, Val Loss: 0.1159\n",
      "Epoch [1744/4000] - Train Loss: 0.0370, Val Loss: 0.1173\n",
      "Epoch [1745/4000] - Train Loss: 0.0383, Val Loss: 0.1166\n",
      "Epoch [1746/4000] - Train Loss: 0.0374, Val Loss: 0.1158\n",
      "Epoch [1747/4000] - Train Loss: 0.0378, Val Loss: 0.1180\n",
      "Epoch [1748/4000] - Train Loss: 0.0371, Val Loss: 0.1167\n",
      "Epoch [1749/4000] - Train Loss: 0.0373, Val Loss: 0.1174\n",
      "Epoch [1750/4000] - Train Loss: 0.0373, Val Loss: 0.1156\n",
      "Epoch [1751/4000] - Train Loss: 0.0377, Val Loss: 0.1168\n",
      "Epoch [1752/4000] - Train Loss: 0.0381, Val Loss: 0.1166\n",
      "Epoch [1753/4000] - Train Loss: 0.0375, Val Loss: 0.1182\n",
      "Epoch [1754/4000] - Train Loss: 0.0380, Val Loss: 0.1179\n",
      "Epoch [1755/4000] - Train Loss: 0.0380, Val Loss: 0.1172\n",
      "Epoch [1756/4000] - Train Loss: 0.0371, Val Loss: 0.1174\n",
      "Epoch [1757/4000] - Train Loss: 0.0378, Val Loss: 0.1161\n",
      "Epoch [1758/4000] - Train Loss: 0.0373, Val Loss: 0.1161\n",
      "Epoch [1759/4000] - Train Loss: 0.0363, Val Loss: 0.1164\n",
      "Epoch [1760/4000] - Train Loss: 0.0365, Val Loss: 0.1163\n",
      "Epoch [1761/4000] - Train Loss: 0.0369, Val Loss: 0.1179\n",
      "Epoch [1762/4000] - Train Loss: 0.0380, Val Loss: 0.1181\n",
      "Epoch [1763/4000] - Train Loss: 0.0377, Val Loss: 0.1183\n",
      "Epoch [1764/4000] - Train Loss: 0.0377, Val Loss: 0.1168\n",
      "Epoch [1765/4000] - Train Loss: 0.0367, Val Loss: 0.1179\n",
      "Epoch [1766/4000] - Train Loss: 0.0376, Val Loss: 0.1207\n",
      "Epoch [1767/4000] - Train Loss: 0.0377, Val Loss: 0.1163\n",
      "Epoch [1768/4000] - Train Loss: 0.0369, Val Loss: 0.1164\n",
      "Epoch [1769/4000] - Train Loss: 0.0374, Val Loss: 0.1166\n",
      "Epoch [1770/4000] - Train Loss: 0.0370, Val Loss: 0.1169\n",
      "Epoch [1771/4000] - Train Loss: 0.0385, Val Loss: 0.1187\n",
      "Epoch [1772/4000] - Train Loss: 0.0381, Val Loss: 0.1191\n",
      "Epoch [1773/4000] - Train Loss: 0.0377, Val Loss: 0.1183\n",
      "Epoch [1774/4000] - Train Loss: 0.0382, Val Loss: 0.1154\n",
      "Epoch [1775/4000] - Train Loss: 0.0385, Val Loss: 0.1198\n",
      "Epoch [1776/4000] - Train Loss: 0.0396, Val Loss: 0.1155\n",
      "Epoch [1777/4000] - Train Loss: 0.0383, Val Loss: 0.1164\n",
      "Epoch [1778/4000] - Train Loss: 0.0376, Val Loss: 0.1144\n",
      "Epoch [1779/4000] - Train Loss: 0.0379, Val Loss: 0.1172\n",
      "Epoch [1780/4000] - Train Loss: 0.0375, Val Loss: 0.1180\n",
      "Epoch [1781/4000] - Train Loss: 0.0372, Val Loss: 0.1181\n",
      "Epoch [1782/4000] - Train Loss: 0.0374, Val Loss: 0.1185\n",
      "Epoch [1783/4000] - Train Loss: 0.0381, Val Loss: 0.1168\n",
      "Epoch [1784/4000] - Train Loss: 0.0392, Val Loss: 0.1156\n",
      "Epoch [1785/4000] - Train Loss: 0.0382, Val Loss: 0.1165\n",
      "Epoch [1786/4000] - Train Loss: 0.0373, Val Loss: 0.1148\n",
      "Epoch [1787/4000] - Train Loss: 0.0385, Val Loss: 0.1184\n",
      "Epoch [1788/4000] - Train Loss: 0.0383, Val Loss: 0.1160\n",
      "Epoch [1789/4000] - Train Loss: 0.0376, Val Loss: 0.1198\n",
      "Epoch [1790/4000] - Train Loss: 0.0373, Val Loss: 0.1187\n",
      "Epoch [1791/4000] - Train Loss: 0.0375, Val Loss: 0.1197\n",
      "Epoch [1792/4000] - Train Loss: 0.0373, Val Loss: 0.1207\n",
      "Epoch [1793/4000] - Train Loss: 0.0376, Val Loss: 0.1172\n",
      "Epoch [1794/4000] - Train Loss: 0.0378, Val Loss: 0.1176\n",
      "Epoch [1795/4000] - Train Loss: 0.0381, Val Loss: 0.1170\n",
      "Epoch [1796/4000] - Train Loss: 0.0371, Val Loss: 0.1174\n",
      "Epoch [1797/4000] - Train Loss: 0.0386, Val Loss: 0.1207\n",
      "Epoch [1798/4000] - Train Loss: 0.0385, Val Loss: 0.1163\n",
      "Epoch [1799/4000] - Train Loss: 0.0377, Val Loss: 0.1189\n",
      "Epoch [1800/4000] - Train Loss: 0.0370, Val Loss: 0.1174\n",
      "Epoch [1801/4000] - Train Loss: 0.0374, Val Loss: 0.1188\n",
      "Epoch [1802/4000] - Train Loss: 0.0371, Val Loss: 0.1154\n",
      "Epoch [1803/4000] - Train Loss: 0.0363, Val Loss: 0.1163\n",
      "Epoch [1804/4000] - Train Loss: 0.0367, Val Loss: 0.1161\n",
      "Epoch [1805/4000] - Train Loss: 0.0376, Val Loss: 0.1144\n",
      "Epoch [1806/4000] - Train Loss: 0.0385, Val Loss: 0.1182\n",
      "Epoch [1807/4000] - Train Loss: 0.0387, Val Loss: 0.1154\n",
      "Epoch [1808/4000] - Train Loss: 0.0378, Val Loss: 0.1166\n",
      "Epoch [1809/4000] - Train Loss: 0.0381, Val Loss: 0.1159\n",
      "Epoch [1810/4000] - Train Loss: 0.0377, Val Loss: 0.1161\n",
      "Epoch [1811/4000] - Train Loss: 0.0366, Val Loss: 0.1147\n",
      "Epoch [1812/4000] - Train Loss: 0.0364, Val Loss: 0.1151\n",
      "Epoch [1813/4000] - Train Loss: 0.0369, Val Loss: 0.1153\n",
      "Epoch [1814/4000] - Train Loss: 0.0372, Val Loss: 0.1147\n",
      "Epoch [1815/4000] - Train Loss: 0.0374, Val Loss: 0.1149\n",
      "Epoch [1816/4000] - Train Loss: 0.0369, Val Loss: 0.1184\n",
      "Epoch [1817/4000] - Train Loss: 0.0375, Val Loss: 0.1161\n",
      "Epoch [1818/4000] - Train Loss: 0.0373, Val Loss: 0.1164\n",
      "Epoch [1819/4000] - Train Loss: 0.0377, Val Loss: 0.1170\n",
      "Epoch [1820/4000] - Train Loss: 0.0377, Val Loss: 0.1175\n",
      "Epoch [1821/4000] - Train Loss: 0.0368, Val Loss: 0.1159\n",
      "Epoch [1822/4000] - Train Loss: 0.0362, Val Loss: 0.1160\n",
      "Epoch [1823/4000] - Train Loss: 0.0367, Val Loss: 0.1166\n",
      "Epoch [1824/4000] - Train Loss: 0.0371, Val Loss: 0.1164\n",
      "Epoch [1825/4000] - Train Loss: 0.0359, Val Loss: 0.1151\n",
      "Epoch [1826/4000] - Train Loss: 0.0356, Val Loss: 0.1165\n",
      "Epoch [1827/4000] - Train Loss: 0.0364, Val Loss: 0.1167\n",
      "Epoch [1828/4000] - Train Loss: 0.0371, Val Loss: 0.1164\n",
      "Epoch [1829/4000] - Train Loss: 0.0378, Val Loss: 0.1154\n",
      "Epoch [1830/4000] - Train Loss: 0.0378, Val Loss: 0.1179\n",
      "Epoch [1831/4000] - Train Loss: 0.0387, Val Loss: 0.1210\n",
      "Epoch [1832/4000] - Train Loss: 0.0387, Val Loss: 0.1205\n",
      "Epoch [1833/4000] - Train Loss: 0.0412, Val Loss: 0.1187\n",
      "Epoch [1834/4000] - Train Loss: 0.0392, Val Loss: 0.1202\n",
      "Epoch [1835/4000] - Train Loss: 0.0383, Val Loss: 0.1202\n",
      "Epoch [1836/4000] - Train Loss: 0.0370, Val Loss: 0.1196\n",
      "Epoch [1837/4000] - Train Loss: 0.0374, Val Loss: 0.1177\n",
      "Epoch [1838/4000] - Train Loss: 0.0377, Val Loss: 0.1196\n",
      "Epoch [1839/4000] - Train Loss: 0.0378, Val Loss: 0.1173\n",
      "Epoch [1840/4000] - Train Loss: 0.0368, Val Loss: 0.1184\n",
      "Epoch [1841/4000] - Train Loss: 0.0377, Val Loss: 0.1185\n",
      "Epoch [1842/4000] - Train Loss: 0.0363, Val Loss: 0.1187\n",
      "Epoch [1843/4000] - Train Loss: 0.0363, Val Loss: 0.1189\n",
      "Epoch [1844/4000] - Train Loss: 0.0359, Val Loss: 0.1170\n",
      "Epoch [1845/4000] - Train Loss: 0.0365, Val Loss: 0.1156\n",
      "Epoch [1846/4000] - Train Loss: 0.0366, Val Loss: 0.1158\n",
      "Epoch [1847/4000] - Train Loss: 0.0367, Val Loss: 0.1160\n",
      "Epoch [1848/4000] - Train Loss: 0.0371, Val Loss: 0.1160\n",
      "Epoch [1849/4000] - Train Loss: 0.0375, Val Loss: 0.1180\n",
      "Epoch [1850/4000] - Train Loss: 0.0373, Val Loss: 0.1159\n",
      "Epoch [1851/4000] - Train Loss: 0.0370, Val Loss: 0.1161\n",
      "Epoch [1852/4000] - Train Loss: 0.0373, Val Loss: 0.1147\n",
      "Epoch [1853/4000] - Train Loss: 0.0365, Val Loss: 0.1153\n",
      "Epoch [1854/4000] - Train Loss: 0.0368, Val Loss: 0.1161\n",
      "Epoch [1855/4000] - Train Loss: 0.0369, Val Loss: 0.1163\n",
      "Epoch [1856/4000] - Train Loss: 0.0365, Val Loss: 0.1173\n",
      "Epoch [1857/4000] - Train Loss: 0.0363, Val Loss: 0.1153\n",
      "Epoch [1858/4000] - Train Loss: 0.0373, Val Loss: 0.1162\n",
      "Epoch [1859/4000] - Train Loss: 0.0394, Val Loss: 0.1178\n",
      "Epoch [1860/4000] - Train Loss: 0.0380, Val Loss: 0.1162\n",
      "Epoch [1861/4000] - Train Loss: 0.0374, Val Loss: 0.1184\n",
      "Epoch [1862/4000] - Train Loss: 0.0378, Val Loss: 0.1179\n",
      "Epoch [1863/4000] - Train Loss: 0.0370, Val Loss: 0.1204\n",
      "Epoch [1864/4000] - Train Loss: 0.0386, Val Loss: 0.1185\n",
      "Epoch [1865/4000] - Train Loss: 0.0379, Val Loss: 0.1193\n",
      "Epoch [1866/4000] - Train Loss: 0.0371, Val Loss: 0.1180\n",
      "Epoch [1867/4000] - Train Loss: 0.0368, Val Loss: 0.1171\n",
      "Epoch [1868/4000] - Train Loss: 0.0364, Val Loss: 0.1174\n",
      "Epoch [1869/4000] - Train Loss: 0.0366, Val Loss: 0.1160\n",
      "Epoch [1870/4000] - Train Loss: 0.0363, Val Loss: 0.1157\n",
      "Epoch [1871/4000] - Train Loss: 0.0375, Val Loss: 0.1158\n",
      "Epoch [1872/4000] - Train Loss: 0.0367, Val Loss: 0.1167\n",
      "Epoch [1873/4000] - Train Loss: 0.0369, Val Loss: 0.1160\n",
      "Epoch [1874/4000] - Train Loss: 0.0386, Val Loss: 0.1177\n",
      "Epoch [1875/4000] - Train Loss: 0.0373, Val Loss: 0.1186\n",
      "Epoch [1876/4000] - Train Loss: 0.0365, Val Loss: 0.1171\n",
      "Epoch [1877/4000] - Train Loss: 0.0362, Val Loss: 0.1177\n",
      "Epoch [1878/4000] - Train Loss: 0.0359, Val Loss: 0.1168\n",
      "Epoch [1879/4000] - Train Loss: 0.0362, Val Loss: 0.1154\n",
      "Epoch [1880/4000] - Train Loss: 0.0360, Val Loss: 0.1162\n",
      "Epoch [1881/4000] - Train Loss: 0.0359, Val Loss: 0.1163\n",
      "Epoch [1882/4000] - Train Loss: 0.0366, Val Loss: 0.1155\n",
      "Epoch [1883/4000] - Train Loss: 0.0378, Val Loss: 0.1164\n",
      "Epoch [1884/4000] - Train Loss: 0.0371, Val Loss: 0.1170\n",
      "Epoch [1885/4000] - Train Loss: 0.0363, Val Loss: 0.1160\n",
      "Epoch [1886/4000] - Train Loss: 0.0355, Val Loss: 0.1170\n",
      "Epoch [1887/4000] - Train Loss: 0.0363, Val Loss: 0.1163\n",
      "Epoch [1888/4000] - Train Loss: 0.0370, Val Loss: 0.1167\n",
      "Epoch [1889/4000] - Train Loss: 0.0361, Val Loss: 0.1160\n",
      "Epoch [1890/4000] - Train Loss: 0.0367, Val Loss: 0.1161\n",
      "Epoch [1891/4000] - Train Loss: 0.0368, Val Loss: 0.1156\n",
      "Epoch [1892/4000] - Train Loss: 0.0372, Val Loss: 0.1171\n",
      "Epoch [1893/4000] - Train Loss: 0.0367, Val Loss: 0.1192\n",
      "Epoch [1894/4000] - Train Loss: 0.0359, Val Loss: 0.1169\n",
      "Epoch [1895/4000] - Train Loss: 0.0365, Val Loss: 0.1162\n",
      "Epoch [1896/4000] - Train Loss: 0.0361, Val Loss: 0.1173\n",
      "Epoch [1897/4000] - Train Loss: 0.0359, Val Loss: 0.1164\n",
      "Epoch [1898/4000] - Train Loss: 0.0378, Val Loss: 0.1165\n",
      "Epoch [1899/4000] - Train Loss: 0.0374, Val Loss: 0.1161\n",
      "Epoch [1900/4000] - Train Loss: 0.0366, Val Loss: 0.1155\n",
      "Epoch [1901/4000] - Train Loss: 0.0368, Val Loss: 0.1192\n",
      "Epoch [1902/4000] - Train Loss: 0.0363, Val Loss: 0.1187\n",
      "Epoch [1903/4000] - Train Loss: 0.0374, Val Loss: 0.1167\n",
      "Epoch [1904/4000] - Train Loss: 0.0374, Val Loss: 0.1152\n",
      "Epoch [1905/4000] - Train Loss: 0.0380, Val Loss: 0.1173\n",
      "Epoch [1906/4000] - Train Loss: 0.0366, Val Loss: 0.1185\n",
      "Epoch [1907/4000] - Train Loss: 0.0378, Val Loss: 0.1214\n",
      "Epoch [1908/4000] - Train Loss: 0.0374, Val Loss: 0.1183\n",
      "Epoch [1909/4000] - Train Loss: 0.0373, Val Loss: 0.1195\n",
      "Epoch [1910/4000] - Train Loss: 0.0384, Val Loss: 0.1212\n",
      "Epoch [1911/4000] - Train Loss: 0.0366, Val Loss: 0.1188\n",
      "Epoch [1912/4000] - Train Loss: 0.0360, Val Loss: 0.1186\n",
      "Epoch [1913/4000] - Train Loss: 0.0360, Val Loss: 0.1173\n",
      "Epoch [1914/4000] - Train Loss: 0.0363, Val Loss: 0.1168\n",
      "Epoch [1915/4000] - Train Loss: 0.0363, Val Loss: 0.1167\n",
      "Epoch [1916/4000] - Train Loss: 0.0363, Val Loss: 0.1177\n",
      "Epoch [1917/4000] - Train Loss: 0.0366, Val Loss: 0.1179\n",
      "Epoch [1918/4000] - Train Loss: 0.0367, Val Loss: 0.1212\n",
      "Epoch [1919/4000] - Train Loss: 0.0372, Val Loss: 0.1187\n",
      "Epoch [1920/4000] - Train Loss: 0.0366, Val Loss: 0.1177\n",
      "Epoch [1921/4000] - Train Loss: 0.0377, Val Loss: 0.1170\n",
      "Epoch [1922/4000] - Train Loss: 0.0370, Val Loss: 0.1161\n",
      "Epoch [1923/4000] - Train Loss: 0.0363, Val Loss: 0.1203\n",
      "Epoch [1924/4000] - Train Loss: 0.0365, Val Loss: 0.1162\n",
      "Epoch [1925/4000] - Train Loss: 0.0360, Val Loss: 0.1161\n",
      "Epoch [1926/4000] - Train Loss: 0.0360, Val Loss: 0.1164\n",
      "Epoch [1927/4000] - Train Loss: 0.0361, Val Loss: 0.1161\n",
      "Epoch [1928/4000] - Train Loss: 0.0352, Val Loss: 0.1157\n",
      "Epoch [1929/4000] - Train Loss: 0.0352, Val Loss: 0.1165\n",
      "Epoch [1930/4000] - Train Loss: 0.0360, Val Loss: 0.1165\n",
      "Epoch [1931/4000] - Train Loss: 0.0349, Val Loss: 0.1170\n",
      "Epoch [1932/4000] - Train Loss: 0.0354, Val Loss: 0.1172\n",
      "Epoch [1933/4000] - Train Loss: 0.0359, Val Loss: 0.1168\n",
      "Epoch [1934/4000] - Train Loss: 0.0352, Val Loss: 0.1175\n",
      "Epoch [1935/4000] - Train Loss: 0.0357, Val Loss: 0.1182\n",
      "Epoch [1936/4000] - Train Loss: 0.0354, Val Loss: 0.1172\n",
      "Epoch [1937/4000] - Train Loss: 0.0351, Val Loss: 0.1174\n",
      "Epoch [1938/4000] - Train Loss: 0.0355, Val Loss: 0.1176\n",
      "Epoch [1939/4000] - Train Loss: 0.0351, Val Loss: 0.1170\n",
      "Epoch [1940/4000] - Train Loss: 0.0356, Val Loss: 0.1208\n",
      "Epoch [1941/4000] - Train Loss: 0.0365, Val Loss: 0.1172\n",
      "Epoch [1942/4000] - Train Loss: 0.0362, Val Loss: 0.1184\n",
      "Epoch [1943/4000] - Train Loss: 0.0367, Val Loss: 0.1191\n",
      "Epoch [1944/4000] - Train Loss: 0.0369, Val Loss: 0.1196\n",
      "Epoch [1945/4000] - Train Loss: 0.0370, Val Loss: 0.1161\n",
      "Epoch [1946/4000] - Train Loss: 0.0377, Val Loss: 0.1190\n",
      "Epoch [1947/4000] - Train Loss: 0.0387, Val Loss: 0.1156\n",
      "Epoch [1948/4000] - Train Loss: 0.0396, Val Loss: 0.1162\n",
      "Epoch [1949/4000] - Train Loss: 0.0382, Val Loss: 0.1137\n",
      "Epoch [1950/4000] - Train Loss: 0.0375, Val Loss: 0.1155\n",
      "Epoch [1951/4000] - Train Loss: 0.0387, Val Loss: 0.1166\n",
      "Epoch [1952/4000] - Train Loss: 0.0376, Val Loss: 0.1165\n",
      "Epoch [1953/4000] - Train Loss: 0.0365, Val Loss: 0.1159\n",
      "Epoch [1954/4000] - Train Loss: 0.0355, Val Loss: 0.1167\n",
      "Epoch [1955/4000] - Train Loss: 0.0359, Val Loss: 0.1158\n",
      "Epoch [1956/4000] - Train Loss: 0.0360, Val Loss: 0.1175\n",
      "Epoch [1957/4000] - Train Loss: 0.0359, Val Loss: 0.1161\n",
      "Epoch [1958/4000] - Train Loss: 0.0358, Val Loss: 0.1173\n",
      "Epoch [1959/4000] - Train Loss: 0.0370, Val Loss: 0.1154\n",
      "Epoch [1960/4000] - Train Loss: 0.0365, Val Loss: 0.1167\n",
      "Epoch [1961/4000] - Train Loss: 0.0365, Val Loss: 0.1168\n",
      "Epoch [1962/4000] - Train Loss: 0.0363, Val Loss: 0.1177\n",
      "Epoch [1963/4000] - Train Loss: 0.0353, Val Loss: 0.1173\n",
      "Epoch [1964/4000] - Train Loss: 0.0366, Val Loss: 0.1201\n",
      "Epoch [1965/4000] - Train Loss: 0.0378, Val Loss: 0.1194\n",
      "Epoch [1966/4000] - Train Loss: 0.0375, Val Loss: 0.1209\n",
      "Epoch [1967/4000] - Train Loss: 0.0370, Val Loss: 0.1197\n",
      "Epoch [1968/4000] - Train Loss: 0.0363, Val Loss: 0.1186\n",
      "Epoch [1969/4000] - Train Loss: 0.0368, Val Loss: 0.1190\n",
      "Epoch [1970/4000] - Train Loss: 0.0356, Val Loss: 0.1170\n",
      "Epoch [1971/4000] - Train Loss: 0.0358, Val Loss: 0.1173\n",
      "Epoch [1972/4000] - Train Loss: 0.0359, Val Loss: 0.1165\n",
      "Epoch [1973/4000] - Train Loss: 0.0362, Val Loss: 0.1159\n",
      "Epoch [1974/4000] - Train Loss: 0.0359, Val Loss: 0.1172\n",
      "Epoch [1975/4000] - Train Loss: 0.0365, Val Loss: 0.1148\n",
      "Epoch [1976/4000] - Train Loss: 0.0363, Val Loss: 0.1171\n",
      "Epoch [1977/4000] - Train Loss: 0.0355, Val Loss: 0.1173\n",
      "Epoch [1978/4000] - Train Loss: 0.0348, Val Loss: 0.1156\n",
      "Epoch [1979/4000] - Train Loss: 0.0359, Val Loss: 0.1181\n",
      "Epoch [1980/4000] - Train Loss: 0.0365, Val Loss: 0.1172\n",
      "Epoch [1981/4000] - Train Loss: 0.0365, Val Loss: 0.1163\n",
      "Epoch [1982/4000] - Train Loss: 0.0372, Val Loss: 0.1180\n",
      "Epoch [1983/4000] - Train Loss: 0.0366, Val Loss: 0.1148\n",
      "Epoch [1984/4000] - Train Loss: 0.0363, Val Loss: 0.1171\n",
      "Epoch [1985/4000] - Train Loss: 0.0357, Val Loss: 0.1163\n",
      "Epoch [1986/4000] - Train Loss: 0.0357, Val Loss: 0.1144\n",
      "Epoch [1987/4000] - Train Loss: 0.0368, Val Loss: 0.1183\n",
      "Epoch [1988/4000] - Train Loss: 0.0357, Val Loss: 0.1172\n",
      "Epoch [1989/4000] - Train Loss: 0.0357, Val Loss: 0.1144\n",
      "Epoch [1990/4000] - Train Loss: 0.0374, Val Loss: 0.1167\n",
      "Epoch [1991/4000] - Train Loss: 0.0376, Val Loss: 0.1169\n",
      "Epoch [1992/4000] - Train Loss: 0.0359, Val Loss: 0.1203\n",
      "Epoch [1993/4000] - Train Loss: 0.0363, Val Loss: 0.1171\n",
      "Epoch [1994/4000] - Train Loss: 0.0369, Val Loss: 0.1166\n",
      "Epoch [1995/4000] - Train Loss: 0.0357, Val Loss: 0.1177\n",
      "Epoch [1996/4000] - Train Loss: 0.0355, Val Loss: 0.1168\n",
      "Epoch [1997/4000] - Train Loss: 0.0354, Val Loss: 0.1171\n",
      "Epoch [1998/4000] - Train Loss: 0.0360, Val Loss: 0.1165\n",
      "Epoch [1999/4000] - Train Loss: 0.0357, Val Loss: 0.1156\n",
      "Epoch [2000/4000] - Train Loss: 0.0367, Val Loss: 0.1171\n",
      "Epoch [2001/4000] - Train Loss: 0.0362, Val Loss: 0.1171\n",
      "Epoch [2002/4000] - Train Loss: 0.0350, Val Loss: 0.1155\n",
      "Epoch [2003/4000] - Train Loss: 0.0356, Val Loss: 0.1164\n",
      "Epoch [2004/4000] - Train Loss: 0.0352, Val Loss: 0.1162\n",
      "Epoch [2005/4000] - Train Loss: 0.0370, Val Loss: 0.1175\n",
      "Epoch [2006/4000] - Train Loss: 0.0369, Val Loss: 0.1179\n",
      "Epoch [2007/4000] - Train Loss: 0.0361, Val Loss: 0.1170\n",
      "Epoch [2008/4000] - Train Loss: 0.0351, Val Loss: 0.1164\n",
      "Epoch [2009/4000] - Train Loss: 0.0352, Val Loss: 0.1154\n",
      "Epoch [2010/4000] - Train Loss: 0.0346, Val Loss: 0.1172\n",
      "Epoch [2011/4000] - Train Loss: 0.0359, Val Loss: 0.1166\n",
      "Epoch [2012/4000] - Train Loss: 0.0360, Val Loss: 0.1171\n",
      "Epoch [2013/4000] - Train Loss: 0.0354, Val Loss: 0.1163\n",
      "Epoch [2014/4000] - Train Loss: 0.0347, Val Loss: 0.1171\n",
      "Epoch [2015/4000] - Train Loss: 0.0351, Val Loss: 0.1173\n",
      "Epoch [2016/4000] - Train Loss: 0.0350, Val Loss: 0.1170\n",
      "Epoch [2017/4000] - Train Loss: 0.0343, Val Loss: 0.1170\n",
      "Epoch [2018/4000] - Train Loss: 0.0349, Val Loss: 0.1169\n",
      "Epoch [2019/4000] - Train Loss: 0.0359, Val Loss: 0.1175\n",
      "Epoch [2020/4000] - Train Loss: 0.0349, Val Loss: 0.1158\n",
      "Epoch [2021/4000] - Train Loss: 0.0353, Val Loss: 0.1189\n",
      "Epoch [2022/4000] - Train Loss: 0.0358, Val Loss: 0.1198\n",
      "Epoch [2023/4000] - Train Loss: 0.0364, Val Loss: 0.1167\n",
      "Epoch [2024/4000] - Train Loss: 0.0362, Val Loss: 0.1166\n",
      "Epoch [2025/4000] - Train Loss: 0.0359, Val Loss: 0.1192\n",
      "Epoch [2026/4000] - Train Loss: 0.0352, Val Loss: 0.1165\n",
      "Epoch [2027/4000] - Train Loss: 0.0356, Val Loss: 0.1166\n",
      "Epoch [2028/4000] - Train Loss: 0.0358, Val Loss: 0.1193\n",
      "Epoch [2029/4000] - Train Loss: 0.0346, Val Loss: 0.1176\n",
      "Epoch [2030/4000] - Train Loss: 0.0348, Val Loss: 0.1180\n",
      "Epoch [2031/4000] - Train Loss: 0.0352, Val Loss: 0.1164\n",
      "Epoch [2032/4000] - Train Loss: 0.0372, Val Loss: 0.1178\n",
      "Epoch [2033/4000] - Train Loss: 0.0367, Val Loss: 0.1200\n",
      "Epoch [2034/4000] - Train Loss: 0.0363, Val Loss: 0.1153\n",
      "Epoch [2035/4000] - Train Loss: 0.0360, Val Loss: 0.1172\n",
      "Epoch [2036/4000] - Train Loss: 0.0362, Val Loss: 0.1253\n",
      "Epoch [2037/4000] - Train Loss: 0.0368, Val Loss: 0.1157\n",
      "Epoch [2038/4000] - Train Loss: 0.0360, Val Loss: 0.1178\n",
      "Epoch [2039/4000] - Train Loss: 0.0366, Val Loss: 0.1161\n",
      "Epoch [2040/4000] - Train Loss: 0.0364, Val Loss: 0.1182\n",
      "Epoch [2041/4000] - Train Loss: 0.0366, Val Loss: 0.1211\n",
      "Epoch [2042/4000] - Train Loss: 0.0372, Val Loss: 0.1149\n",
      "Epoch [2043/4000] - Train Loss: 0.0371, Val Loss: 0.1174\n",
      "Epoch [2044/4000] - Train Loss: 0.0371, Val Loss: 0.1156\n",
      "Epoch [2045/4000] - Train Loss: 0.0358, Val Loss: 0.1171\n",
      "Epoch [2046/4000] - Train Loss: 0.0349, Val Loss: 0.1147\n",
      "Epoch [2047/4000] - Train Loss: 0.0364, Val Loss: 0.1162\n",
      "Epoch [2048/4000] - Train Loss: 0.0352, Val Loss: 0.1180\n",
      "Epoch [2049/4000] - Train Loss: 0.0349, Val Loss: 0.1158\n",
      "Epoch [2050/4000] - Train Loss: 0.0351, Val Loss: 0.1165\n",
      "Epoch [2051/4000] - Train Loss: 0.0345, Val Loss: 0.1157\n",
      "Epoch [2052/4000] - Train Loss: 0.0350, Val Loss: 0.1154\n",
      "Epoch [2053/4000] - Train Loss: 0.0354, Val Loss: 0.1165\n",
      "Epoch [2054/4000] - Train Loss: 0.0357, Val Loss: 0.1183\n",
      "Epoch [2055/4000] - Train Loss: 0.0353, Val Loss: 0.1162\n",
      "Epoch [2056/4000] - Train Loss: 0.0352, Val Loss: 0.1157\n",
      "Epoch [2057/4000] - Train Loss: 0.0348, Val Loss: 0.1166\n",
      "Epoch [2058/4000] - Train Loss: 0.0353, Val Loss: 0.1174\n",
      "Epoch [2059/4000] - Train Loss: 0.0357, Val Loss: 0.1164\n",
      "Epoch [2060/4000] - Train Loss: 0.0355, Val Loss: 0.1168\n",
      "Epoch [2061/4000] - Train Loss: 0.0356, Val Loss: 0.1160\n",
      "Epoch [2062/4000] - Train Loss: 0.0352, Val Loss: 0.1167\n",
      "Epoch [2063/4000] - Train Loss: 0.0351, Val Loss: 0.1189\n",
      "Epoch [2064/4000] - Train Loss: 0.0352, Val Loss: 0.1169\n",
      "Epoch [2065/4000] - Train Loss: 0.0343, Val Loss: 0.1166\n",
      "Epoch [2066/4000] - Train Loss: 0.0345, Val Loss: 0.1177\n",
      "Epoch [2067/4000] - Train Loss: 0.0344, Val Loss: 0.1179\n",
      "Epoch [2068/4000] - Train Loss: 0.0348, Val Loss: 0.1156\n",
      "Epoch [2069/4000] - Train Loss: 0.0354, Val Loss: 0.1178\n",
      "Epoch [2070/4000] - Train Loss: 0.0359, Val Loss: 0.1175\n",
      "Epoch [2071/4000] - Train Loss: 0.0342, Val Loss: 0.1193\n",
      "Epoch [2072/4000] - Train Loss: 0.0348, Val Loss: 0.1160\n",
      "Epoch [2073/4000] - Train Loss: 0.0354, Val Loss: 0.1154\n",
      "Epoch [2074/4000] - Train Loss: 0.0348, Val Loss: 0.1174\n",
      "Epoch [2075/4000] - Train Loss: 0.0350, Val Loss: 0.1195\n",
      "Epoch [2076/4000] - Train Loss: 0.0356, Val Loss: 0.1161\n",
      "Epoch [2077/4000] - Train Loss: 0.0353, Val Loss: 0.1177\n",
      "Epoch [2078/4000] - Train Loss: 0.0354, Val Loss: 0.1218\n",
      "Epoch [2079/4000] - Train Loss: 0.0355, Val Loss: 0.1176\n",
      "Epoch [2080/4000] - Train Loss: 0.0359, Val Loss: 0.1177\n",
      "Epoch [2081/4000] - Train Loss: 0.0355, Val Loss: 0.1147\n",
      "Epoch [2082/4000] - Train Loss: 0.0357, Val Loss: 0.1182\n",
      "Epoch [2083/4000] - Train Loss: 0.0349, Val Loss: 0.1177\n",
      "Epoch [2084/4000] - Train Loss: 0.0349, Val Loss: 0.1195\n",
      "Epoch [2085/4000] - Train Loss: 0.0349, Val Loss: 0.1162\n",
      "Epoch [2086/4000] - Train Loss: 0.0356, Val Loss: 0.1180\n",
      "Epoch [2087/4000] - Train Loss: 0.0354, Val Loss: 0.1175\n",
      "Epoch [2088/4000] - Train Loss: 0.0347, Val Loss: 0.1171\n",
      "Epoch [2089/4000] - Train Loss: 0.0354, Val Loss: 0.1177\n",
      "Epoch [2090/4000] - Train Loss: 0.0360, Val Loss: 0.1199\n",
      "Epoch [2091/4000] - Train Loss: 0.0373, Val Loss: 0.1169\n",
      "Epoch [2092/4000] - Train Loss: 0.0378, Val Loss: 0.1174\n",
      "Epoch [2093/4000] - Train Loss: 0.0387, Val Loss: 0.1219\n",
      "Epoch [2094/4000] - Train Loss: 0.0407, Val Loss: 0.1262\n",
      "Epoch [2095/4000] - Train Loss: 0.0450, Val Loss: 0.1322\n",
      "Epoch [2096/4000] - Train Loss: 0.0442, Val Loss: 0.1206\n",
      "Epoch [2097/4000] - Train Loss: 0.0387, Val Loss: 0.1173\n",
      "Epoch [2098/4000] - Train Loss: 0.0358, Val Loss: 0.1150\n",
      "Epoch [2099/4000] - Train Loss: 0.0360, Val Loss: 0.1173\n",
      "Epoch [2100/4000] - Train Loss: 0.0354, Val Loss: 0.1182\n",
      "Epoch [2101/4000] - Train Loss: 0.0357, Val Loss: 0.1177\n",
      "Epoch [2102/4000] - Train Loss: 0.0359, Val Loss: 0.1163\n",
      "Epoch [2103/4000] - Train Loss: 0.0348, Val Loss: 0.1167\n",
      "Epoch [2104/4000] - Train Loss: 0.0350, Val Loss: 0.1168\n",
      "Epoch [2105/4000] - Train Loss: 0.0349, Val Loss: 0.1166\n",
      "Epoch [2106/4000] - Train Loss: 0.0344, Val Loss: 0.1167\n",
      "Epoch [2107/4000] - Train Loss: 0.0344, Val Loss: 0.1175\n",
      "Epoch [2108/4000] - Train Loss: 0.0352, Val Loss: 0.1175\n",
      "Epoch [2109/4000] - Train Loss: 0.0351, Val Loss: 0.1171\n",
      "Epoch [2110/4000] - Train Loss: 0.0346, Val Loss: 0.1174\n",
      "Epoch [2111/4000] - Train Loss: 0.0346, Val Loss: 0.1172\n",
      "Epoch [2112/4000] - Train Loss: 0.0345, Val Loss: 0.1176\n",
      "Epoch [2113/4000] - Train Loss: 0.0349, Val Loss: 0.1170\n",
      "Epoch [2114/4000] - Train Loss: 0.0353, Val Loss: 0.1204\n",
      "Epoch [2115/4000] - Train Loss: 0.0346, Val Loss: 0.1171\n",
      "Epoch [2116/4000] - Train Loss: 0.0345, Val Loss: 0.1174\n",
      "Epoch [2117/4000] - Train Loss: 0.0344, Val Loss: 0.1166\n",
      "Epoch [2118/4000] - Train Loss: 0.0346, Val Loss: 0.1160\n",
      "Epoch [2119/4000] - Train Loss: 0.0343, Val Loss: 0.1165\n",
      "Epoch [2120/4000] - Train Loss: 0.0341, Val Loss: 0.1160\n",
      "Epoch [2121/4000] - Train Loss: 0.0343, Val Loss: 0.1162\n",
      "Epoch [2122/4000] - Train Loss: 0.0343, Val Loss: 0.1175\n",
      "Epoch [2123/4000] - Train Loss: 0.0343, Val Loss: 0.1153\n",
      "Epoch [2124/4000] - Train Loss: 0.0344, Val Loss: 0.1154\n",
      "Epoch [2125/4000] - Train Loss: 0.0341, Val Loss: 0.1150\n",
      "Epoch [2126/4000] - Train Loss: 0.0340, Val Loss: 0.1180\n",
      "Epoch [2127/4000] - Train Loss: 0.0341, Val Loss: 0.1167\n",
      "Epoch [2128/4000] - Train Loss: 0.0347, Val Loss: 0.1188\n",
      "Epoch [2129/4000] - Train Loss: 0.0354, Val Loss: 0.1173\n",
      "Epoch [2130/4000] - Train Loss: 0.0342, Val Loss: 0.1151\n",
      "Epoch [2131/4000] - Train Loss: 0.0343, Val Loss: 0.1165\n",
      "Epoch [2132/4000] - Train Loss: 0.0349, Val Loss: 0.1162\n",
      "Epoch [2133/4000] - Train Loss: 0.0345, Val Loss: 0.1162\n",
      "Epoch [2134/4000] - Train Loss: 0.0342, Val Loss: 0.1150\n",
      "Epoch [2135/4000] - Train Loss: 0.0349, Val Loss: 0.1173\n",
      "Epoch [2136/4000] - Train Loss: 0.0352, Val Loss: 0.1175\n",
      "Epoch [2137/4000] - Train Loss: 0.0344, Val Loss: 0.1171\n",
      "Epoch [2138/4000] - Train Loss: 0.0347, Val Loss: 0.1168\n",
      "Epoch [2139/4000] - Train Loss: 0.0345, Val Loss: 0.1179\n",
      "Epoch [2140/4000] - Train Loss: 0.0346, Val Loss: 0.1157\n",
      "Epoch [2141/4000] - Train Loss: 0.0338, Val Loss: 0.1178\n",
      "Epoch [2142/4000] - Train Loss: 0.0347, Val Loss: 0.1187\n",
      "Epoch [2143/4000] - Train Loss: 0.0350, Val Loss: 0.1175\n",
      "Epoch [2144/4000] - Train Loss: 0.0346, Val Loss: 0.1176\n",
      "Epoch [2145/4000] - Train Loss: 0.0344, Val Loss: 0.1172\n",
      "Epoch [2146/4000] - Train Loss: 0.0342, Val Loss: 0.1164\n",
      "Epoch [2147/4000] - Train Loss: 0.0339, Val Loss: 0.1163\n",
      "Epoch [2148/4000] - Train Loss: 0.0343, Val Loss: 0.1188\n",
      "Epoch [2149/4000] - Train Loss: 0.0347, Val Loss: 0.1169\n",
      "Epoch [2150/4000] - Train Loss: 0.0353, Val Loss: 0.1172\n",
      "Epoch [2151/4000] - Train Loss: 0.0345, Val Loss: 0.1174\n",
      "Epoch [2152/4000] - Train Loss: 0.0348, Val Loss: 0.1165\n",
      "Epoch [2153/4000] - Train Loss: 0.0353, Val Loss: 0.1158\n",
      "Epoch [2154/4000] - Train Loss: 0.0338, Val Loss: 0.1168\n",
      "Epoch [2155/4000] - Train Loss: 0.0338, Val Loss: 0.1151\n",
      "Epoch [2156/4000] - Train Loss: 0.0335, Val Loss: 0.1173\n",
      "Epoch [2157/4000] - Train Loss: 0.0344, Val Loss: 0.1165\n",
      "Epoch [2158/4000] - Train Loss: 0.0341, Val Loss: 0.1172\n",
      "Epoch [2159/4000] - Train Loss: 0.0345, Val Loss: 0.1171\n",
      "Epoch [2160/4000] - Train Loss: 0.0342, Val Loss: 0.1171\n",
      "Epoch [2161/4000] - Train Loss: 0.0353, Val Loss: 0.1169\n",
      "Epoch [2162/4000] - Train Loss: 0.0341, Val Loss: 0.1185\n",
      "Epoch [2163/4000] - Train Loss: 0.0344, Val Loss: 0.1173\n",
      "Epoch [2164/4000] - Train Loss: 0.0352, Val Loss: 0.1191\n",
      "Epoch [2165/4000] - Train Loss: 0.0348, Val Loss: 0.1170\n",
      "Epoch [2166/4000] - Train Loss: 0.0351, Val Loss: 0.1168\n",
      "Epoch [2167/4000] - Train Loss: 0.0345, Val Loss: 0.1167\n",
      "Epoch [2168/4000] - Train Loss: 0.0344, Val Loss: 0.1189\n",
      "Epoch [2169/4000] - Train Loss: 0.0351, Val Loss: 0.1196\n",
      "Epoch [2170/4000] - Train Loss: 0.0343, Val Loss: 0.1160\n",
      "Epoch [2171/4000] - Train Loss: 0.0342, Val Loss: 0.1169\n",
      "Epoch [2172/4000] - Train Loss: 0.0350, Val Loss: 0.1183\n",
      "Epoch [2173/4000] - Train Loss: 0.0351, Val Loss: 0.1187\n",
      "Epoch [2174/4000] - Train Loss: 0.0344, Val Loss: 0.1188\n",
      "Epoch [2175/4000] - Train Loss: 0.0345, Val Loss: 0.1162\n",
      "Epoch [2176/4000] - Train Loss: 0.0346, Val Loss: 0.1161\n",
      "Epoch [2177/4000] - Train Loss: 0.0349, Val Loss: 0.1169\n",
      "Epoch [2178/4000] - Train Loss: 0.0335, Val Loss: 0.1174\n",
      "Epoch [2179/4000] - Train Loss: 0.0339, Val Loss: 0.1183\n",
      "Epoch [2180/4000] - Train Loss: 0.0341, Val Loss: 0.1176\n",
      "Epoch [2181/4000] - Train Loss: 0.0337, Val Loss: 0.1182\n",
      "Epoch [2182/4000] - Train Loss: 0.0342, Val Loss: 0.1166\n",
      "Epoch [2183/4000] - Train Loss: 0.0344, Val Loss: 0.1169\n",
      "Epoch [2184/4000] - Train Loss: 0.0341, Val Loss: 0.1192\n",
      "Epoch [2185/4000] - Train Loss: 0.0342, Val Loss: 0.1188\n",
      "Epoch [2186/4000] - Train Loss: 0.0337, Val Loss: 0.1182\n",
      "Epoch [2187/4000] - Train Loss: 0.0341, Val Loss: 0.1168\n",
      "Epoch [2188/4000] - Train Loss: 0.0336, Val Loss: 0.1196\n",
      "Epoch [2189/4000] - Train Loss: 0.0337, Val Loss: 0.1177\n",
      "Epoch [2190/4000] - Train Loss: 0.0343, Val Loss: 0.1190\n",
      "Epoch [2191/4000] - Train Loss: 0.0338, Val Loss: 0.1177\n",
      "Epoch [2192/4000] - Train Loss: 0.0347, Val Loss: 0.1193\n",
      "Epoch [2193/4000] - Train Loss: 0.0347, Val Loss: 0.1172\n",
      "Epoch [2194/4000] - Train Loss: 0.0346, Val Loss: 0.1187\n",
      "Epoch [2195/4000] - Train Loss: 0.0334, Val Loss: 0.1202\n",
      "Epoch [2196/4000] - Train Loss: 0.0331, Val Loss: 0.1176\n",
      "Epoch [2197/4000] - Train Loss: 0.0335, Val Loss: 0.1158\n",
      "Epoch [2198/4000] - Train Loss: 0.0350, Val Loss: 0.1190\n",
      "Epoch [2199/4000] - Train Loss: 0.0339, Val Loss: 0.1174\n",
      "Epoch [2200/4000] - Train Loss: 0.0336, Val Loss: 0.1192\n",
      "Epoch [2201/4000] - Train Loss: 0.0351, Val Loss: 0.1187\n",
      "Epoch [2202/4000] - Train Loss: 0.0346, Val Loss: 0.1180\n",
      "Epoch [2203/4000] - Train Loss: 0.0343, Val Loss: 0.1183\n",
      "Epoch [2204/4000] - Train Loss: 0.0357, Val Loss: 0.1223\n",
      "Epoch [2205/4000] - Train Loss: 0.0353, Val Loss: 0.1181\n",
      "Epoch [2206/4000] - Train Loss: 0.0347, Val Loss: 0.1175\n",
      "Epoch [2207/4000] - Train Loss: 0.0340, Val Loss: 0.1178\n",
      "Epoch [2208/4000] - Train Loss: 0.0346, Val Loss: 0.1162\n",
      "Epoch [2209/4000] - Train Loss: 0.0339, Val Loss: 0.1177\n",
      "Epoch [2210/4000] - Train Loss: 0.0343, Val Loss: 0.1180\n",
      "Epoch [2211/4000] - Train Loss: 0.0341, Val Loss: 0.1175\n",
      "Epoch [2212/4000] - Train Loss: 0.0343, Val Loss: 0.1189\n",
      "Epoch [2213/4000] - Train Loss: 0.0354, Val Loss: 0.1186\n",
      "Epoch [2214/4000] - Train Loss: 0.0354, Val Loss: 0.1201\n",
      "Epoch [2215/4000] - Train Loss: 0.0357, Val Loss: 0.1162\n",
      "Epoch [2216/4000] - Train Loss: 0.0353, Val Loss: 0.1158\n",
      "Epoch [2217/4000] - Train Loss: 0.0345, Val Loss: 0.1184\n",
      "Epoch [2218/4000] - Train Loss: 0.0350, Val Loss: 0.1183\n",
      "Epoch [2219/4000] - Train Loss: 0.0351, Val Loss: 0.1242\n",
      "Epoch [2220/4000] - Train Loss: 0.0398, Val Loss: 0.1203\n",
      "Epoch [2221/4000] - Train Loss: 0.0363, Val Loss: 0.1196\n",
      "Epoch [2222/4000] - Train Loss: 0.0364, Val Loss: 0.1208\n",
      "Epoch [2223/4000] - Train Loss: 0.0377, Val Loss: 0.1183\n",
      "Epoch [2224/4000] - Train Loss: 0.0354, Val Loss: 0.1174\n",
      "Epoch [2225/4000] - Train Loss: 0.0340, Val Loss: 0.1170\n",
      "Epoch [2226/4000] - Train Loss: 0.0346, Val Loss: 0.1203\n",
      "Epoch [2227/4000] - Train Loss: 0.0348, Val Loss: 0.1175\n",
      "Epoch [2228/4000] - Train Loss: 0.0340, Val Loss: 0.1166\n",
      "Epoch [2229/4000] - Train Loss: 0.0343, Val Loss: 0.1167\n",
      "Epoch [2230/4000] - Train Loss: 0.0340, Val Loss: 0.1170\n",
      "Epoch [2231/4000] - Train Loss: 0.0338, Val Loss: 0.1165\n",
      "Epoch [2232/4000] - Train Loss: 0.0351, Val Loss: 0.1163\n",
      "Epoch [2233/4000] - Train Loss: 0.0351, Val Loss: 0.1188\n",
      "Epoch [2234/4000] - Train Loss: 0.0363, Val Loss: 0.1163\n",
      "Epoch [2235/4000] - Train Loss: 0.0357, Val Loss: 0.1187\n",
      "Epoch [2236/4000] - Train Loss: 0.0360, Val Loss: 0.1171\n",
      "Epoch [2237/4000] - Train Loss: 0.0348, Val Loss: 0.1167\n",
      "Epoch [2238/4000] - Train Loss: 0.0337, Val Loss: 0.1160\n",
      "Epoch [2239/4000] - Train Loss: 0.0336, Val Loss: 0.1180\n",
      "Epoch [2240/4000] - Train Loss: 0.0338, Val Loss: 0.1171\n",
      "Epoch [2241/4000] - Train Loss: 0.0339, Val Loss: 0.1179\n",
      "Epoch [2242/4000] - Train Loss: 0.0338, Val Loss: 0.1186\n",
      "Epoch [2243/4000] - Train Loss: 0.0334, Val Loss: 0.1173\n",
      "Epoch [2244/4000] - Train Loss: 0.0330, Val Loss: 0.1173\n",
      "Epoch [2245/4000] - Train Loss: 0.0338, Val Loss: 0.1172\n",
      "Epoch [2246/4000] - Train Loss: 0.0337, Val Loss: 0.1171\n",
      "Epoch [2247/4000] - Train Loss: 0.0337, Val Loss: 0.1182\n",
      "Epoch [2248/4000] - Train Loss: 0.0344, Val Loss: 0.1173\n",
      "Epoch [2249/4000] - Train Loss: 0.0340, Val Loss: 0.1165\n",
      "Epoch [2250/4000] - Train Loss: 0.0348, Val Loss: 0.1150\n",
      "Epoch [2251/4000] - Train Loss: 0.0352, Val Loss: 0.1159\n",
      "Epoch [2252/4000] - Train Loss: 0.0342, Val Loss: 0.1168\n",
      "Epoch [2253/4000] - Train Loss: 0.0338, Val Loss: 0.1154\n",
      "Epoch [2254/4000] - Train Loss: 0.0336, Val Loss: 0.1161\n",
      "Epoch [2255/4000] - Train Loss: 0.0340, Val Loss: 0.1163\n",
      "Epoch [2256/4000] - Train Loss: 0.0342, Val Loss: 0.1169\n",
      "Epoch [2257/4000] - Train Loss: 0.0331, Val Loss: 0.1161\n",
      "Epoch [2258/4000] - Train Loss: 0.0339, Val Loss: 0.1169\n",
      "Epoch [2259/4000] - Train Loss: 0.0346, Val Loss: 0.1175\n",
      "Epoch [2260/4000] - Train Loss: 0.0351, Val Loss: 0.1151\n",
      "Epoch [2261/4000] - Train Loss: 0.0350, Val Loss: 0.1186\n",
      "Epoch [2262/4000] - Train Loss: 0.0337, Val Loss: 0.1177\n",
      "Epoch [2263/4000] - Train Loss: 0.0339, Val Loss: 0.1193\n",
      "Epoch [2264/4000] - Train Loss: 0.0345, Val Loss: 0.1185\n",
      "Epoch [2265/4000] - Train Loss: 0.0349, Val Loss: 0.1157\n",
      "Epoch [2266/4000] - Train Loss: 0.0341, Val Loss: 0.1176\n",
      "Epoch [2267/4000] - Train Loss: 0.0346, Val Loss: 0.1165\n",
      "Epoch [2268/4000] - Train Loss: 0.0352, Val Loss: 0.1167\n",
      "Epoch [2269/4000] - Train Loss: 0.0344, Val Loss: 0.1170\n",
      "Epoch [2270/4000] - Train Loss: 0.0336, Val Loss: 0.1172\n",
      "Epoch [2271/4000] - Train Loss: 0.0340, Val Loss: 0.1186\n",
      "Epoch [2272/4000] - Train Loss: 0.0341, Val Loss: 0.1168\n",
      "Epoch [2273/4000] - Train Loss: 0.0346, Val Loss: 0.1163\n",
      "Epoch [2274/4000] - Train Loss: 0.0335, Val Loss: 0.1181\n",
      "Epoch [2275/4000] - Train Loss: 0.0337, Val Loss: 0.1170\n",
      "Epoch [2276/4000] - Train Loss: 0.0346, Val Loss: 0.1171\n",
      "Epoch [2277/4000] - Train Loss: 0.0339, Val Loss: 0.1178\n",
      "Epoch [2278/4000] - Train Loss: 0.0335, Val Loss: 0.1169\n",
      "Epoch [2279/4000] - Train Loss: 0.0328, Val Loss: 0.1178\n",
      "Epoch [2280/4000] - Train Loss: 0.0330, Val Loss: 0.1189\n",
      "Epoch [2281/4000] - Train Loss: 0.0337, Val Loss: 0.1173\n",
      "Epoch [2282/4000] - Train Loss: 0.0341, Val Loss: 0.1184\n",
      "Epoch [2283/4000] - Train Loss: 0.0331, Val Loss: 0.1177\n",
      "Epoch [2284/4000] - Train Loss: 0.0335, Val Loss: 0.1177\n",
      "Epoch [2285/4000] - Train Loss: 0.0337, Val Loss: 0.1160\n",
      "Epoch [2286/4000] - Train Loss: 0.0336, Val Loss: 0.1178\n",
      "Epoch [2287/4000] - Train Loss: 0.0334, Val Loss: 0.1172\n",
      "Epoch [2288/4000] - Train Loss: 0.0335, Val Loss: 0.1200\n",
      "Epoch [2289/4000] - Train Loss: 0.0347, Val Loss: 0.1170\n",
      "Epoch [2290/4000] - Train Loss: 0.0344, Val Loss: 0.1172\n",
      "Epoch [2291/4000] - Train Loss: 0.0335, Val Loss: 0.1169\n",
      "Epoch [2292/4000] - Train Loss: 0.0332, Val Loss: 0.1163\n",
      "Epoch [2293/4000] - Train Loss: 0.0336, Val Loss: 0.1180\n",
      "Epoch [2294/4000] - Train Loss: 0.0340, Val Loss: 0.1157\n",
      "Epoch [2295/4000] - Train Loss: 0.0342, Val Loss: 0.1196\n",
      "Epoch [2296/4000] - Train Loss: 0.0335, Val Loss: 0.1169\n",
      "Epoch [2297/4000] - Train Loss: 0.0349, Val Loss: 0.1227\n",
      "Epoch [2298/4000] - Train Loss: 0.0343, Val Loss: 0.1177\n",
      "Epoch [2299/4000] - Train Loss: 0.0340, Val Loss: 0.1185\n",
      "Epoch [2300/4000] - Train Loss: 0.0339, Val Loss: 0.1185\n",
      "Epoch [2301/4000] - Train Loss: 0.0337, Val Loss: 0.1174\n",
      "Epoch [2302/4000] - Train Loss: 0.0341, Val Loss: 0.1201\n",
      "Epoch [2303/4000] - Train Loss: 0.0336, Val Loss: 0.1181\n",
      "Epoch [2304/4000] - Train Loss: 0.0334, Val Loss: 0.1173\n",
      "Epoch [2305/4000] - Train Loss: 0.0343, Val Loss: 0.1176\n",
      "Epoch [2306/4000] - Train Loss: 0.0339, Val Loss: 0.1174\n",
      "Epoch [2307/4000] - Train Loss: 0.0352, Val Loss: 0.1186\n",
      "Epoch [2308/4000] - Train Loss: 0.0348, Val Loss: 0.1195\n",
      "Epoch [2309/4000] - Train Loss: 0.0345, Val Loss: 0.1184\n",
      "Epoch [2310/4000] - Train Loss: 0.0352, Val Loss: 0.1209\n",
      "Epoch [2311/4000] - Train Loss: 0.0350, Val Loss: 0.1169\n",
      "Epoch [2312/4000] - Train Loss: 0.0338, Val Loss: 0.1194\n",
      "Epoch [2313/4000] - Train Loss: 0.0334, Val Loss: 0.1201\n",
      "Epoch [2314/4000] - Train Loss: 0.0338, Val Loss: 0.1178\n",
      "Epoch [2315/4000] - Train Loss: 0.0346, Val Loss: 0.1192\n",
      "Epoch [2316/4000] - Train Loss: 0.0346, Val Loss: 0.1202\n",
      "Epoch [2317/4000] - Train Loss: 0.0345, Val Loss: 0.1184\n",
      "Epoch [2318/4000] - Train Loss: 0.0344, Val Loss: 0.1189\n",
      "Epoch [2319/4000] - Train Loss: 0.0346, Val Loss: 0.1189\n",
      "Epoch [2320/4000] - Train Loss: 0.0352, Val Loss: 0.1200\n",
      "Epoch [2321/4000] - Train Loss: 0.0342, Val Loss: 0.1197\n",
      "Epoch [2322/4000] - Train Loss: 0.0337, Val Loss: 0.1189\n",
      "Epoch [2323/4000] - Train Loss: 0.0342, Val Loss: 0.1180\n",
      "Epoch [2324/4000] - Train Loss: 0.0343, Val Loss: 0.1207\n",
      "Epoch [2325/4000] - Train Loss: 0.0351, Val Loss: 0.1161\n",
      "Epoch [2326/4000] - Train Loss: 0.0347, Val Loss: 0.1219\n",
      "Epoch [2327/4000] - Train Loss: 0.0350, Val Loss: 0.1177\n",
      "Epoch [2328/4000] - Train Loss: 0.0339, Val Loss: 0.1170\n",
      "Epoch [2329/4000] - Train Loss: 0.0337, Val Loss: 0.1181\n",
      "Epoch [2330/4000] - Train Loss: 0.0337, Val Loss: 0.1166\n",
      "Epoch [2331/4000] - Train Loss: 0.0329, Val Loss: 0.1170\n",
      "Epoch [2332/4000] - Train Loss: 0.0334, Val Loss: 0.1176\n",
      "Epoch [2333/4000] - Train Loss: 0.0331, Val Loss: 0.1175\n",
      "Epoch [2334/4000] - Train Loss: 0.0334, Val Loss: 0.1165\n",
      "Epoch [2335/4000] - Train Loss: 0.0335, Val Loss: 0.1174\n",
      "Epoch [2336/4000] - Train Loss: 0.0331, Val Loss: 0.1164\n",
      "Epoch [2337/4000] - Train Loss: 0.0336, Val Loss: 0.1190\n",
      "Epoch [2338/4000] - Train Loss: 0.0341, Val Loss: 0.1183\n",
      "Epoch [2339/4000] - Train Loss: 0.0331, Val Loss: 0.1182\n",
      "Epoch [2340/4000] - Train Loss: 0.0332, Val Loss: 0.1185\n",
      "Epoch [2341/4000] - Train Loss: 0.0339, Val Loss: 0.1170\n",
      "Epoch [2342/4000] - Train Loss: 0.0332, Val Loss: 0.1154\n",
      "Epoch [2343/4000] - Train Loss: 0.0335, Val Loss: 0.1172\n",
      "Epoch [2344/4000] - Train Loss: 0.0338, Val Loss: 0.1164\n",
      "Epoch [2345/4000] - Train Loss: 0.0334, Val Loss: 0.1186\n",
      "Epoch [2346/4000] - Train Loss: 0.0332, Val Loss: 0.1206\n",
      "Epoch [2347/4000] - Train Loss: 0.0332, Val Loss: 0.1201\n",
      "Epoch [2348/4000] - Train Loss: 0.0340, Val Loss: 0.1198\n",
      "Epoch [2349/4000] - Train Loss: 0.0359, Val Loss: 0.1225\n",
      "Epoch [2350/4000] - Train Loss: 0.0347, Val Loss: 0.1190\n",
      "Epoch [2351/4000] - Train Loss: 0.0337, Val Loss: 0.1195\n",
      "Epoch [2352/4000] - Train Loss: 0.0340, Val Loss: 0.1174\n",
      "Epoch [2353/4000] - Train Loss: 0.0342, Val Loss: 0.1182\n",
      "Epoch [2354/4000] - Train Loss: 0.0346, Val Loss: 0.1189\n",
      "Epoch [2355/4000] - Train Loss: 0.0333, Val Loss: 0.1168\n",
      "Epoch [2356/4000] - Train Loss: 0.0332, Val Loss: 0.1173\n",
      "Epoch [2357/4000] - Train Loss: 0.0335, Val Loss: 0.1173\n",
      "Epoch [2358/4000] - Train Loss: 0.0336, Val Loss: 0.1205\n",
      "Epoch [2359/4000] - Train Loss: 0.0341, Val Loss: 0.1207\n",
      "Epoch [2360/4000] - Train Loss: 0.0349, Val Loss: 0.1174\n",
      "Epoch [2361/4000] - Train Loss: 0.0348, Val Loss: 0.1140\n",
      "Epoch [2362/4000] - Train Loss: 0.0337, Val Loss: 0.1169\n",
      "Epoch [2363/4000] - Train Loss: 0.0402, Val Loss: 0.1217\n",
      "Epoch [2364/4000] - Train Loss: 0.0385, Val Loss: 0.1172\n",
      "Epoch [2365/4000] - Train Loss: 0.0351, Val Loss: 0.1209\n",
      "Epoch [2366/4000] - Train Loss: 0.0355, Val Loss: 0.1216\n",
      "Epoch [2367/4000] - Train Loss: 0.0348, Val Loss: 0.1178\n",
      "Epoch [2368/4000] - Train Loss: 0.0334, Val Loss: 0.1183\n",
      "Epoch [2369/4000] - Train Loss: 0.0338, Val Loss: 0.1162\n",
      "Epoch [2370/4000] - Train Loss: 0.0333, Val Loss: 0.1161\n",
      "Epoch [2371/4000] - Train Loss: 0.0340, Val Loss: 0.1175\n",
      "Epoch [2372/4000] - Train Loss: 0.0333, Val Loss: 0.1181\n",
      "Epoch [2373/4000] - Train Loss: 0.0338, Val Loss: 0.1166\n",
      "Epoch [2374/4000] - Train Loss: 0.0338, Val Loss: 0.1175\n",
      "Epoch [2375/4000] - Train Loss: 0.0333, Val Loss: 0.1171\n",
      "Epoch [2376/4000] - Train Loss: 0.0327, Val Loss: 0.1171\n",
      "Epoch [2377/4000] - Train Loss: 0.0331, Val Loss: 0.1191\n",
      "Epoch [2378/4000] - Train Loss: 0.0331, Val Loss: 0.1168\n",
      "Epoch [2379/4000] - Train Loss: 0.0327, Val Loss: 0.1187\n",
      "Epoch [2380/4000] - Train Loss: 0.0334, Val Loss: 0.1160\n",
      "Epoch [2381/4000] - Train Loss: 0.0331, Val Loss: 0.1192\n",
      "Epoch [2382/4000] - Train Loss: 0.0340, Val Loss: 0.1164\n",
      "Epoch [2383/4000] - Train Loss: 0.0330, Val Loss: 0.1181\n",
      "Epoch [2384/4000] - Train Loss: 0.0334, Val Loss: 0.1170\n",
      "Epoch [2385/4000] - Train Loss: 0.0336, Val Loss: 0.1179\n",
      "Epoch [2386/4000] - Train Loss: 0.0330, Val Loss: 0.1176\n",
      "Epoch [2387/4000] - Train Loss: 0.0330, Val Loss: 0.1172\n",
      "Epoch [2388/4000] - Train Loss: 0.0338, Val Loss: 0.1225\n",
      "Epoch [2389/4000] - Train Loss: 0.0346, Val Loss: 0.1182\n",
      "Epoch [2390/4000] - Train Loss: 0.0336, Val Loss: 0.1174\n",
      "Epoch [2391/4000] - Train Loss: 0.0335, Val Loss: 0.1163\n",
      "Epoch [2392/4000] - Train Loss: 0.0336, Val Loss: 0.1195\n",
      "Epoch [2393/4000] - Train Loss: 0.0331, Val Loss: 0.1213\n",
      "Epoch [2394/4000] - Train Loss: 0.0333, Val Loss: 0.1185\n",
      "Epoch [2395/4000] - Train Loss: 0.0339, Val Loss: 0.1204\n",
      "Epoch [2396/4000] - Train Loss: 0.0339, Val Loss: 0.1176\n",
      "Epoch [2397/4000] - Train Loss: 0.0348, Val Loss: 0.1158\n",
      "Epoch [2398/4000] - Train Loss: 0.0339, Val Loss: 0.1185\n",
      "Epoch [2399/4000] - Train Loss: 0.0344, Val Loss: 0.1153\n",
      "Epoch [2400/4000] - Train Loss: 0.0343, Val Loss: 0.1203\n",
      "Epoch [2401/4000] - Train Loss: 0.0358, Val Loss: 0.1204\n",
      "Epoch [2402/4000] - Train Loss: 0.0363, Val Loss: 0.1202\n",
      "Epoch [2403/4000] - Train Loss: 0.0350, Val Loss: 0.1233\n",
      "Epoch [2404/4000] - Train Loss: 0.0338, Val Loss: 0.1178\n",
      "Epoch [2405/4000] - Train Loss: 0.0329, Val Loss: 0.1169\n",
      "Epoch [2406/4000] - Train Loss: 0.0327, Val Loss: 0.1179\n",
      "Epoch [2407/4000] - Train Loss: 0.0327, Val Loss: 0.1174\n",
      "Epoch [2408/4000] - Train Loss: 0.0328, Val Loss: 0.1189\n",
      "Epoch [2409/4000] - Train Loss: 0.0335, Val Loss: 0.1184\n",
      "Epoch [2410/4000] - Train Loss: 0.0325, Val Loss: 0.1174\n",
      "Epoch [2411/4000] - Train Loss: 0.0333, Val Loss: 0.1163\n",
      "Epoch [2412/4000] - Train Loss: 0.0330, Val Loss: 0.1167\n",
      "Epoch [2413/4000] - Train Loss: 0.0324, Val Loss: 0.1173\n",
      "Epoch [2414/4000] - Train Loss: 0.0331, Val Loss: 0.1194\n",
      "Epoch [2415/4000] - Train Loss: 0.0336, Val Loss: 0.1178\n",
      "Epoch [2416/4000] - Train Loss: 0.0333, Val Loss: 0.1185\n",
      "Epoch [2417/4000] - Train Loss: 0.0328, Val Loss: 0.1186\n",
      "Epoch [2418/4000] - Train Loss: 0.0324, Val Loss: 0.1180\n",
      "Epoch [2419/4000] - Train Loss: 0.0339, Val Loss: 0.1183\n",
      "Epoch [2420/4000] - Train Loss: 0.0343, Val Loss: 0.1169\n",
      "Epoch [2421/4000] - Train Loss: 0.0330, Val Loss: 0.1183\n",
      "Epoch [2422/4000] - Train Loss: 0.0336, Val Loss: 0.1179\n",
      "Epoch [2423/4000] - Train Loss: 0.0332, Val Loss: 0.1180\n",
      "Epoch [2424/4000] - Train Loss: 0.0345, Val Loss: 0.1180\n",
      "Epoch [2425/4000] - Train Loss: 0.0352, Val Loss: 0.1227\n",
      "Epoch [2426/4000] - Train Loss: 0.0359, Val Loss: 0.1154\n",
      "Epoch [2427/4000] - Train Loss: 0.0344, Val Loss: 0.1180\n",
      "Epoch [2428/4000] - Train Loss: 0.0346, Val Loss: 0.1158\n",
      "Epoch [2429/4000] - Train Loss: 0.0342, Val Loss: 0.1180\n",
      "Epoch [2430/4000] - Train Loss: 0.0336, Val Loss: 0.1184\n",
      "Epoch [2431/4000] - Train Loss: 0.0342, Val Loss: 0.1222\n",
      "Epoch [2432/4000] - Train Loss: 0.0343, Val Loss: 0.1172\n",
      "Epoch [2433/4000] - Train Loss: 0.0334, Val Loss: 0.1146\n",
      "Epoch [2434/4000] - Train Loss: 0.0339, Val Loss: 0.1165\n",
      "Epoch [2435/4000] - Train Loss: 0.0329, Val Loss: 0.1166\n",
      "Epoch [2436/4000] - Train Loss: 0.0340, Val Loss: 0.1175\n",
      "Epoch [2437/4000] - Train Loss: 0.0331, Val Loss: 0.1160\n",
      "Epoch [2438/4000] - Train Loss: 0.0325, Val Loss: 0.1176\n",
      "Epoch [2439/4000] - Train Loss: 0.0331, Val Loss: 0.1169\n",
      "Epoch [2440/4000] - Train Loss: 0.0341, Val Loss: 0.1174\n",
      "Epoch [2441/4000] - Train Loss: 0.0334, Val Loss: 0.1163\n",
      "Epoch [2442/4000] - Train Loss: 0.0322, Val Loss: 0.1188\n",
      "Epoch [2443/4000] - Train Loss: 0.0322, Val Loss: 0.1177\n",
      "Epoch [2444/4000] - Train Loss: 0.0326, Val Loss: 0.1173\n",
      "Epoch [2445/4000] - Train Loss: 0.0324, Val Loss: 0.1181\n",
      "Epoch [2446/4000] - Train Loss: 0.0325, Val Loss: 0.1163\n",
      "Epoch [2447/4000] - Train Loss: 0.0335, Val Loss: 0.1178\n",
      "Epoch [2448/4000] - Train Loss: 0.0328, Val Loss: 0.1162\n",
      "Epoch [2449/4000] - Train Loss: 0.0321, Val Loss: 0.1163\n",
      "Epoch [2450/4000] - Train Loss: 0.0327, Val Loss: 0.1196\n",
      "Epoch [2451/4000] - Train Loss: 0.0330, Val Loss: 0.1164\n",
      "Epoch [2452/4000] - Train Loss: 0.0319, Val Loss: 0.1186\n",
      "Epoch [2453/4000] - Train Loss: 0.0326, Val Loss: 0.1166\n",
      "Epoch [2454/4000] - Train Loss: 0.0323, Val Loss: 0.1181\n",
      "Epoch [2455/4000] - Train Loss: 0.0327, Val Loss: 0.1165\n",
      "Epoch [2456/4000] - Train Loss: 0.0330, Val Loss: 0.1186\n",
      "Epoch [2457/4000] - Train Loss: 0.0336, Val Loss: 0.1175\n",
      "Epoch [2458/4000] - Train Loss: 0.0347, Val Loss: 0.1184\n",
      "Epoch [2459/4000] - Train Loss: 0.0351, Val Loss: 0.1190\n",
      "Epoch [2460/4000] - Train Loss: 0.0355, Val Loss: 0.1182\n",
      "Epoch [2461/4000] - Train Loss: 0.0351, Val Loss: 0.1168\n",
      "Epoch [2462/4000] - Train Loss: 0.0341, Val Loss: 0.1168\n",
      "Epoch [2463/4000] - Train Loss: 0.0334, Val Loss: 0.1179\n",
      "Epoch [2464/4000] - Train Loss: 0.0341, Val Loss: 0.1177\n",
      "Epoch [2465/4000] - Train Loss: 0.0324, Val Loss: 0.1171\n",
      "Epoch [2466/4000] - Train Loss: 0.0325, Val Loss: 0.1159\n",
      "Epoch [2467/4000] - Train Loss: 0.0325, Val Loss: 0.1165\n",
      "Epoch [2468/4000] - Train Loss: 0.0324, Val Loss: 0.1172\n",
      "Epoch [2469/4000] - Train Loss: 0.0330, Val Loss: 0.1185\n",
      "Epoch [2470/4000] - Train Loss: 0.0336, Val Loss: 0.1189\n",
      "Epoch [2471/4000] - Train Loss: 0.0334, Val Loss: 0.1198\n",
      "Epoch [2472/4000] - Train Loss: 0.0335, Val Loss: 0.1176\n",
      "Epoch [2473/4000] - Train Loss: 0.0328, Val Loss: 0.1167\n",
      "Epoch [2474/4000] - Train Loss: 0.0324, Val Loss: 0.1165\n",
      "Epoch [2475/4000] - Train Loss: 0.0320, Val Loss: 0.1182\n",
      "Epoch [2476/4000] - Train Loss: 0.0333, Val Loss: 0.1168\n",
      "Epoch [2477/4000] - Train Loss: 0.0326, Val Loss: 0.1166\n",
      "Epoch [2478/4000] - Train Loss: 0.0319, Val Loss: 0.1188\n",
      "Epoch [2479/4000] - Train Loss: 0.0333, Val Loss: 0.1185\n",
      "Epoch [2480/4000] - Train Loss: 0.0334, Val Loss: 0.1162\n",
      "Epoch [2481/4000] - Train Loss: 0.0326, Val Loss: 0.1188\n",
      "Epoch [2482/4000] - Train Loss: 0.0327, Val Loss: 0.1172\n",
      "Epoch [2483/4000] - Train Loss: 0.0332, Val Loss: 0.1178\n",
      "Epoch [2484/4000] - Train Loss: 0.0335, Val Loss: 0.1179\n",
      "Epoch [2485/4000] - Train Loss: 0.0325, Val Loss: 0.1175\n",
      "Epoch [2486/4000] - Train Loss: 0.0323, Val Loss: 0.1176\n",
      "Epoch [2487/4000] - Train Loss: 0.0321, Val Loss: 0.1178\n",
      "Epoch [2488/4000] - Train Loss: 0.0328, Val Loss: 0.1181\n",
      "Epoch [2489/4000] - Train Loss: 0.0324, Val Loss: 0.1174\n",
      "Epoch [2490/4000] - Train Loss: 0.0325, Val Loss: 0.1184\n",
      "Epoch [2491/4000] - Train Loss: 0.0337, Val Loss: 0.1174\n",
      "Epoch [2492/4000] - Train Loss: 0.0324, Val Loss: 0.1168\n",
      "Epoch [2493/4000] - Train Loss: 0.0329, Val Loss: 0.1181\n",
      "Epoch [2494/4000] - Train Loss: 0.0321, Val Loss: 0.1187\n",
      "Epoch [2495/4000] - Train Loss: 0.0325, Val Loss: 0.1173\n",
      "Epoch [2496/4000] - Train Loss: 0.0317, Val Loss: 0.1181\n",
      "Epoch [2497/4000] - Train Loss: 0.0328, Val Loss: 0.1180\n",
      "Epoch [2498/4000] - Train Loss: 0.0334, Val Loss: 0.1168\n",
      "Epoch [2499/4000] - Train Loss: 0.0322, Val Loss: 0.1178\n",
      "Epoch [2500/4000] - Train Loss: 0.0325, Val Loss: 0.1174\n",
      "Epoch [2501/4000] - Train Loss: 0.0329, Val Loss: 0.1187\n",
      "Epoch [2502/4000] - Train Loss: 0.0341, Val Loss: 0.1208\n",
      "Epoch [2503/4000] - Train Loss: 0.0342, Val Loss: 0.1183\n",
      "Epoch [2504/4000] - Train Loss: 0.0332, Val Loss: 0.1152\n",
      "Epoch [2505/4000] - Train Loss: 0.0336, Val Loss: 0.1181\n",
      "Epoch [2506/4000] - Train Loss: 0.0331, Val Loss: 0.1189\n",
      "Epoch [2507/4000] - Train Loss: 0.0330, Val Loss: 0.1207\n",
      "Epoch [2508/4000] - Train Loss: 0.0330, Val Loss: 0.1196\n",
      "Epoch [2509/4000] - Train Loss: 0.0324, Val Loss: 0.1172\n",
      "Epoch [2510/4000] - Train Loss: 0.0321, Val Loss: 0.1185\n",
      "Epoch [2511/4000] - Train Loss: 0.0316, Val Loss: 0.1162\n",
      "Epoch [2512/4000] - Train Loss: 0.0324, Val Loss: 0.1170\n",
      "Epoch [2513/4000] - Train Loss: 0.0323, Val Loss: 0.1165\n",
      "Epoch [2514/4000] - Train Loss: 0.0323, Val Loss: 0.1170\n",
      "Epoch [2515/4000] - Train Loss: 0.0325, Val Loss: 0.1184\n",
      "Epoch [2516/4000] - Train Loss: 0.0329, Val Loss: 0.1169\n",
      "Epoch [2517/4000] - Train Loss: 0.0330, Val Loss: 0.1170\n",
      "Epoch [2518/4000] - Train Loss: 0.0324, Val Loss: 0.1172\n",
      "Epoch [2519/4000] - Train Loss: 0.0331, Val Loss: 0.1169\n",
      "Epoch [2520/4000] - Train Loss: 0.0326, Val Loss: 0.1196\n",
      "Epoch [2521/4000] - Train Loss: 0.0323, Val Loss: 0.1206\n",
      "Epoch [2522/4000] - Train Loss: 0.0322, Val Loss: 0.1186\n",
      "Epoch [2523/4000] - Train Loss: 0.0329, Val Loss: 0.1188\n",
      "Epoch [2524/4000] - Train Loss: 0.0324, Val Loss: 0.1179\n",
      "Epoch [2525/4000] - Train Loss: 0.0330, Val Loss: 0.1184\n",
      "Epoch [2526/4000] - Train Loss: 0.0321, Val Loss: 0.1168\n",
      "Epoch [2527/4000] - Train Loss: 0.0327, Val Loss: 0.1173\n",
      "Epoch [2528/4000] - Train Loss: 0.0320, Val Loss: 0.1170\n",
      "Epoch [2529/4000] - Train Loss: 0.0323, Val Loss: 0.1186\n",
      "Epoch [2530/4000] - Train Loss: 0.0334, Val Loss: 0.1199\n",
      "Epoch [2531/4000] - Train Loss: 0.0330, Val Loss: 0.1191\n",
      "Epoch [2532/4000] - Train Loss: 0.0329, Val Loss: 0.1173\n",
      "Epoch [2533/4000] - Train Loss: 0.0349, Val Loss: 0.1196\n",
      "Epoch [2534/4000] - Train Loss: 0.0332, Val Loss: 0.1206\n",
      "Epoch [2535/4000] - Train Loss: 0.0325, Val Loss: 0.1184\n",
      "Epoch [2536/4000] - Train Loss: 0.0337, Val Loss: 0.1182\n",
      "Epoch [2537/4000] - Train Loss: 0.0332, Val Loss: 0.1244\n",
      "Epoch [2538/4000] - Train Loss: 0.0334, Val Loss: 0.1147\n",
      "Epoch [2539/4000] - Train Loss: 0.0341, Val Loss: 0.1172\n",
      "Epoch [2540/4000] - Train Loss: 0.0337, Val Loss: 0.1182\n",
      "Epoch [2541/4000] - Train Loss: 0.0331, Val Loss: 0.1182\n",
      "Epoch [2542/4000] - Train Loss: 0.0329, Val Loss: 0.1174\n",
      "Epoch [2543/4000] - Train Loss: 0.0344, Val Loss: 0.1186\n",
      "Epoch [2544/4000] - Train Loss: 0.0332, Val Loss: 0.1190\n",
      "Epoch [2545/4000] - Train Loss: 0.0330, Val Loss: 0.1174\n",
      "Epoch [2546/4000] - Train Loss: 0.0332, Val Loss: 0.1199\n",
      "Epoch [2547/4000] - Train Loss: 0.0335, Val Loss: 0.1177\n",
      "Epoch [2548/4000] - Train Loss: 0.0336, Val Loss: 0.1183\n",
      "Epoch [2549/4000] - Train Loss: 0.0330, Val Loss: 0.1192\n",
      "Epoch [2550/4000] - Train Loss: 0.0325, Val Loss: 0.1172\n",
      "Epoch [2551/4000] - Train Loss: 0.0332, Val Loss: 0.1176\n",
      "Epoch [2552/4000] - Train Loss: 0.0326, Val Loss: 0.1180\n",
      "Epoch [2553/4000] - Train Loss: 0.0324, Val Loss: 0.1177\n",
      "Epoch [2554/4000] - Train Loss: 0.0326, Val Loss: 0.1164\n",
      "Epoch [2555/4000] - Train Loss: 0.0335, Val Loss: 0.1170\n",
      "Epoch [2556/4000] - Train Loss: 0.0328, Val Loss: 0.1175\n",
      "Epoch [2557/4000] - Train Loss: 0.0331, Val Loss: 0.1175\n",
      "Epoch [2558/4000] - Train Loss: 0.0336, Val Loss: 0.1191\n",
      "Epoch [2559/4000] - Train Loss: 0.0324, Val Loss: 0.1173\n",
      "Epoch [2560/4000] - Train Loss: 0.0325, Val Loss: 0.1183\n",
      "Epoch [2561/4000] - Train Loss: 0.0335, Val Loss: 0.1171\n",
      "Epoch [2562/4000] - Train Loss: 0.0337, Val Loss: 0.1168\n",
      "Epoch [2563/4000] - Train Loss: 0.0328, Val Loss: 0.1194\n",
      "Epoch [2564/4000] - Train Loss: 0.0329, Val Loss: 0.1203\n",
      "Epoch [2565/4000] - Train Loss: 0.0352, Val Loss: 0.1202\n",
      "Epoch [2566/4000] - Train Loss: 0.0339, Val Loss: 0.1186\n",
      "Epoch [2567/4000] - Train Loss: 0.0334, Val Loss: 0.1189\n",
      "Epoch [2568/4000] - Train Loss: 0.0327, Val Loss: 0.1180\n",
      "Epoch [2569/4000] - Train Loss: 0.0332, Val Loss: 0.1173\n",
      "Epoch [2570/4000] - Train Loss: 0.0349, Val Loss: 0.1201\n",
      "Epoch [2571/4000] - Train Loss: 0.0352, Val Loss: 0.1180\n",
      "Epoch [2572/4000] - Train Loss: 0.0354, Val Loss: 0.1201\n",
      "Epoch [2573/4000] - Train Loss: 0.0339, Val Loss: 0.1179\n",
      "Epoch [2574/4000] - Train Loss: 0.0344, Val Loss: 0.1187\n",
      "Epoch [2575/4000] - Train Loss: 0.0331, Val Loss: 0.1168\n",
      "Epoch [2576/4000] - Train Loss: 0.0334, Val Loss: 0.1181\n",
      "Epoch [2577/4000] - Train Loss: 0.0342, Val Loss: 0.1206\n",
      "Epoch [2578/4000] - Train Loss: 0.0335, Val Loss: 0.1211\n",
      "Epoch [2579/4000] - Train Loss: 0.0326, Val Loss: 0.1160\n",
      "Epoch [2580/4000] - Train Loss: 0.0320, Val Loss: 0.1167\n",
      "Epoch [2581/4000] - Train Loss: 0.0322, Val Loss: 0.1160\n",
      "Epoch [2582/4000] - Train Loss: 0.0323, Val Loss: 0.1177\n",
      "Epoch [2583/4000] - Train Loss: 0.0324, Val Loss: 0.1168\n",
      "Epoch [2584/4000] - Train Loss: 0.0322, Val Loss: 0.1169\n",
      "Epoch [2585/4000] - Train Loss: 0.0319, Val Loss: 0.1156\n",
      "Epoch [2586/4000] - Train Loss: 0.0325, Val Loss: 0.1175\n",
      "Epoch [2587/4000] - Train Loss: 0.0325, Val Loss: 0.1160\n",
      "Epoch [2588/4000] - Train Loss: 0.0323, Val Loss: 0.1186\n",
      "Epoch [2589/4000] - Train Loss: 0.0321, Val Loss: 0.1170\n",
      "Epoch [2590/4000] - Train Loss: 0.0325, Val Loss: 0.1162\n",
      "Epoch [2591/4000] - Train Loss: 0.0327, Val Loss: 0.1181\n",
      "Epoch [2592/4000] - Train Loss: 0.0315, Val Loss: 0.1173\n",
      "Epoch [2593/4000] - Train Loss: 0.0320, Val Loss: 0.1182\n",
      "Epoch [2594/4000] - Train Loss: 0.0321, Val Loss: 0.1171\n",
      "Epoch [2595/4000] - Train Loss: 0.0326, Val Loss: 0.1173\n",
      "Epoch [2596/4000] - Train Loss: 0.0326, Val Loss: 0.1156\n",
      "Epoch [2597/4000] - Train Loss: 0.0315, Val Loss: 0.1184\n",
      "Epoch [2598/4000] - Train Loss: 0.0306, Val Loss: 0.1179\n",
      "Epoch [2599/4000] - Train Loss: 0.0321, Val Loss: 0.1185\n",
      "Epoch [2600/4000] - Train Loss: 0.0322, Val Loss: 0.1167\n",
      "Epoch [2601/4000] - Train Loss: 0.0326, Val Loss: 0.1176\n",
      "Epoch [2602/4000] - Train Loss: 0.0320, Val Loss: 0.1155\n",
      "Epoch [2603/4000] - Train Loss: 0.0320, Val Loss: 0.1173\n",
      "Epoch [2604/4000] - Train Loss: 0.0327, Val Loss: 0.1177\n",
      "Epoch [2605/4000] - Train Loss: 0.0328, Val Loss: 0.1161\n",
      "Epoch [2606/4000] - Train Loss: 0.0333, Val Loss: 0.1187\n",
      "Epoch [2607/4000] - Train Loss: 0.0324, Val Loss: 0.1179\n",
      "Epoch [2608/4000] - Train Loss: 0.0322, Val Loss: 0.1172\n",
      "Epoch [2609/4000] - Train Loss: 0.0332, Val Loss: 0.1175\n",
      "Epoch [2610/4000] - Train Loss: 0.0322, Val Loss: 0.1185\n",
      "Epoch [2611/4000] - Train Loss: 0.0321, Val Loss: 0.1172\n",
      "Epoch [2612/4000] - Train Loss: 0.0321, Val Loss: 0.1184\n",
      "Epoch [2613/4000] - Train Loss: 0.0311, Val Loss: 0.1176\n",
      "Epoch [2614/4000] - Train Loss: 0.0314, Val Loss: 0.1175\n",
      "Epoch [2615/4000] - Train Loss: 0.0317, Val Loss: 0.1180\n",
      "Epoch [2616/4000] - Train Loss: 0.0331, Val Loss: 0.1161\n",
      "Epoch [2617/4000] - Train Loss: 0.0323, Val Loss: 0.1179\n",
      "Epoch [2618/4000] - Train Loss: 0.0325, Val Loss: 0.1157\n",
      "Epoch [2619/4000] - Train Loss: 0.0323, Val Loss: 0.1195\n",
      "Epoch [2620/4000] - Train Loss: 0.0320, Val Loss: 0.1172\n",
      "Epoch [2621/4000] - Train Loss: 0.0329, Val Loss: 0.1167\n",
      "Epoch [2622/4000] - Train Loss: 0.0325, Val Loss: 0.1174\n",
      "Epoch [2623/4000] - Train Loss: 0.0322, Val Loss: 0.1183\n",
      "Epoch [2624/4000] - Train Loss: 0.0327, Val Loss: 0.1158\n",
      "Epoch [2625/4000] - Train Loss: 0.0324, Val Loss: 0.1182\n",
      "Epoch [2626/4000] - Train Loss: 0.0327, Val Loss: 0.1194\n",
      "Epoch [2627/4000] - Train Loss: 0.0328, Val Loss: 0.1170\n",
      "Epoch [2628/4000] - Train Loss: 0.0324, Val Loss: 0.1173\n",
      "Epoch [2629/4000] - Train Loss: 0.0328, Val Loss: 0.1188\n",
      "Epoch [2630/4000] - Train Loss: 0.0342, Val Loss: 0.1188\n",
      "Epoch [2631/4000] - Train Loss: 0.0356, Val Loss: 0.1186\n",
      "Epoch [2632/4000] - Train Loss: 0.0336, Val Loss: 0.1170\n",
      "Epoch [2633/4000] - Train Loss: 0.0321, Val Loss: 0.1176\n",
      "Epoch [2634/4000] - Train Loss: 0.0323, Val Loss: 0.1186\n",
      "Epoch [2635/4000] - Train Loss: 0.0331, Val Loss: 0.1163\n",
      "Epoch [2636/4000] - Train Loss: 0.0327, Val Loss: 0.1159\n",
      "Epoch [2637/4000] - Train Loss: 0.0325, Val Loss: 0.1188\n",
      "Epoch [2638/4000] - Train Loss: 0.0326, Val Loss: 0.1189\n",
      "Epoch [2639/4000] - Train Loss: 0.0320, Val Loss: 0.1165\n",
      "Epoch [2640/4000] - Train Loss: 0.0317, Val Loss: 0.1180\n",
      "Epoch [2641/4000] - Train Loss: 0.0321, Val Loss: 0.1174\n",
      "Epoch [2642/4000] - Train Loss: 0.0321, Val Loss: 0.1167\n",
      "Epoch [2643/4000] - Train Loss: 0.0326, Val Loss: 0.1176\n",
      "Epoch [2644/4000] - Train Loss: 0.0322, Val Loss: 0.1188\n",
      "Epoch [2645/4000] - Train Loss: 0.0319, Val Loss: 0.1152\n",
      "Epoch [2646/4000] - Train Loss: 0.0328, Val Loss: 0.1168\n",
      "Epoch [2647/4000] - Train Loss: 0.0328, Val Loss: 0.1175\n",
      "Epoch [2648/4000] - Train Loss: 0.0323, Val Loss: 0.1164\n",
      "Epoch [2649/4000] - Train Loss: 0.0323, Val Loss: 0.1170\n",
      "Epoch [2650/4000] - Train Loss: 0.0328, Val Loss: 0.1145\n",
      "Epoch [2651/4000] - Train Loss: 0.0319, Val Loss: 0.1165\n",
      "Epoch [2652/4000] - Train Loss: 0.0324, Val Loss: 0.1162\n",
      "Epoch [2653/4000] - Train Loss: 0.0335, Val Loss: 0.1164\n",
      "Epoch [2654/4000] - Train Loss: 0.0328, Val Loss: 0.1199\n",
      "Epoch [2655/4000] - Train Loss: 0.0340, Val Loss: 0.1180\n",
      "Epoch [2656/4000] - Train Loss: 0.0331, Val Loss: 0.1195\n",
      "Epoch [2657/4000] - Train Loss: 0.0349, Val Loss: 0.1182\n",
      "Epoch [2658/4000] - Train Loss: 0.0328, Val Loss: 0.1158\n",
      "Epoch [2659/4000] - Train Loss: 0.0324, Val Loss: 0.1168\n",
      "Epoch [2660/4000] - Train Loss: 0.0334, Val Loss: 0.1172\n",
      "Epoch [2661/4000] - Train Loss: 0.0332, Val Loss: 0.1201\n",
      "Epoch [2662/4000] - Train Loss: 0.0322, Val Loss: 0.1170\n",
      "Epoch [2663/4000] - Train Loss: 0.0321, Val Loss: 0.1177\n",
      "Epoch [2664/4000] - Train Loss: 0.0312, Val Loss: 0.1202\n",
      "Epoch [2665/4000] - Train Loss: 0.0320, Val Loss: 0.1180\n",
      "Epoch [2666/4000] - Train Loss: 0.0321, Val Loss: 0.1178\n",
      "Epoch [2667/4000] - Train Loss: 0.0311, Val Loss: 0.1165\n",
      "Epoch [2668/4000] - Train Loss: 0.0313, Val Loss: 0.1173\n",
      "Epoch [2669/4000] - Train Loss: 0.0326, Val Loss: 0.1171\n",
      "Epoch [2670/4000] - Train Loss: 0.0313, Val Loss: 0.1169\n",
      "Epoch [2671/4000] - Train Loss: 0.0315, Val Loss: 0.1168\n",
      "Epoch [2672/4000] - Train Loss: 0.0316, Val Loss: 0.1170\n",
      "Epoch [2673/4000] - Train Loss: 0.0323, Val Loss: 0.1172\n",
      "Epoch [2674/4000] - Train Loss: 0.0312, Val Loss: 0.1186\n",
      "Epoch [2675/4000] - Train Loss: 0.0317, Val Loss: 0.1161\n",
      "Epoch [2676/4000] - Train Loss: 0.0313, Val Loss: 0.1166\n",
      "Epoch [2677/4000] - Train Loss: 0.0326, Val Loss: 0.1177\n",
      "Epoch [2678/4000] - Train Loss: 0.0331, Val Loss: 0.1173\n",
      "Epoch [2679/4000] - Train Loss: 0.0356, Val Loss: 0.1238\n",
      "Epoch [2680/4000] - Train Loss: 0.0370, Val Loss: 0.1278\n",
      "Epoch [2681/4000] - Train Loss: 0.0425, Val Loss: 0.1454\n",
      "Epoch [2682/4000] - Train Loss: 0.0471, Val Loss: 0.1212\n",
      "Epoch [2683/4000] - Train Loss: 0.0386, Val Loss: 0.1259\n",
      "Epoch [2684/4000] - Train Loss: 0.0342, Val Loss: 0.1161\n",
      "Epoch [2685/4000] - Train Loss: 0.0347, Val Loss: 0.1147\n",
      "Epoch [2686/4000] - Train Loss: 0.0340, Val Loss: 0.1193\n",
      "Epoch [2687/4000] - Train Loss: 0.0347, Val Loss: 0.1168\n",
      "Epoch [2688/4000] - Train Loss: 0.0337, Val Loss: 0.1177\n",
      "Epoch [2689/4000] - Train Loss: 0.0335, Val Loss: 0.1186\n",
      "Epoch [2690/4000] - Train Loss: 0.0327, Val Loss: 0.1165\n",
      "Epoch [2691/4000] - Train Loss: 0.0319, Val Loss: 0.1165\n",
      "Epoch [2692/4000] - Train Loss: 0.0318, Val Loss: 0.1163\n",
      "Epoch [2693/4000] - Train Loss: 0.0316, Val Loss: 0.1158\n",
      "Epoch [2694/4000] - Train Loss: 0.0324, Val Loss: 0.1161\n",
      "Epoch [2695/4000] - Train Loss: 0.0316, Val Loss: 0.1163\n",
      "Epoch [2696/4000] - Train Loss: 0.0315, Val Loss: 0.1158\n",
      "Epoch [2697/4000] - Train Loss: 0.0315, Val Loss: 0.1162\n",
      "Epoch [2698/4000] - Train Loss: 0.0315, Val Loss: 0.1168\n",
      "Epoch [2699/4000] - Train Loss: 0.0317, Val Loss: 0.1160\n",
      "Epoch [2700/4000] - Train Loss: 0.0318, Val Loss: 0.1157\n",
      "Epoch [2701/4000] - Train Loss: 0.0311, Val Loss: 0.1165\n",
      "Epoch [2702/4000] - Train Loss: 0.0324, Val Loss: 0.1169\n",
      "Epoch [2703/4000] - Train Loss: 0.0322, Val Loss: 0.1171\n",
      "Epoch [2704/4000] - Train Loss: 0.0311, Val Loss: 0.1165\n",
      "Epoch [2705/4000] - Train Loss: 0.0316, Val Loss: 0.1169\n",
      "Epoch [2706/4000] - Train Loss: 0.0324, Val Loss: 0.1188\n",
      "Epoch [2707/4000] - Train Loss: 0.0318, Val Loss: 0.1158\n",
      "Epoch [2708/4000] - Train Loss: 0.0317, Val Loss: 0.1159\n",
      "Epoch [2709/4000] - Train Loss: 0.0318, Val Loss: 0.1158\n",
      "Epoch [2710/4000] - Train Loss: 0.0319, Val Loss: 0.1163\n",
      "Epoch [2711/4000] - Train Loss: 0.0310, Val Loss: 0.1161\n",
      "Epoch [2712/4000] - Train Loss: 0.0314, Val Loss: 0.1165\n",
      "Epoch [2713/4000] - Train Loss: 0.0314, Val Loss: 0.1165\n",
      "Epoch [2714/4000] - Train Loss: 0.0311, Val Loss: 0.1164\n",
      "Epoch [2715/4000] - Train Loss: 0.0320, Val Loss: 0.1158\n",
      "Epoch [2716/4000] - Train Loss: 0.0319, Val Loss: 0.1176\n",
      "Epoch [2717/4000] - Train Loss: 0.0324, Val Loss: 0.1158\n",
      "Epoch [2718/4000] - Train Loss: 0.0307, Val Loss: 0.1171\n",
      "Epoch [2719/4000] - Train Loss: 0.0316, Val Loss: 0.1164\n",
      "Epoch [2720/4000] - Train Loss: 0.0319, Val Loss: 0.1182\n",
      "Epoch [2721/4000] - Train Loss: 0.0318, Val Loss: 0.1156\n",
      "Epoch [2722/4000] - Train Loss: 0.0320, Val Loss: 0.1172\n",
      "Epoch [2723/4000] - Train Loss: 0.0325, Val Loss: 0.1174\n",
      "Epoch [2724/4000] - Train Loss: 0.0320, Val Loss: 0.1179\n",
      "Epoch [2725/4000] - Train Loss: 0.0318, Val Loss: 0.1174\n",
      "Epoch [2726/4000] - Train Loss: 0.0312, Val Loss: 0.1155\n",
      "Epoch [2727/4000] - Train Loss: 0.0316, Val Loss: 0.1183\n",
      "Epoch [2728/4000] - Train Loss: 0.0318, Val Loss: 0.1163\n",
      "Epoch [2729/4000] - Train Loss: 0.0313, Val Loss: 0.1185\n",
      "Epoch [2730/4000] - Train Loss: 0.0326, Val Loss: 0.1173\n",
      "Epoch [2731/4000] - Train Loss: 0.0323, Val Loss: 0.1172\n",
      "Epoch [2732/4000] - Train Loss: 0.0312, Val Loss: 0.1172\n",
      "Epoch [2733/4000] - Train Loss: 0.0306, Val Loss: 0.1172\n",
      "Epoch [2734/4000] - Train Loss: 0.0311, Val Loss: 0.1164\n",
      "Epoch [2735/4000] - Train Loss: 0.0317, Val Loss: 0.1152\n",
      "Epoch [2736/4000] - Train Loss: 0.0315, Val Loss: 0.1176\n",
      "Epoch [2737/4000] - Train Loss: 0.0317, Val Loss: 0.1170\n",
      "Epoch [2738/4000] - Train Loss: 0.0318, Val Loss: 0.1180\n",
      "Epoch [2739/4000] - Train Loss: 0.0312, Val Loss: 0.1157\n",
      "Epoch [2740/4000] - Train Loss: 0.0307, Val Loss: 0.1170\n",
      "Epoch [2741/4000] - Train Loss: 0.0309, Val Loss: 0.1167\n",
      "Epoch [2742/4000] - Train Loss: 0.0313, Val Loss: 0.1172\n",
      "Epoch [2743/4000] - Train Loss: 0.0311, Val Loss: 0.1176\n",
      "Epoch [2744/4000] - Train Loss: 0.0320, Val Loss: 0.1175\n",
      "Epoch [2745/4000] - Train Loss: 0.0316, Val Loss: 0.1166\n",
      "Epoch [2746/4000] - Train Loss: 0.0319, Val Loss: 0.1177\n",
      "Epoch [2747/4000] - Train Loss: 0.0320, Val Loss: 0.1189\n",
      "Epoch [2748/4000] - Train Loss: 0.0316, Val Loss: 0.1177\n",
      "Epoch [2749/4000] - Train Loss: 0.0316, Val Loss: 0.1179\n",
      "Epoch [2750/4000] - Train Loss: 0.0311, Val Loss: 0.1187\n",
      "Epoch [2751/4000] - Train Loss: 0.0315, Val Loss: 0.1172\n",
      "Epoch [2752/4000] - Train Loss: 0.0312, Val Loss: 0.1190\n",
      "Epoch [2753/4000] - Train Loss: 0.0309, Val Loss: 0.1173\n",
      "Epoch [2754/4000] - Train Loss: 0.0316, Val Loss: 0.1186\n",
      "Epoch [2755/4000] - Train Loss: 0.0317, Val Loss: 0.1183\n",
      "Epoch [2756/4000] - Train Loss: 0.0323, Val Loss: 0.1173\n",
      "Epoch [2757/4000] - Train Loss: 0.0322, Val Loss: 0.1186\n",
      "Epoch [2758/4000] - Train Loss: 0.0321, Val Loss: 0.1160\n",
      "Epoch [2759/4000] - Train Loss: 0.0319, Val Loss: 0.1175\n",
      "Epoch [2760/4000] - Train Loss: 0.0319, Val Loss: 0.1180\n",
      "Epoch [2761/4000] - Train Loss: 0.0320, Val Loss: 0.1190\n",
      "Epoch [2762/4000] - Train Loss: 0.0327, Val Loss: 0.1184\n",
      "Epoch [2763/4000] - Train Loss: 0.0325, Val Loss: 0.1176\n",
      "Epoch [2764/4000] - Train Loss: 0.0318, Val Loss: 0.1164\n",
      "Epoch [2765/4000] - Train Loss: 0.0316, Val Loss: 0.1173\n",
      "Epoch [2766/4000] - Train Loss: 0.0321, Val Loss: 0.1183\n",
      "Epoch [2767/4000] - Train Loss: 0.0314, Val Loss: 0.1193\n",
      "Epoch [2768/4000] - Train Loss: 0.0316, Val Loss: 0.1199\n",
      "Epoch [2769/4000] - Train Loss: 0.0318, Val Loss: 0.1169\n",
      "Epoch [2770/4000] - Train Loss: 0.0316, Val Loss: 0.1171\n",
      "Epoch [2771/4000] - Train Loss: 0.0310, Val Loss: 0.1150\n",
      "Epoch [2772/4000] - Train Loss: 0.0303, Val Loss: 0.1163\n",
      "Epoch [2773/4000] - Train Loss: 0.0316, Val Loss: 0.1166\n",
      "Epoch [2774/4000] - Train Loss: 0.0309, Val Loss: 0.1191\n",
      "Epoch [2775/4000] - Train Loss: 0.0309, Val Loss: 0.1178\n",
      "Epoch [2776/4000] - Train Loss: 0.0317, Val Loss: 0.1167\n",
      "Epoch [2777/4000] - Train Loss: 0.0323, Val Loss: 0.1155\n",
      "Epoch [2778/4000] - Train Loss: 0.0321, Val Loss: 0.1169\n",
      "Epoch [2779/4000] - Train Loss: 0.0311, Val Loss: 0.1170\n",
      "Epoch [2780/4000] - Train Loss: 0.0315, Val Loss: 0.1169\n",
      "Epoch [2781/4000] - Train Loss: 0.0318, Val Loss: 0.1183\n",
      "Epoch [2782/4000] - Train Loss: 0.0315, Val Loss: 0.1165\n",
      "Epoch [2783/4000] - Train Loss: 0.0313, Val Loss: 0.1181\n",
      "Epoch [2784/4000] - Train Loss: 0.0314, Val Loss: 0.1200\n",
      "Epoch [2785/4000] - Train Loss: 0.0317, Val Loss: 0.1162\n",
      "Epoch [2786/4000] - Train Loss: 0.0323, Val Loss: 0.1179\n",
      "Epoch [2787/4000] - Train Loss: 0.0318, Val Loss: 0.1163\n",
      "Epoch [2788/4000] - Train Loss: 0.0314, Val Loss: 0.1204\n",
      "Epoch [2789/4000] - Train Loss: 0.0316, Val Loss: 0.1170\n",
      "Epoch [2790/4000] - Train Loss: 0.0317, Val Loss: 0.1166\n",
      "Epoch [2791/4000] - Train Loss: 0.0313, Val Loss: 0.1155\n",
      "Epoch [2792/4000] - Train Loss: 0.0314, Val Loss: 0.1179\n",
      "Epoch [2793/4000] - Train Loss: 0.0314, Val Loss: 0.1180\n",
      "Epoch [2794/4000] - Train Loss: 0.0312, Val Loss: 0.1177\n",
      "Epoch [2795/4000] - Train Loss: 0.0313, Val Loss: 0.1174\n",
      "Epoch [2796/4000] - Train Loss: 0.0308, Val Loss: 0.1163\n",
      "Epoch [2797/4000] - Train Loss: 0.0314, Val Loss: 0.1163\n",
      "Epoch [2798/4000] - Train Loss: 0.0314, Val Loss: 0.1165\n",
      "Epoch [2799/4000] - Train Loss: 0.0313, Val Loss: 0.1178\n",
      "Epoch [2800/4000] - Train Loss: 0.0325, Val Loss: 0.1157\n",
      "Epoch [2801/4000] - Train Loss: 0.0318, Val Loss: 0.1188\n",
      "Epoch [2802/4000] - Train Loss: 0.0316, Val Loss: 0.1186\n",
      "Epoch [2803/4000] - Train Loss: 0.0312, Val Loss: 0.1184\n",
      "Epoch [2804/4000] - Train Loss: 0.0314, Val Loss: 0.1176\n",
      "Epoch [2805/4000] - Train Loss: 0.0318, Val Loss: 0.1173\n",
      "Epoch [2806/4000] - Train Loss: 0.0311, Val Loss: 0.1165\n",
      "Epoch [2807/4000] - Train Loss: 0.0314, Val Loss: 0.1178\n",
      "Epoch [2808/4000] - Train Loss: 0.0318, Val Loss: 0.1169\n",
      "Epoch [2809/4000] - Train Loss: 0.0325, Val Loss: 0.1163\n",
      "Epoch [2810/4000] - Train Loss: 0.0319, Val Loss: 0.1190\n",
      "Epoch [2811/4000] - Train Loss: 0.0321, Val Loss: 0.1177\n",
      "Epoch [2812/4000] - Train Loss: 0.0337, Val Loss: 0.1198\n",
      "Epoch [2813/4000] - Train Loss: 0.0346, Val Loss: 0.1193\n",
      "Epoch [2814/4000] - Train Loss: 0.0328, Val Loss: 0.1152\n",
      "Epoch [2815/4000] - Train Loss: 0.0319, Val Loss: 0.1202\n",
      "Epoch [2816/4000] - Train Loss: 0.0320, Val Loss: 0.1152\n",
      "Epoch [2817/4000] - Train Loss: 0.0310, Val Loss: 0.1167\n",
      "Epoch [2818/4000] - Train Loss: 0.0316, Val Loss: 0.1173\n",
      "Epoch [2819/4000] - Train Loss: 0.0313, Val Loss: 0.1167\n",
      "Epoch [2820/4000] - Train Loss: 0.0314, Val Loss: 0.1176\n",
      "Epoch [2821/4000] - Train Loss: 0.0313, Val Loss: 0.1183\n",
      "Epoch [2822/4000] - Train Loss: 0.0313, Val Loss: 0.1165\n",
      "Epoch [2823/4000] - Train Loss: 0.0313, Val Loss: 0.1177\n",
      "Epoch [2824/4000] - Train Loss: 0.0305, Val Loss: 0.1170\n",
      "Epoch [2825/4000] - Train Loss: 0.0319, Val Loss: 0.1177\n",
      "Epoch [2826/4000] - Train Loss: 0.0317, Val Loss: 0.1153\n",
      "Epoch [2827/4000] - Train Loss: 0.0313, Val Loss: 0.1175\n",
      "Epoch [2828/4000] - Train Loss: 0.0312, Val Loss: 0.1181\n",
      "Epoch [2829/4000] - Train Loss: 0.0320, Val Loss: 0.1164\n",
      "Epoch [2830/4000] - Train Loss: 0.0313, Val Loss: 0.1167\n",
      "Epoch [2831/4000] - Train Loss: 0.0321, Val Loss: 0.1181\n",
      "Epoch [2832/4000] - Train Loss: 0.0336, Val Loss: 0.1201\n",
      "Epoch [2833/4000] - Train Loss: 0.0320, Val Loss: 0.1173\n",
      "Epoch [2834/4000] - Train Loss: 0.0321, Val Loss: 0.1192\n",
      "Epoch [2835/4000] - Train Loss: 0.0325, Val Loss: 0.1206\n",
      "Epoch [2836/4000] - Train Loss: 0.0318, Val Loss: 0.1159\n",
      "Epoch [2837/4000] - Train Loss: 0.0315, Val Loss: 0.1203\n",
      "Epoch [2838/4000] - Train Loss: 0.0315, Val Loss: 0.1158\n",
      "Epoch [2839/4000] - Train Loss: 0.0317, Val Loss: 0.1188\n",
      "Epoch [2840/4000] - Train Loss: 0.0317, Val Loss: 0.1180\n",
      "Epoch [2841/4000] - Train Loss: 0.0339, Val Loss: 0.1189\n",
      "Epoch [2842/4000] - Train Loss: 0.0329, Val Loss: 0.1180\n",
      "Epoch [2843/4000] - Train Loss: 0.0324, Val Loss: 0.1175\n",
      "Epoch [2844/4000] - Train Loss: 0.0318, Val Loss: 0.1173\n",
      "Epoch [2845/4000] - Train Loss: 0.0312, Val Loss: 0.1186\n",
      "Epoch [2846/4000] - Train Loss: 0.0337, Val Loss: 0.1162\n",
      "Epoch [2847/4000] - Train Loss: 0.0330, Val Loss: 0.1178\n",
      "Epoch [2848/4000] - Train Loss: 0.0317, Val Loss: 0.1152\n",
      "Epoch [2849/4000] - Train Loss: 0.0316, Val Loss: 0.1169\n",
      "Epoch [2850/4000] - Train Loss: 0.0309, Val Loss: 0.1188\n",
      "Epoch [2851/4000] - Train Loss: 0.0306, Val Loss: 0.1167\n",
      "Epoch [2852/4000] - Train Loss: 0.0307, Val Loss: 0.1166\n",
      "Epoch [2853/4000] - Train Loss: 0.0313, Val Loss: 0.1172\n",
      "Epoch [2854/4000] - Train Loss: 0.0316, Val Loss: 0.1172\n",
      "Epoch [2855/4000] - Train Loss: 0.0308, Val Loss: 0.1167\n",
      "Epoch [2856/4000] - Train Loss: 0.0311, Val Loss: 0.1181\n",
      "Epoch [2857/4000] - Train Loss: 0.0313, Val Loss: 0.1191\n",
      "Epoch [2858/4000] - Train Loss: 0.0311, Val Loss: 0.1174\n",
      "Epoch [2859/4000] - Train Loss: 0.0314, Val Loss: 0.1170\n",
      "Epoch [2860/4000] - Train Loss: 0.0307, Val Loss: 0.1174\n",
      "Epoch [2861/4000] - Train Loss: 0.0309, Val Loss: 0.1157\n",
      "Epoch [2862/4000] - Train Loss: 0.0307, Val Loss: 0.1192\n",
      "Epoch [2863/4000] - Train Loss: 0.0311, Val Loss: 0.1163\n",
      "Epoch [2864/4000] - Train Loss: 0.0321, Val Loss: 0.1181\n",
      "Epoch [2865/4000] - Train Loss: 0.0309, Val Loss: 0.1189\n",
      "Epoch [2866/4000] - Train Loss: 0.0308, Val Loss: 0.1196\n",
      "Epoch [2867/4000] - Train Loss: 0.0320, Val Loss: 0.1165\n",
      "Epoch [2868/4000] - Train Loss: 0.0320, Val Loss: 0.1186\n",
      "Epoch [2869/4000] - Train Loss: 0.0312, Val Loss: 0.1186\n",
      "Epoch [2870/4000] - Train Loss: 0.0314, Val Loss: 0.1166\n",
      "Epoch [2871/4000] - Train Loss: 0.0313, Val Loss: 0.1183\n",
      "Epoch [2872/4000] - Train Loss: 0.0319, Val Loss: 0.1190\n",
      "Epoch [2873/4000] - Train Loss: 0.0325, Val Loss: 0.1182\n",
      "Epoch [2874/4000] - Train Loss: 0.0319, Val Loss: 0.1184\n",
      "Epoch [2875/4000] - Train Loss: 0.0338, Val Loss: 0.1179\n",
      "Epoch [2876/4000] - Train Loss: 0.0330, Val Loss: 0.1146\n",
      "Epoch [2877/4000] - Train Loss: 0.0366, Val Loss: 0.1200\n",
      "Epoch [2878/4000] - Train Loss: 0.0354, Val Loss: 0.1222\n",
      "Epoch [2879/4000] - Train Loss: 0.0349, Val Loss: 0.1198\n",
      "Epoch [2880/4000] - Train Loss: 0.0339, Val Loss: 0.1219\n",
      "Epoch [2881/4000] - Train Loss: 0.0334, Val Loss: 0.1184\n",
      "Epoch [2882/4000] - Train Loss: 0.0330, Val Loss: 0.1195\n",
      "Epoch [2883/4000] - Train Loss: 0.0320, Val Loss: 0.1150\n",
      "Epoch [2884/4000] - Train Loss: 0.0311, Val Loss: 0.1173\n",
      "Epoch [2885/4000] - Train Loss: 0.0315, Val Loss: 0.1161\n",
      "Epoch [2886/4000] - Train Loss: 0.0314, Val Loss: 0.1169\n",
      "Epoch [2887/4000] - Train Loss: 0.0306, Val Loss: 0.1165\n",
      "Epoch [2888/4000] - Train Loss: 0.0313, Val Loss: 0.1174\n",
      "Epoch [2889/4000] - Train Loss: 0.0313, Val Loss: 0.1174\n",
      "Epoch [2890/4000] - Train Loss: 0.0320, Val Loss: 0.1178\n",
      "Epoch [2891/4000] - Train Loss: 0.0317, Val Loss: 0.1172\n",
      "Epoch [2892/4000] - Train Loss: 0.0312, Val Loss: 0.1171\n",
      "Epoch [2893/4000] - Train Loss: 0.0305, Val Loss: 0.1172\n",
      "Epoch [2894/4000] - Train Loss: 0.0312, Val Loss: 0.1167\n",
      "Epoch [2895/4000] - Train Loss: 0.0310, Val Loss: 0.1175\n",
      "Epoch [2896/4000] - Train Loss: 0.0302, Val Loss: 0.1178\n",
      "Epoch [2897/4000] - Train Loss: 0.0310, Val Loss: 0.1181\n",
      "Epoch [2898/4000] - Train Loss: 0.0313, Val Loss: 0.1168\n",
      "Epoch [2899/4000] - Train Loss: 0.0312, Val Loss: 0.1172\n",
      "Epoch [2900/4000] - Train Loss: 0.0308, Val Loss: 0.1176\n",
      "Epoch [2901/4000] - Train Loss: 0.0307, Val Loss: 0.1181\n",
      "Epoch [2902/4000] - Train Loss: 0.0314, Val Loss: 0.1186\n",
      "Epoch [2903/4000] - Train Loss: 0.0311, Val Loss: 0.1177\n",
      "Epoch [2904/4000] - Train Loss: 0.0307, Val Loss: 0.1168\n",
      "Epoch [2905/4000] - Train Loss: 0.0308, Val Loss: 0.1172\n",
      "Epoch [2906/4000] - Train Loss: 0.0307, Val Loss: 0.1179\n",
      "Epoch [2907/4000] - Train Loss: 0.0311, Val Loss: 0.1197\n",
      "Epoch [2908/4000] - Train Loss: 0.0311, Val Loss: 0.1182\n",
      "Epoch [2909/4000] - Train Loss: 0.0305, Val Loss: 0.1177\n",
      "Epoch [2910/4000] - Train Loss: 0.0322, Val Loss: 0.1154\n",
      "Epoch [2911/4000] - Train Loss: 0.0316, Val Loss: 0.1179\n",
      "Epoch [2912/4000] - Train Loss: 0.0314, Val Loss: 0.1164\n",
      "Epoch [2913/4000] - Train Loss: 0.0315, Val Loss: 0.1177\n",
      "Epoch [2914/4000] - Train Loss: 0.0311, Val Loss: 0.1154\n",
      "Epoch [2915/4000] - Train Loss: 0.0315, Val Loss: 0.1175\n",
      "Epoch [2916/4000] - Train Loss: 0.0311, Val Loss: 0.1163\n",
      "Epoch [2917/4000] - Train Loss: 0.0313, Val Loss: 0.1172\n",
      "Epoch [2918/4000] - Train Loss: 0.0313, Val Loss: 0.1167\n",
      "Epoch [2919/4000] - Train Loss: 0.0306, Val Loss: 0.1169\n",
      "Epoch [2920/4000] - Train Loss: 0.0306, Val Loss: 0.1174\n",
      "Epoch [2921/4000] - Train Loss: 0.0307, Val Loss: 0.1173\n",
      "Epoch [2922/4000] - Train Loss: 0.0309, Val Loss: 0.1176\n",
      "Epoch [2923/4000] - Train Loss: 0.0303, Val Loss: 0.1167\n",
      "Epoch [2924/4000] - Train Loss: 0.0305, Val Loss: 0.1198\n",
      "Epoch [2925/4000] - Train Loss: 0.0303, Val Loss: 0.1159\n",
      "Epoch [2926/4000] - Train Loss: 0.0314, Val Loss: 0.1169\n",
      "Epoch [2927/4000] - Train Loss: 0.0313, Val Loss: 0.1181\n",
      "Epoch [2928/4000] - Train Loss: 0.0314, Val Loss: 0.1164\n",
      "Epoch [2929/4000] - Train Loss: 0.0311, Val Loss: 0.1203\n",
      "Epoch [2930/4000] - Train Loss: 0.0311, Val Loss: 0.1165\n",
      "Epoch [2931/4000] - Train Loss: 0.0315, Val Loss: 0.1168\n",
      "Epoch [2932/4000] - Train Loss: 0.0317, Val Loss: 0.1199\n",
      "Epoch [2933/4000] - Train Loss: 0.0323, Val Loss: 0.1199\n",
      "Epoch [2934/4000] - Train Loss: 0.0327, Val Loss: 0.1194\n",
      "Epoch [2935/4000] - Train Loss: 0.0325, Val Loss: 0.1194\n",
      "Epoch [2936/4000] - Train Loss: 0.0319, Val Loss: 0.1173\n",
      "Epoch [2937/4000] - Train Loss: 0.0321, Val Loss: 0.1165\n",
      "Epoch [2938/4000] - Train Loss: 0.0325, Val Loss: 0.1173\n",
      "Epoch [2939/4000] - Train Loss: 0.0317, Val Loss: 0.1180\n",
      "Epoch [2940/4000] - Train Loss: 0.0313, Val Loss: 0.1205\n",
      "Epoch [2941/4000] - Train Loss: 0.0324, Val Loss: 0.1170\n",
      "Epoch [2942/4000] - Train Loss: 0.0315, Val Loss: 0.1177\n",
      "Epoch [2943/4000] - Train Loss: 0.0320, Val Loss: 0.1174\n",
      "Epoch [2944/4000] - Train Loss: 0.0318, Val Loss: 0.1183\n",
      "Epoch [2945/4000] - Train Loss: 0.0307, Val Loss: 0.1181\n",
      "Epoch [2946/4000] - Train Loss: 0.0310, Val Loss: 0.1189\n",
      "Epoch [2947/4000] - Train Loss: 0.0312, Val Loss: 0.1157\n",
      "Epoch [2948/4000] - Train Loss: 0.0314, Val Loss: 0.1176\n",
      "Epoch [2949/4000] - Train Loss: 0.0313, Val Loss: 0.1172\n",
      "Epoch [2950/4000] - Train Loss: 0.0332, Val Loss: 0.1182\n",
      "Epoch [2951/4000] - Train Loss: 0.0325, Val Loss: 0.1215\n",
      "Epoch [2952/4000] - Train Loss: 0.0323, Val Loss: 0.1159\n",
      "Epoch [2953/4000] - Train Loss: 0.0313, Val Loss: 0.1177\n",
      "Epoch [2954/4000] - Train Loss: 0.0325, Val Loss: 0.1156\n",
      "Epoch [2955/4000] - Train Loss: 0.0330, Val Loss: 0.1191\n",
      "Epoch [2956/4000] - Train Loss: 0.0323, Val Loss: 0.1169\n",
      "Epoch [2957/4000] - Train Loss: 0.0316, Val Loss: 0.1203\n",
      "Epoch [2958/4000] - Train Loss: 0.0310, Val Loss: 0.1190\n",
      "Epoch [2959/4000] - Train Loss: 0.0311, Val Loss: 0.1177\n",
      "Epoch [2960/4000] - Train Loss: 0.0309, Val Loss: 0.1155\n",
      "Epoch [2961/4000] - Train Loss: 0.0312, Val Loss: 0.1184\n",
      "Epoch [2962/4000] - Train Loss: 0.0308, Val Loss: 0.1160\n",
      "Epoch [2963/4000] - Train Loss: 0.0303, Val Loss: 0.1167\n",
      "Epoch [2964/4000] - Train Loss: 0.0299, Val Loss: 0.1177\n",
      "Epoch [2965/4000] - Train Loss: 0.0305, Val Loss: 0.1173\n",
      "Epoch [2966/4000] - Train Loss: 0.0305, Val Loss: 0.1175\n",
      "Epoch [2967/4000] - Train Loss: 0.0305, Val Loss: 0.1164\n",
      "Epoch [2968/4000] - Train Loss: 0.0301, Val Loss: 0.1169\n",
      "Epoch [2969/4000] - Train Loss: 0.0302, Val Loss: 0.1185\n",
      "Epoch [2970/4000] - Train Loss: 0.0305, Val Loss: 0.1176\n",
      "Epoch [2971/4000] - Train Loss: 0.0305, Val Loss: 0.1168\n",
      "Epoch [2972/4000] - Train Loss: 0.0307, Val Loss: 0.1146\n",
      "Epoch [2973/4000] - Train Loss: 0.0309, Val Loss: 0.1171\n",
      "Epoch [2974/4000] - Train Loss: 0.0317, Val Loss: 0.1184\n",
      "Epoch [2975/4000] - Train Loss: 0.0311, Val Loss: 0.1190\n",
      "Epoch [2976/4000] - Train Loss: 0.0315, Val Loss: 0.1175\n",
      "Epoch [2977/4000] - Train Loss: 0.0334, Val Loss: 0.1158\n",
      "Epoch [2978/4000] - Train Loss: 0.0321, Val Loss: 0.1199\n",
      "Epoch [2979/4000] - Train Loss: 0.0317, Val Loss: 0.1168\n",
      "Epoch [2980/4000] - Train Loss: 0.0310, Val Loss: 0.1181\n",
      "Epoch [2981/4000] - Train Loss: 0.0314, Val Loss: 0.1171\n",
      "Epoch [2982/4000] - Train Loss: 0.0307, Val Loss: 0.1170\n",
      "Epoch [2983/4000] - Train Loss: 0.0306, Val Loss: 0.1159\n",
      "Epoch [2984/4000] - Train Loss: 0.0303, Val Loss: 0.1167\n",
      "Epoch [2985/4000] - Train Loss: 0.0304, Val Loss: 0.1176\n",
      "Epoch [2986/4000] - Train Loss: 0.0300, Val Loss: 0.1191\n",
      "Epoch [2987/4000] - Train Loss: 0.0312, Val Loss: 0.1169\n",
      "Epoch [2988/4000] - Train Loss: 0.0311, Val Loss: 0.1170\n",
      "Epoch [2989/4000] - Train Loss: 0.0304, Val Loss: 0.1167\n",
      "Epoch [2990/4000] - Train Loss: 0.0304, Val Loss: 0.1172\n",
      "Epoch [2991/4000] - Train Loss: 0.0308, Val Loss: 0.1183\n",
      "Epoch [2992/4000] - Train Loss: 0.0314, Val Loss: 0.1182\n",
      "Epoch [2993/4000] - Train Loss: 0.0302, Val Loss: 0.1156\n",
      "Epoch [2994/4000] - Train Loss: 0.0304, Val Loss: 0.1165\n",
      "Epoch [2995/4000] - Train Loss: 0.0309, Val Loss: 0.1161\n",
      "Epoch [2996/4000] - Train Loss: 0.0317, Val Loss: 0.1178\n",
      "Epoch [2997/4000] - Train Loss: 0.0311, Val Loss: 0.1155\n",
      "Epoch [2998/4000] - Train Loss: 0.0304, Val Loss: 0.1167\n",
      "Epoch [2999/4000] - Train Loss: 0.0317, Val Loss: 0.1147\n",
      "Epoch [3000/4000] - Train Loss: 0.0318, Val Loss: 0.1191\n",
      "Epoch [3001/4000] - Train Loss: 0.0330, Val Loss: 0.1189\n",
      "Epoch [3002/4000] - Train Loss: 0.0321, Val Loss: 0.1189\n",
      "Epoch [3003/4000] - Train Loss: 0.0334, Val Loss: 0.1184\n",
      "Epoch [3004/4000] - Train Loss: 0.0320, Val Loss: 0.1162\n",
      "Epoch [3005/4000] - Train Loss: 0.0316, Val Loss: 0.1189\n",
      "Epoch [3006/4000] - Train Loss: 0.0312, Val Loss: 0.1181\n",
      "Epoch [3007/4000] - Train Loss: 0.0315, Val Loss: 0.1191\n",
      "Epoch [3008/4000] - Train Loss: 0.0310, Val Loss: 0.1169\n",
      "Epoch [3009/4000] - Train Loss: 0.0316, Val Loss: 0.1183\n",
      "Epoch [3010/4000] - Train Loss: 0.0303, Val Loss: 0.1171\n",
      "Epoch [3011/4000] - Train Loss: 0.0307, Val Loss: 0.1194\n",
      "Epoch [3012/4000] - Train Loss: 0.0306, Val Loss: 0.1159\n",
      "Epoch [3013/4000] - Train Loss: 0.0304, Val Loss: 0.1186\n",
      "Epoch [3014/4000] - Train Loss: 0.0320, Val Loss: 0.1169\n",
      "Epoch [3015/4000] - Train Loss: 0.0319, Val Loss: 0.1211\n",
      "Epoch [3016/4000] - Train Loss: 0.0324, Val Loss: 0.1143\n",
      "Epoch [3017/4000] - Train Loss: 0.0309, Val Loss: 0.1171\n",
      "Epoch [3018/4000] - Train Loss: 0.0303, Val Loss: 0.1175\n",
      "Epoch [3019/4000] - Train Loss: 0.0306, Val Loss: 0.1183\n",
      "Epoch [3020/4000] - Train Loss: 0.0314, Val Loss: 0.1188\n",
      "Epoch [3021/4000] - Train Loss: 0.0304, Val Loss: 0.1195\n",
      "Epoch [3022/4000] - Train Loss: 0.0309, Val Loss: 0.1192\n",
      "Epoch [3023/4000] - Train Loss: 0.0308, Val Loss: 0.1175\n",
      "Epoch [3024/4000] - Train Loss: 0.0309, Val Loss: 0.1173\n",
      "Epoch [3025/4000] - Train Loss: 0.0313, Val Loss: 0.1175\n",
      "Epoch [3026/4000] - Train Loss: 0.0317, Val Loss: 0.1170\n",
      "Epoch [3027/4000] - Train Loss: 0.0324, Val Loss: 0.1184\n",
      "Epoch [3028/4000] - Train Loss: 0.0327, Val Loss: 0.1182\n",
      "Epoch [3029/4000] - Train Loss: 0.0321, Val Loss: 0.1194\n",
      "Epoch [3030/4000] - Train Loss: 0.0315, Val Loss: 0.1141\n",
      "Epoch [3031/4000] - Train Loss: 0.0319, Val Loss: 0.1168\n",
      "Epoch [3032/4000] - Train Loss: 0.0317, Val Loss: 0.1145\n",
      "Epoch [3033/4000] - Train Loss: 0.0319, Val Loss: 0.1170\n",
      "Epoch [3034/4000] - Train Loss: 0.0312, Val Loss: 0.1158\n",
      "Epoch [3035/4000] - Train Loss: 0.0323, Val Loss: 0.1163\n",
      "Epoch [3036/4000] - Train Loss: 0.0329, Val Loss: 0.1167\n",
      "Epoch [3037/4000] - Train Loss: 0.0314, Val Loss: 0.1162\n",
      "Epoch [3038/4000] - Train Loss: 0.0306, Val Loss: 0.1177\n",
      "Epoch [3039/4000] - Train Loss: 0.0304, Val Loss: 0.1193\n",
      "Epoch [3040/4000] - Train Loss: 0.0310, Val Loss: 0.1167\n",
      "Epoch [3041/4000] - Train Loss: 0.0305, Val Loss: 0.1183\n",
      "Epoch [3042/4000] - Train Loss: 0.0317, Val Loss: 0.1155\n",
      "Epoch [3043/4000] - Train Loss: 0.0307, Val Loss: 0.1167\n",
      "Epoch [3044/4000] - Train Loss: 0.0301, Val Loss: 0.1180\n",
      "Epoch [3045/4000] - Train Loss: 0.0310, Val Loss: 0.1175\n",
      "Epoch [3046/4000] - Train Loss: 0.0309, Val Loss: 0.1161\n",
      "Epoch [3047/4000] - Train Loss: 0.0307, Val Loss: 0.1181\n",
      "Epoch [3048/4000] - Train Loss: 0.0310, Val Loss: 0.1184\n",
      "Epoch [3049/4000] - Train Loss: 0.0313, Val Loss: 0.1168\n",
      "Epoch [3050/4000] - Train Loss: 0.0308, Val Loss: 0.1166\n",
      "Epoch [3051/4000] - Train Loss: 0.0305, Val Loss: 0.1165\n",
      "Epoch [3052/4000] - Train Loss: 0.0303, Val Loss: 0.1170\n",
      "Epoch [3053/4000] - Train Loss: 0.0311, Val Loss: 0.1143\n",
      "Epoch [3054/4000] - Train Loss: 0.0312, Val Loss: 0.1182\n",
      "Epoch [3055/4000] - Train Loss: 0.0309, Val Loss: 0.1171\n",
      "Epoch [3056/4000] - Train Loss: 0.0308, Val Loss: 0.1158\n",
      "Epoch [3057/4000] - Train Loss: 0.0305, Val Loss: 0.1158\n",
      "Epoch [3058/4000] - Train Loss: 0.0301, Val Loss: 0.1175\n",
      "Epoch [3059/4000] - Train Loss: 0.0304, Val Loss: 0.1158\n",
      "Epoch [3060/4000] - Train Loss: 0.0301, Val Loss: 0.1146\n",
      "Epoch [3061/4000] - Train Loss: 0.0301, Val Loss: 0.1167\n",
      "Epoch [3062/4000] - Train Loss: 0.0299, Val Loss: 0.1181\n",
      "Epoch [3063/4000] - Train Loss: 0.0308, Val Loss: 0.1168\n",
      "Epoch [3064/4000] - Train Loss: 0.0313, Val Loss: 0.1175\n",
      "Epoch [3065/4000] - Train Loss: 0.0315, Val Loss: 0.1168\n",
      "Epoch [3066/4000] - Train Loss: 0.0308, Val Loss: 0.1162\n",
      "Epoch [3067/4000] - Train Loss: 0.0311, Val Loss: 0.1171\n",
      "Epoch [3068/4000] - Train Loss: 0.0330, Val Loss: 0.1161\n",
      "Epoch [3069/4000] - Train Loss: 0.0320, Val Loss: 0.1149\n",
      "Epoch [3070/4000] - Train Loss: 0.0306, Val Loss: 0.1157\n",
      "Epoch [3071/4000] - Train Loss: 0.0304, Val Loss: 0.1174\n",
      "Epoch [3072/4000] - Train Loss: 0.0304, Val Loss: 0.1171\n",
      "Epoch [3073/4000] - Train Loss: 0.0321, Val Loss: 0.1196\n",
      "Epoch [3074/4000] - Train Loss: 0.0328, Val Loss: 0.1194\n",
      "Epoch [3075/4000] - Train Loss: 0.0328, Val Loss: 0.1185\n",
      "Epoch [3076/4000] - Train Loss: 0.0323, Val Loss: 0.1162\n",
      "Epoch [3077/4000] - Train Loss: 0.0311, Val Loss: 0.1174\n",
      "Epoch [3078/4000] - Train Loss: 0.0304, Val Loss: 0.1161\n",
      "Epoch [3079/4000] - Train Loss: 0.0306, Val Loss: 0.1185\n",
      "Epoch [3080/4000] - Train Loss: 0.0320, Val Loss: 0.1174\n",
      "Epoch [3081/4000] - Train Loss: 0.0319, Val Loss: 0.1167\n",
      "Epoch [3082/4000] - Train Loss: 0.0306, Val Loss: 0.1165\n",
      "Epoch [3083/4000] - Train Loss: 0.0301, Val Loss: 0.1166\n",
      "Epoch [3084/4000] - Train Loss: 0.0308, Val Loss: 0.1160\n",
      "Epoch [3085/4000] - Train Loss: 0.0301, Val Loss: 0.1163\n",
      "Epoch [3086/4000] - Train Loss: 0.0294, Val Loss: 0.1168\n",
      "Epoch [3087/4000] - Train Loss: 0.0298, Val Loss: 0.1162\n",
      "Epoch [3088/4000] - Train Loss: 0.0308, Val Loss: 0.1169\n",
      "Epoch [3089/4000] - Train Loss: 0.0302, Val Loss: 0.1173\n",
      "Epoch [3090/4000] - Train Loss: 0.0308, Val Loss: 0.1164\n",
      "Epoch [3091/4000] - Train Loss: 0.0313, Val Loss: 0.1171\n",
      "Epoch [3092/4000] - Train Loss: 0.0310, Val Loss: 0.1178\n",
      "Epoch [3093/4000] - Train Loss: 0.0318, Val Loss: 0.1176\n",
      "Epoch [3094/4000] - Train Loss: 0.0306, Val Loss: 0.1165\n",
      "Epoch [3095/4000] - Train Loss: 0.0308, Val Loss: 0.1179\n",
      "Epoch [3096/4000] - Train Loss: 0.0313, Val Loss: 0.1165\n",
      "Epoch [3097/4000] - Train Loss: 0.0311, Val Loss: 0.1176\n",
      "Epoch [3098/4000] - Train Loss: 0.0306, Val Loss: 0.1164\n",
      "Epoch [3099/4000] - Train Loss: 0.0306, Val Loss: 0.1166\n",
      "Epoch [3100/4000] - Train Loss: 0.0306, Val Loss: 0.1174\n",
      "Epoch [3101/4000] - Train Loss: 0.0301, Val Loss: 0.1173\n",
      "Epoch [3102/4000] - Train Loss: 0.0308, Val Loss: 0.1186\n",
      "Epoch [3103/4000] - Train Loss: 0.0304, Val Loss: 0.1159\n",
      "Epoch [3104/4000] - Train Loss: 0.0305, Val Loss: 0.1167\n",
      "Epoch [3105/4000] - Train Loss: 0.0303, Val Loss: 0.1162\n",
      "Epoch [3106/4000] - Train Loss: 0.0304, Val Loss: 0.1179\n",
      "Epoch [3107/4000] - Train Loss: 0.0307, Val Loss: 0.1163\n",
      "Epoch [3108/4000] - Train Loss: 0.0299, Val Loss: 0.1163\n",
      "Epoch [3109/4000] - Train Loss: 0.0304, Val Loss: 0.1175\n",
      "Epoch [3110/4000] - Train Loss: 0.0301, Val Loss: 0.1181\n",
      "Epoch [3111/4000] - Train Loss: 0.0301, Val Loss: 0.1178\n",
      "Epoch [3112/4000] - Train Loss: 0.0324, Val Loss: 0.1170\n",
      "Epoch [3113/4000] - Train Loss: 0.0315, Val Loss: 0.1167\n",
      "Epoch [3114/4000] - Train Loss: 0.0310, Val Loss: 0.1183\n",
      "Epoch [3115/4000] - Train Loss: 0.0301, Val Loss: 0.1167\n",
      "Epoch [3116/4000] - Train Loss: 0.0302, Val Loss: 0.1155\n",
      "Epoch [3117/4000] - Train Loss: 0.0302, Val Loss: 0.1152\n",
      "Epoch [3118/4000] - Train Loss: 0.0317, Val Loss: 0.1168\n",
      "Epoch [3119/4000] - Train Loss: 0.0300, Val Loss: 0.1186\n",
      "Epoch [3120/4000] - Train Loss: 0.0310, Val Loss: 0.1168\n",
      "Epoch [3121/4000] - Train Loss: 0.0310, Val Loss: 0.1170\n",
      "Epoch [3122/4000] - Train Loss: 0.0303, Val Loss: 0.1155\n",
      "Epoch [3123/4000] - Train Loss: 0.0304, Val Loss: 0.1177\n",
      "Epoch [3124/4000] - Train Loss: 0.0304, Val Loss: 0.1167\n",
      "Epoch [3125/4000] - Train Loss: 0.0301, Val Loss: 0.1191\n",
      "Epoch [3126/4000] - Train Loss: 0.0307, Val Loss: 0.1172\n",
      "Epoch [3127/4000] - Train Loss: 0.0309, Val Loss: 0.1175\n",
      "Epoch [3128/4000] - Train Loss: 0.0311, Val Loss: 0.1173\n",
      "Epoch [3129/4000] - Train Loss: 0.0304, Val Loss: 0.1180\n",
      "Epoch [3130/4000] - Train Loss: 0.0299, Val Loss: 0.1168\n",
      "Epoch [3131/4000] - Train Loss: 0.0306, Val Loss: 0.1169\n",
      "Epoch [3132/4000] - Train Loss: 0.0312, Val Loss: 0.1156\n",
      "Epoch [3133/4000] - Train Loss: 0.0307, Val Loss: 0.1198\n",
      "Epoch [3134/4000] - Train Loss: 0.0301, Val Loss: 0.1169\n",
      "Epoch [3135/4000] - Train Loss: 0.0304, Val Loss: 0.1175\n",
      "Epoch [3136/4000] - Train Loss: 0.0307, Val Loss: 0.1172\n",
      "Epoch [3137/4000] - Train Loss: 0.0297, Val Loss: 0.1156\n",
      "Epoch [3138/4000] - Train Loss: 0.0311, Val Loss: 0.1159\n",
      "Epoch [3139/4000] - Train Loss: 0.0321, Val Loss: 0.1176\n",
      "Epoch [3140/4000] - Train Loss: 0.0318, Val Loss: 0.1147\n",
      "Epoch [3141/4000] - Train Loss: 0.0326, Val Loss: 0.1216\n",
      "Epoch [3142/4000] - Train Loss: 0.0323, Val Loss: 0.1170\n",
      "Epoch [3143/4000] - Train Loss: 0.0326, Val Loss: 0.1185\n",
      "Epoch [3144/4000] - Train Loss: 0.0332, Val Loss: 0.1185\n",
      "Epoch [3145/4000] - Train Loss: 0.0339, Val Loss: 0.1185\n",
      "Epoch [3146/4000] - Train Loss: 0.0331, Val Loss: 0.1173\n",
      "Epoch [3147/4000] - Train Loss: 0.0317, Val Loss: 0.1200\n",
      "Epoch [3148/4000] - Train Loss: 0.0312, Val Loss: 0.1141\n",
      "Epoch [3149/4000] - Train Loss: 0.0305, Val Loss: 0.1157\n",
      "Epoch [3150/4000] - Train Loss: 0.0317, Val Loss: 0.1165\n",
      "Epoch [3151/4000] - Train Loss: 0.0325, Val Loss: 0.1185\n",
      "Epoch [3152/4000] - Train Loss: 0.0314, Val Loss: 0.1151\n",
      "Epoch [3153/4000] - Train Loss: 0.0302, Val Loss: 0.1170\n",
      "Epoch [3154/4000] - Train Loss: 0.0308, Val Loss: 0.1173\n",
      "Epoch [3155/4000] - Train Loss: 0.0309, Val Loss: 0.1165\n",
      "Epoch [3156/4000] - Train Loss: 0.0300, Val Loss: 0.1152\n",
      "Epoch [3157/4000] - Train Loss: 0.0303, Val Loss: 0.1164\n",
      "Epoch [3158/4000] - Train Loss: 0.0316, Val Loss: 0.1177\n",
      "Epoch [3159/4000] - Train Loss: 0.0308, Val Loss: 0.1177\n",
      "Epoch [3160/4000] - Train Loss: 0.0307, Val Loss: 0.1188\n",
      "Epoch [3161/4000] - Train Loss: 0.0310, Val Loss: 0.1158\n",
      "Epoch [3162/4000] - Train Loss: 0.0296, Val Loss: 0.1151\n",
      "Epoch [3163/4000] - Train Loss: 0.0301, Val Loss: 0.1159\n",
      "Epoch [3164/4000] - Train Loss: 0.0305, Val Loss: 0.1163\n",
      "Epoch [3165/4000] - Train Loss: 0.0299, Val Loss: 0.1151\n",
      "Epoch [3166/4000] - Train Loss: 0.0304, Val Loss: 0.1172\n",
      "Epoch [3167/4000] - Train Loss: 0.0302, Val Loss: 0.1163\n",
      "Epoch [3168/4000] - Train Loss: 0.0303, Val Loss: 0.1173\n",
      "Epoch [3169/4000] - Train Loss: 0.0300, Val Loss: 0.1149\n",
      "Epoch [3170/4000] - Train Loss: 0.0306, Val Loss: 0.1168\n",
      "Epoch [3171/4000] - Train Loss: 0.0299, Val Loss: 0.1184\n",
      "Epoch [3172/4000] - Train Loss: 0.0306, Val Loss: 0.1182\n",
      "Epoch [3173/4000] - Train Loss: 0.0302, Val Loss: 0.1163\n",
      "Epoch [3174/4000] - Train Loss: 0.0304, Val Loss: 0.1158\n",
      "Epoch [3175/4000] - Train Loss: 0.0302, Val Loss: 0.1169\n",
      "Epoch [3176/4000] - Train Loss: 0.0309, Val Loss: 0.1166\n",
      "Epoch [3177/4000] - Train Loss: 0.0309, Val Loss: 0.1176\n",
      "Epoch [3178/4000] - Train Loss: 0.0313, Val Loss: 0.1145\n",
      "Epoch [3179/4000] - Train Loss: 0.0313, Val Loss: 0.1177\n",
      "Epoch [3180/4000] - Train Loss: 0.0304, Val Loss: 0.1157\n",
      "Epoch [3181/4000] - Train Loss: 0.0300, Val Loss: 0.1156\n",
      "Epoch [3182/4000] - Train Loss: 0.0298, Val Loss: 0.1175\n",
      "Epoch [3183/4000] - Train Loss: 0.0315, Val Loss: 0.1213\n",
      "Epoch [3184/4000] - Train Loss: 0.0315, Val Loss: 0.1166\n",
      "Epoch [3185/4000] - Train Loss: 0.0331, Val Loss: 0.1170\n",
      "Epoch [3186/4000] - Train Loss: 0.0310, Val Loss: 0.1159\n",
      "Epoch [3187/4000] - Train Loss: 0.0327, Val Loss: 0.1154\n",
      "Epoch [3188/4000] - Train Loss: 0.0337, Val Loss: 0.1161\n",
      "Epoch [3189/4000] - Train Loss: 0.0326, Val Loss: 0.1182\n",
      "Epoch [3190/4000] - Train Loss: 0.0313, Val Loss: 0.1143\n",
      "Epoch [3191/4000] - Train Loss: 0.0305, Val Loss: 0.1144\n",
      "Epoch [3192/4000] - Train Loss: 0.0306, Val Loss: 0.1149\n",
      "Epoch [3193/4000] - Train Loss: 0.0304, Val Loss: 0.1180\n",
      "Epoch [3194/4000] - Train Loss: 0.0305, Val Loss: 0.1159\n",
      "Epoch [3195/4000] - Train Loss: 0.0299, Val Loss: 0.1183\n",
      "Epoch [3196/4000] - Train Loss: 0.0313, Val Loss: 0.1151\n",
      "Epoch [3197/4000] - Train Loss: 0.0311, Val Loss: 0.1177\n",
      "Epoch [3198/4000] - Train Loss: 0.0299, Val Loss: 0.1171\n",
      "Epoch [3199/4000] - Train Loss: 0.0296, Val Loss: 0.1155\n",
      "Epoch [3200/4000] - Train Loss: 0.0297, Val Loss: 0.1149\n",
      "Epoch [3201/4000] - Train Loss: 0.0301, Val Loss: 0.1161\n",
      "Epoch [3202/4000] - Train Loss: 0.0303, Val Loss: 0.1166\n",
      "Epoch [3203/4000] - Train Loss: 0.0304, Val Loss: 0.1170\n",
      "Epoch [3204/4000] - Train Loss: 0.0303, Val Loss: 0.1206\n",
      "Epoch [3205/4000] - Train Loss: 0.0302, Val Loss: 0.1170\n",
      "Epoch [3206/4000] - Train Loss: 0.0298, Val Loss: 0.1166\n",
      "Epoch [3207/4000] - Train Loss: 0.0295, Val Loss: 0.1172\n",
      "Epoch [3208/4000] - Train Loss: 0.0296, Val Loss: 0.1164\n",
      "Epoch [3209/4000] - Train Loss: 0.0306, Val Loss: 0.1163\n",
      "Epoch [3210/4000] - Train Loss: 0.0304, Val Loss: 0.1152\n",
      "Epoch [3211/4000] - Train Loss: 0.0293, Val Loss: 0.1159\n",
      "Epoch [3212/4000] - Train Loss: 0.0304, Val Loss: 0.1156\n",
      "Epoch [3213/4000] - Train Loss: 0.0301, Val Loss: 0.1156\n",
      "Epoch [3214/4000] - Train Loss: 0.0298, Val Loss: 0.1167\n",
      "Epoch [3215/4000] - Train Loss: 0.0305, Val Loss: 0.1173\n",
      "Epoch [3216/4000] - Train Loss: 0.0299, Val Loss: 0.1157\n",
      "Epoch [3217/4000] - Train Loss: 0.0303, Val Loss: 0.1159\n",
      "Epoch [3218/4000] - Train Loss: 0.0300, Val Loss: 0.1139\n",
      "Epoch [3219/4000] - Train Loss: 0.0301, Val Loss: 0.1163\n",
      "Epoch [3220/4000] - Train Loss: 0.0303, Val Loss: 0.1151\n",
      "Epoch [3221/4000] - Train Loss: 0.0302, Val Loss: 0.1163\n",
      "Epoch [3222/4000] - Train Loss: 0.0301, Val Loss: 0.1160\n",
      "Epoch [3223/4000] - Train Loss: 0.0303, Val Loss: 0.1173\n",
      "Epoch [3224/4000] - Train Loss: 0.0308, Val Loss: 0.1167\n",
      "Epoch [3225/4000] - Train Loss: 0.0315, Val Loss: 0.1162\n",
      "Epoch [3226/4000] - Train Loss: 0.0312, Val Loss: 0.1155\n",
      "Epoch [3227/4000] - Train Loss: 0.0320, Val Loss: 0.1156\n",
      "Epoch [3228/4000] - Train Loss: 0.0299, Val Loss: 0.1154\n",
      "Epoch [3229/4000] - Train Loss: 0.0305, Val Loss: 0.1159\n",
      "Epoch [3230/4000] - Train Loss: 0.0301, Val Loss: 0.1162\n",
      "Epoch [3231/4000] - Train Loss: 0.0300, Val Loss: 0.1165\n",
      "Epoch [3232/4000] - Train Loss: 0.0298, Val Loss: 0.1171\n",
      "Epoch [3233/4000] - Train Loss: 0.0301, Val Loss: 0.1162\n",
      "Epoch [3234/4000] - Train Loss: 0.0303, Val Loss: 0.1158\n",
      "Epoch [3235/4000] - Train Loss: 0.0318, Val Loss: 0.1188\n",
      "Epoch [3236/4000] - Train Loss: 0.0308, Val Loss: 0.1180\n",
      "Epoch [3237/4000] - Train Loss: 0.0300, Val Loss: 0.1157\n",
      "Epoch [3238/4000] - Train Loss: 0.0300, Val Loss: 0.1169\n",
      "Epoch [3239/4000] - Train Loss: 0.0303, Val Loss: 0.1147\n",
      "Epoch [3240/4000] - Train Loss: 0.0298, Val Loss: 0.1164\n",
      "Epoch [3241/4000] - Train Loss: 0.0302, Val Loss: 0.1155\n",
      "Epoch [3242/4000] - Train Loss: 0.0300, Val Loss: 0.1154\n",
      "Epoch [3243/4000] - Train Loss: 0.0301, Val Loss: 0.1146\n",
      "Epoch [3244/4000] - Train Loss: 0.0296, Val Loss: 0.1168\n",
      "Epoch [3245/4000] - Train Loss: 0.0293, Val Loss: 0.1160\n",
      "Epoch [3246/4000] - Train Loss: 0.0311, Val Loss: 0.1162\n",
      "Epoch [3247/4000] - Train Loss: 0.0304, Val Loss: 0.1147\n",
      "Epoch [3248/4000] - Train Loss: 0.0306, Val Loss: 0.1153\n",
      "Epoch [3249/4000] - Train Loss: 0.0317, Val Loss: 0.1161\n",
      "Epoch [3250/4000] - Train Loss: 0.0308, Val Loss: 0.1178\n",
      "Epoch [3251/4000] - Train Loss: 0.0302, Val Loss: 0.1162\n",
      "Epoch [3252/4000] - Train Loss: 0.0298, Val Loss: 0.1166\n",
      "Epoch [3253/4000] - Train Loss: 0.0302, Val Loss: 0.1176\n",
      "Epoch [3254/4000] - Train Loss: 0.0307, Val Loss: 0.1170\n",
      "Epoch [3255/4000] - Train Loss: 0.0303, Val Loss: 0.1166\n",
      "Epoch [3256/4000] - Train Loss: 0.0309, Val Loss: 0.1146\n",
      "Epoch [3257/4000] - Train Loss: 0.0323, Val Loss: 0.1166\n",
      "Epoch [3258/4000] - Train Loss: 0.0319, Val Loss: 0.1158\n",
      "Epoch [3259/4000] - Train Loss: 0.0321, Val Loss: 0.1226\n",
      "Epoch [3260/4000] - Train Loss: 0.0332, Val Loss: 0.1206\n",
      "Epoch [3261/4000] - Train Loss: 0.0348, Val Loss: 0.1230\n",
      "Epoch [3262/4000] - Train Loss: 0.0341, Val Loss: 0.1182\n",
      "Epoch [3263/4000] - Train Loss: 0.0366, Val Loss: 0.1144\n",
      "Epoch [3264/4000] - Train Loss: 0.0348, Val Loss: 0.1182\n",
      "Epoch [3265/4000] - Train Loss: 0.0321, Val Loss: 0.1177\n",
      "Epoch [3266/4000] - Train Loss: 0.0317, Val Loss: 0.1153\n",
      "Epoch [3267/4000] - Train Loss: 0.0307, Val Loss: 0.1179\n",
      "Epoch [3268/4000] - Train Loss: 0.0299, Val Loss: 0.1166\n",
      "Epoch [3269/4000] - Train Loss: 0.0303, Val Loss: 0.1166\n",
      "Epoch [3270/4000] - Train Loss: 0.0301, Val Loss: 0.1157\n",
      "Epoch [3271/4000] - Train Loss: 0.0309, Val Loss: 0.1149\n",
      "Epoch [3272/4000] - Train Loss: 0.0305, Val Loss: 0.1161\n",
      "Epoch [3273/4000] - Train Loss: 0.0301, Val Loss: 0.1144\n",
      "Epoch [3274/4000] - Train Loss: 0.0299, Val Loss: 0.1157\n",
      "Epoch [3275/4000] - Train Loss: 0.0301, Val Loss: 0.1145\n",
      "Epoch [3276/4000] - Train Loss: 0.0298, Val Loss: 0.1163\n",
      "Epoch [3277/4000] - Train Loss: 0.0300, Val Loss: 0.1162\n",
      "Epoch [3278/4000] - Train Loss: 0.0299, Val Loss: 0.1171\n",
      "Epoch [3279/4000] - Train Loss: 0.0299, Val Loss: 0.1165\n",
      "Epoch [3280/4000] - Train Loss: 0.0323, Val Loss: 0.1163\n",
      "Epoch [3281/4000] - Train Loss: 0.0319, Val Loss: 0.1168\n",
      "Epoch [3282/4000] - Train Loss: 0.0303, Val Loss: 0.1151\n",
      "Epoch [3283/4000] - Train Loss: 0.0299, Val Loss: 0.1164\n",
      "Epoch [3284/4000] - Train Loss: 0.0297, Val Loss: 0.1154\n",
      "Epoch [3285/4000] - Train Loss: 0.0295, Val Loss: 0.1158\n",
      "Epoch [3286/4000] - Train Loss: 0.0309, Val Loss: 0.1136\n",
      "Epoch [3287/4000] - Train Loss: 0.0305, Val Loss: 0.1163\n",
      "Epoch [3288/4000] - Train Loss: 0.0294, Val Loss: 0.1155\n",
      "Epoch [3289/4000] - Train Loss: 0.0303, Val Loss: 0.1156\n",
      "Epoch [3290/4000] - Train Loss: 0.0293, Val Loss: 0.1149\n",
      "Epoch [3291/4000] - Train Loss: 0.0291, Val Loss: 0.1165\n",
      "Epoch [3292/4000] - Train Loss: 0.0301, Val Loss: 0.1181\n",
      "Epoch [3293/4000] - Train Loss: 0.0305, Val Loss: 0.1169\n",
      "Epoch [3294/4000] - Train Loss: 0.0299, Val Loss: 0.1162\n",
      "Epoch [3295/4000] - Train Loss: 0.0298, Val Loss: 0.1169\n",
      "Epoch [3296/4000] - Train Loss: 0.0296, Val Loss: 0.1150\n",
      "Epoch [3297/4000] - Train Loss: 0.0305, Val Loss: 0.1153\n",
      "Epoch [3298/4000] - Train Loss: 0.0301, Val Loss: 0.1158\n",
      "Epoch [3299/4000] - Train Loss: 0.0311, Val Loss: 0.1168\n",
      "Epoch [3300/4000] - Train Loss: 0.0306, Val Loss: 0.1163\n",
      "Epoch [3301/4000] - Train Loss: 0.0299, Val Loss: 0.1169\n",
      "Epoch [3302/4000] - Train Loss: 0.0301, Val Loss: 0.1154\n",
      "Epoch [3303/4000] - Train Loss: 0.0300, Val Loss: 0.1154\n",
      "Epoch [3304/4000] - Train Loss: 0.0294, Val Loss: 0.1161\n",
      "Epoch [3305/4000] - Train Loss: 0.0295, Val Loss: 0.1155\n",
      "Epoch [3306/4000] - Train Loss: 0.0291, Val Loss: 0.1149\n",
      "Epoch [3307/4000] - Train Loss: 0.0290, Val Loss: 0.1160\n",
      "Epoch [3308/4000] - Train Loss: 0.0294, Val Loss: 0.1165\n",
      "Epoch [3309/4000] - Train Loss: 0.0302, Val Loss: 0.1163\n",
      "Epoch [3310/4000] - Train Loss: 0.0294, Val Loss: 0.1146\n",
      "Epoch [3311/4000] - Train Loss: 0.0298, Val Loss: 0.1161\n",
      "Epoch [3312/4000] - Train Loss: 0.0296, Val Loss: 0.1159\n",
      "Epoch [3313/4000] - Train Loss: 0.0301, Val Loss: 0.1158\n",
      "Epoch [3314/4000] - Train Loss: 0.0298, Val Loss: 0.1157\n",
      "Epoch [3315/4000] - Train Loss: 0.0294, Val Loss: 0.1186\n",
      "Epoch [3316/4000] - Train Loss: 0.0303, Val Loss: 0.1152\n",
      "Epoch [3317/4000] - Train Loss: 0.0301, Val Loss: 0.1145\n",
      "Epoch [3318/4000] - Train Loss: 0.0305, Val Loss: 0.1168\n",
      "Epoch [3319/4000] - Train Loss: 0.0306, Val Loss: 0.1162\n",
      "Epoch [3320/4000] - Train Loss: 0.0308, Val Loss: 0.1165\n",
      "Epoch [3321/4000] - Train Loss: 0.0304, Val Loss: 0.1163\n",
      "Epoch [3322/4000] - Train Loss: 0.0299, Val Loss: 0.1160\n",
      "Epoch [3323/4000] - Train Loss: 0.0296, Val Loss: 0.1181\n",
      "Epoch [3324/4000] - Train Loss: 0.0296, Val Loss: 0.1168\n",
      "Epoch [3325/4000] - Train Loss: 0.0297, Val Loss: 0.1160\n",
      "Epoch [3326/4000] - Train Loss: 0.0304, Val Loss: 0.1170\n",
      "Epoch [3327/4000] - Train Loss: 0.0310, Val Loss: 0.1147\n",
      "Epoch [3328/4000] - Train Loss: 0.0306, Val Loss: 0.1175\n",
      "Epoch [3329/4000] - Train Loss: 0.0311, Val Loss: 0.1159\n",
      "Epoch [3330/4000] - Train Loss: 0.0313, Val Loss: 0.1185\n",
      "Epoch [3331/4000] - Train Loss: 0.0308, Val Loss: 0.1146\n",
      "Epoch [3332/4000] - Train Loss: 0.0301, Val Loss: 0.1166\n",
      "Epoch [3333/4000] - Train Loss: 0.0307, Val Loss: 0.1152\n",
      "Epoch [3334/4000] - Train Loss: 0.0299, Val Loss: 0.1151\n",
      "Epoch [3335/4000] - Train Loss: 0.0301, Val Loss: 0.1169\n",
      "Epoch [3336/4000] - Train Loss: 0.0304, Val Loss: 0.1149\n",
      "Epoch [3337/4000] - Train Loss: 0.0295, Val Loss: 0.1152\n",
      "Epoch [3338/4000] - Train Loss: 0.0305, Val Loss: 0.1161\n",
      "Epoch [3339/4000] - Train Loss: 0.0301, Val Loss: 0.1162\n",
      "Epoch [3340/4000] - Train Loss: 0.0297, Val Loss: 0.1171\n",
      "Epoch [3341/4000] - Train Loss: 0.0290, Val Loss: 0.1162\n",
      "Epoch [3342/4000] - Train Loss: 0.0298, Val Loss: 0.1167\n",
      "Epoch [3343/4000] - Train Loss: 0.0303, Val Loss: 0.1171\n",
      "Epoch [3344/4000] - Train Loss: 0.0304, Val Loss: 0.1149\n",
      "Epoch [3345/4000] - Train Loss: 0.0297, Val Loss: 0.1154\n",
      "Epoch [3346/4000] - Train Loss: 0.0300, Val Loss: 0.1160\n",
      "Epoch [3347/4000] - Train Loss: 0.0304, Val Loss: 0.1185\n",
      "Epoch [3348/4000] - Train Loss: 0.0310, Val Loss: 0.1167\n",
      "Epoch [3349/4000] - Train Loss: 0.0307, Val Loss: 0.1172\n",
      "Epoch [3350/4000] - Train Loss: 0.0301, Val Loss: 0.1185\n",
      "Epoch [3351/4000] - Train Loss: 0.0305, Val Loss: 0.1171\n",
      "Epoch [3352/4000] - Train Loss: 0.0301, Val Loss: 0.1147\n",
      "Epoch [3353/4000] - Train Loss: 0.0305, Val Loss: 0.1173\n",
      "Epoch [3354/4000] - Train Loss: 0.0299, Val Loss: 0.1168\n",
      "Epoch [3355/4000] - Train Loss: 0.0302, Val Loss: 0.1143\n",
      "Epoch [3356/4000] - Train Loss: 0.0296, Val Loss: 0.1160\n",
      "Epoch [3357/4000] - Train Loss: 0.0289, Val Loss: 0.1153\n",
      "Epoch [3358/4000] - Train Loss: 0.0295, Val Loss: 0.1166\n",
      "Epoch [3359/4000] - Train Loss: 0.0301, Val Loss: 0.1147\n",
      "Epoch [3360/4000] - Train Loss: 0.0300, Val Loss: 0.1158\n",
      "Epoch [3361/4000] - Train Loss: 0.0302, Val Loss: 0.1170\n",
      "Epoch [3362/4000] - Train Loss: 0.0296, Val Loss: 0.1164\n",
      "Epoch [3363/4000] - Train Loss: 0.0316, Val Loss: 0.1160\n",
      "Epoch [3364/4000] - Train Loss: 0.0305, Val Loss: 0.1162\n",
      "Epoch [3365/4000] - Train Loss: 0.0311, Val Loss: 0.1160\n",
      "Epoch [3366/4000] - Train Loss: 0.0324, Val Loss: 0.1212\n",
      "Epoch [3367/4000] - Train Loss: 0.0343, Val Loss: 0.1161\n",
      "Epoch [3368/4000] - Train Loss: 0.0333, Val Loss: 0.1195\n",
      "Epoch [3369/4000] - Train Loss: 0.0321, Val Loss: 0.1141\n",
      "Epoch [3370/4000] - Train Loss: 0.0308, Val Loss: 0.1174\n",
      "Epoch [3371/4000] - Train Loss: 0.0308, Val Loss: 0.1163\n",
      "Epoch [3372/4000] - Train Loss: 0.0309, Val Loss: 0.1175\n",
      "Epoch [3373/4000] - Train Loss: 0.0305, Val Loss: 0.1165\n",
      "Epoch [3374/4000] - Train Loss: 0.0302, Val Loss: 0.1155\n",
      "Epoch [3375/4000] - Train Loss: 0.0299, Val Loss: 0.1142\n",
      "Epoch [3376/4000] - Train Loss: 0.0301, Val Loss: 0.1159\n",
      "Epoch [3377/4000] - Train Loss: 0.0303, Val Loss: 0.1154\n",
      "Epoch [3378/4000] - Train Loss: 0.0307, Val Loss: 0.1143\n",
      "Epoch [3379/4000] - Train Loss: 0.0303, Val Loss: 0.1175\n",
      "Epoch [3380/4000] - Train Loss: 0.0305, Val Loss: 0.1155\n",
      "Epoch [3381/4000] - Train Loss: 0.0308, Val Loss: 0.1164\n",
      "Epoch [3382/4000] - Train Loss: 0.0299, Val Loss: 0.1151\n",
      "Epoch [3383/4000] - Train Loss: 0.0298, Val Loss: 0.1164\n",
      "Epoch [3384/4000] - Train Loss: 0.0305, Val Loss: 0.1163\n",
      "Epoch [3385/4000] - Train Loss: 0.0306, Val Loss: 0.1177\n",
      "Epoch [3386/4000] - Train Loss: 0.0304, Val Loss: 0.1162\n",
      "Epoch [3387/4000] - Train Loss: 0.0306, Val Loss: 0.1152\n",
      "Epoch [3388/4000] - Train Loss: 0.0300, Val Loss: 0.1161\n",
      "Epoch [3389/4000] - Train Loss: 0.0307, Val Loss: 0.1181\n",
      "Epoch [3390/4000] - Train Loss: 0.0321, Val Loss: 0.1192\n",
      "Epoch [3391/4000] - Train Loss: 0.0320, Val Loss: 0.1163\n",
      "Epoch [3392/4000] - Train Loss: 0.0309, Val Loss: 0.1169\n",
      "Epoch [3393/4000] - Train Loss: 0.0308, Val Loss: 0.1165\n",
      "Epoch [3394/4000] - Train Loss: 0.0305, Val Loss: 0.1159\n",
      "Epoch [3395/4000] - Train Loss: 0.0298, Val Loss: 0.1150\n",
      "Epoch [3396/4000] - Train Loss: 0.0292, Val Loss: 0.1167\n",
      "Epoch [3397/4000] - Train Loss: 0.0294, Val Loss: 0.1142\n",
      "Epoch [3398/4000] - Train Loss: 0.0300, Val Loss: 0.1153\n",
      "Epoch [3399/4000] - Train Loss: 0.0295, Val Loss: 0.1155\n",
      "Epoch [3400/4000] - Train Loss: 0.0289, Val Loss: 0.1142\n",
      "Epoch [3401/4000] - Train Loss: 0.0291, Val Loss: 0.1161\n",
      "Epoch [3402/4000] - Train Loss: 0.0300, Val Loss: 0.1147\n",
      "Epoch [3403/4000] - Train Loss: 0.0291, Val Loss: 0.1159\n",
      "Epoch [3404/4000] - Train Loss: 0.0297, Val Loss: 0.1146\n",
      "Epoch [3405/4000] - Train Loss: 0.0291, Val Loss: 0.1160\n",
      "Epoch [3406/4000] - Train Loss: 0.0289, Val Loss: 0.1143\n",
      "Epoch [3407/4000] - Train Loss: 0.0288, Val Loss: 0.1149\n",
      "Epoch [3408/4000] - Train Loss: 0.0288, Val Loss: 0.1151\n",
      "Epoch [3409/4000] - Train Loss: 0.0297, Val Loss: 0.1164\n",
      "Epoch [3410/4000] - Train Loss: 0.0297, Val Loss: 0.1157\n",
      "Epoch [3411/4000] - Train Loss: 0.0297, Val Loss: 0.1154\n",
      "Epoch [3412/4000] - Train Loss: 0.0291, Val Loss: 0.1155\n",
      "Epoch [3413/4000] - Train Loss: 0.0304, Val Loss: 0.1165\n",
      "Epoch [3414/4000] - Train Loss: 0.0299, Val Loss: 0.1153\n",
      "Epoch [3415/4000] - Train Loss: 0.0297, Val Loss: 0.1143\n",
      "Epoch [3416/4000] - Train Loss: 0.0300, Val Loss: 0.1146\n",
      "Epoch [3417/4000] - Train Loss: 0.0302, Val Loss: 0.1172\n",
      "Epoch [3418/4000] - Train Loss: 0.0301, Val Loss: 0.1170\n",
      "Epoch [3419/4000] - Train Loss: 0.0304, Val Loss: 0.1150\n",
      "Epoch [3420/4000] - Train Loss: 0.0310, Val Loss: 0.1135\n",
      "Epoch [3421/4000] - Train Loss: 0.0309, Val Loss: 0.1145\n",
      "Epoch [3422/4000] - Train Loss: 0.0311, Val Loss: 0.1151\n",
      "Epoch [3423/4000] - Train Loss: 0.0304, Val Loss: 0.1145\n",
      "Epoch [3424/4000] - Train Loss: 0.0303, Val Loss: 0.1152\n",
      "Epoch [3425/4000] - Train Loss: 0.0308, Val Loss: 0.1145\n",
      "Epoch [3426/4000] - Train Loss: 0.0316, Val Loss: 0.1163\n",
      "Epoch [3427/4000] - Train Loss: 0.0299, Val Loss: 0.1144\n",
      "Epoch [3428/4000] - Train Loss: 0.0301, Val Loss: 0.1150\n",
      "Epoch [3429/4000] - Train Loss: 0.0298, Val Loss: 0.1162\n",
      "Epoch [3430/4000] - Train Loss: 0.0293, Val Loss: 0.1168\n",
      "Epoch [3431/4000] - Train Loss: 0.0296, Val Loss: 0.1155\n",
      "Epoch [3432/4000] - Train Loss: 0.0292, Val Loss: 0.1140\n",
      "Epoch [3433/4000] - Train Loss: 0.0302, Val Loss: 0.1141\n",
      "Epoch [3434/4000] - Train Loss: 0.0306, Val Loss: 0.1174\n",
      "Epoch [3435/4000] - Train Loss: 0.0309, Val Loss: 0.1160\n",
      "Epoch [3436/4000] - Train Loss: 0.0312, Val Loss: 0.1144\n",
      "Epoch [3437/4000] - Train Loss: 0.0300, Val Loss: 0.1156\n",
      "Epoch [3438/4000] - Train Loss: 0.0316, Val Loss: 0.1158\n",
      "Epoch [3439/4000] - Train Loss: 0.0304, Val Loss: 0.1169\n",
      "Epoch [3440/4000] - Train Loss: 0.0317, Val Loss: 0.1184\n",
      "Epoch [3441/4000] - Train Loss: 0.0312, Val Loss: 0.1146\n",
      "Epoch [3442/4000] - Train Loss: 0.0312, Val Loss: 0.1152\n",
      "Epoch [3443/4000] - Train Loss: 0.0306, Val Loss: 0.1144\n",
      "Epoch [3444/4000] - Train Loss: 0.0297, Val Loss: 0.1147\n",
      "Epoch [3445/4000] - Train Loss: 0.0295, Val Loss: 0.1153\n",
      "Epoch [3446/4000] - Train Loss: 0.0294, Val Loss: 0.1147\n",
      "Epoch [3447/4000] - Train Loss: 0.0293, Val Loss: 0.1156\n",
      "Epoch [3448/4000] - Train Loss: 0.0294, Val Loss: 0.1142\n",
      "Epoch [3449/4000] - Train Loss: 0.0302, Val Loss: 0.1153\n",
      "Epoch [3450/4000] - Train Loss: 0.0295, Val Loss: 0.1145\n",
      "Epoch [3451/4000] - Train Loss: 0.0305, Val Loss: 0.1142\n",
      "Epoch [3452/4000] - Train Loss: 0.0308, Val Loss: 0.1141\n",
      "Epoch [3453/4000] - Train Loss: 0.0295, Val Loss: 0.1140\n",
      "Epoch [3454/4000] - Train Loss: 0.0293, Val Loss: 0.1152\n",
      "Epoch [3455/4000] - Train Loss: 0.0290, Val Loss: 0.1133\n",
      "Epoch [3456/4000] - Train Loss: 0.0293, Val Loss: 0.1144\n",
      "Epoch [3457/4000] - Train Loss: 0.0291, Val Loss: 0.1142\n",
      "Epoch [3458/4000] - Train Loss: 0.0293, Val Loss: 0.1154\n",
      "Epoch [3459/4000] - Train Loss: 0.0289, Val Loss: 0.1164\n",
      "Epoch [3460/4000] - Train Loss: 0.0303, Val Loss: 0.1157\n",
      "Epoch [3461/4000] - Train Loss: 0.0307, Val Loss: 0.1147\n",
      "Epoch [3462/4000] - Train Loss: 0.0296, Val Loss: 0.1145\n",
      "Epoch [3463/4000] - Train Loss: 0.0299, Val Loss: 0.1153\n",
      "Epoch [3464/4000] - Train Loss: 0.0297, Val Loss: 0.1159\n",
      "Epoch [3465/4000] - Train Loss: 0.0294, Val Loss: 0.1153\n",
      "Epoch [3466/4000] - Train Loss: 0.0291, Val Loss: 0.1151\n",
      "Epoch [3467/4000] - Train Loss: 0.0294, Val Loss: 0.1155\n",
      "Epoch [3468/4000] - Train Loss: 0.0296, Val Loss: 0.1158\n",
      "Epoch [3469/4000] - Train Loss: 0.0298, Val Loss: 0.1150\n",
      "Epoch [3470/4000] - Train Loss: 0.0303, Val Loss: 0.1157\n",
      "Epoch [3471/4000] - Train Loss: 0.0299, Val Loss: 0.1149\n",
      "Epoch [3472/4000] - Train Loss: 0.0297, Val Loss: 0.1175\n",
      "Epoch [3473/4000] - Train Loss: 0.0304, Val Loss: 0.1148\n",
      "Epoch [3474/4000] - Train Loss: 0.0300, Val Loss: 0.1142\n",
      "Epoch [3475/4000] - Train Loss: 0.0295, Val Loss: 0.1164\n",
      "Epoch [3476/4000] - Train Loss: 0.0294, Val Loss: 0.1153\n",
      "Epoch [3477/4000] - Train Loss: 0.0293, Val Loss: 0.1170\n",
      "Epoch [3478/4000] - Train Loss: 0.0298, Val Loss: 0.1140\n",
      "Epoch [3479/4000] - Train Loss: 0.0308, Val Loss: 0.1155\n",
      "Epoch [3480/4000] - Train Loss: 0.0311, Val Loss: 0.1209\n",
      "Epoch [3481/4000] - Train Loss: 0.0298, Val Loss: 0.1142\n",
      "Epoch [3482/4000] - Train Loss: 0.0310, Val Loss: 0.1143\n",
      "Epoch [3483/4000] - Train Loss: 0.0302, Val Loss: 0.1169\n",
      "Epoch [3484/4000] - Train Loss: 0.0298, Val Loss: 0.1159\n",
      "Epoch [3485/4000] - Train Loss: 0.0303, Val Loss: 0.1165\n",
      "Epoch [3486/4000] - Train Loss: 0.0292, Val Loss: 0.1164\n",
      "Epoch [3487/4000] - Train Loss: 0.0291, Val Loss: 0.1167\n",
      "Epoch [3488/4000] - Train Loss: 0.0289, Val Loss: 0.1145\n",
      "Epoch [3489/4000] - Train Loss: 0.0297, Val Loss: 0.1155\n",
      "Epoch [3490/4000] - Train Loss: 0.0301, Val Loss: 0.1142\n",
      "Epoch [3491/4000] - Train Loss: 0.0292, Val Loss: 0.1144\n",
      "Epoch [3492/4000] - Train Loss: 0.0288, Val Loss: 0.1139\n",
      "Epoch [3493/4000] - Train Loss: 0.0292, Val Loss: 0.1143\n",
      "Epoch [3494/4000] - Train Loss: 0.0282, Val Loss: 0.1149\n",
      "Epoch [3495/4000] - Train Loss: 0.0286, Val Loss: 0.1139\n",
      "Epoch [3496/4000] - Train Loss: 0.0287, Val Loss: 0.1153\n",
      "Epoch [3497/4000] - Train Loss: 0.0295, Val Loss: 0.1164\n",
      "Epoch [3498/4000] - Train Loss: 0.0300, Val Loss: 0.1158\n",
      "Epoch [3499/4000] - Train Loss: 0.0296, Val Loss: 0.1151\n",
      "Epoch [3500/4000] - Train Loss: 0.0304, Val Loss: 0.1161\n",
      "Epoch [3501/4000] - Train Loss: 0.0304, Val Loss: 0.1159\n",
      "Epoch [3502/4000] - Train Loss: 0.0295, Val Loss: 0.1158\n",
      "Epoch [3503/4000] - Train Loss: 0.0298, Val Loss: 0.1164\n",
      "Epoch [3504/4000] - Train Loss: 0.0297, Val Loss: 0.1149\n",
      "Epoch [3505/4000] - Train Loss: 0.0292, Val Loss: 0.1172\n",
      "Epoch [3506/4000] - Train Loss: 0.0294, Val Loss: 0.1163\n",
      "Epoch [3507/4000] - Train Loss: 0.0298, Val Loss: 0.1166\n",
      "Epoch [3508/4000] - Train Loss: 0.0306, Val Loss: 0.1173\n",
      "Epoch [3509/4000] - Train Loss: 0.0312, Val Loss: 0.1169\n",
      "Epoch [3510/4000] - Train Loss: 0.0301, Val Loss: 0.1158\n",
      "Epoch [3511/4000] - Train Loss: 0.0306, Val Loss: 0.1163\n",
      "Epoch [3512/4000] - Train Loss: 0.0296, Val Loss: 0.1144\n",
      "Epoch [3513/4000] - Train Loss: 0.0295, Val Loss: 0.1135\n",
      "Epoch [3514/4000] - Train Loss: 0.0307, Val Loss: 0.1136\n",
      "Epoch [3515/4000] - Train Loss: 0.0297, Val Loss: 0.1175\n",
      "Epoch [3516/4000] - Train Loss: 0.0304, Val Loss: 0.1171\n",
      "Epoch [3517/4000] - Train Loss: 0.0308, Val Loss: 0.1133\n",
      "Epoch [3518/4000] - Train Loss: 0.0291, Val Loss: 0.1157\n",
      "Epoch [3519/4000] - Train Loss: 0.0290, Val Loss: 0.1155\n",
      "Epoch [3520/4000] - Train Loss: 0.0286, Val Loss: 0.1159\n",
      "Epoch [3521/4000] - Train Loss: 0.0294, Val Loss: 0.1157\n",
      "Epoch [3522/4000] - Train Loss: 0.0286, Val Loss: 0.1161\n",
      "Epoch [3523/4000] - Train Loss: 0.0291, Val Loss: 0.1144\n",
      "Epoch [3524/4000] - Train Loss: 0.0292, Val Loss: 0.1159\n",
      "Epoch [3525/4000] - Train Loss: 0.0286, Val Loss: 0.1145\n",
      "Epoch [3526/4000] - Train Loss: 0.0290, Val Loss: 0.1145\n",
      "Epoch [3527/4000] - Train Loss: 0.0306, Val Loss: 0.1154\n",
      "Epoch [3528/4000] - Train Loss: 0.0307, Val Loss: 0.1176\n",
      "Epoch [3529/4000] - Train Loss: 0.0315, Val Loss: 0.1176\n",
      "Epoch [3530/4000] - Train Loss: 0.0305, Val Loss: 0.1127\n",
      "Epoch [3531/4000] - Train Loss: 0.0296, Val Loss: 0.1169\n",
      "Epoch [3532/4000] - Train Loss: 0.0299, Val Loss: 0.1159\n",
      "Epoch [3533/4000] - Train Loss: 0.0299, Val Loss: 0.1152\n",
      "Epoch [3534/4000] - Train Loss: 0.0297, Val Loss: 0.1155\n",
      "Epoch [3535/4000] - Train Loss: 0.0296, Val Loss: 0.1161\n",
      "Epoch [3536/4000] - Train Loss: 0.0296, Val Loss: 0.1139\n",
      "Epoch [3537/4000] - Train Loss: 0.0298, Val Loss: 0.1137\n",
      "Epoch [3538/4000] - Train Loss: 0.0309, Val Loss: 0.1166\n",
      "Epoch [3539/4000] - Train Loss: 0.0307, Val Loss: 0.1151\n",
      "Epoch [3540/4000] - Train Loss: 0.0291, Val Loss: 0.1137\n",
      "Epoch [3541/4000] - Train Loss: 0.0297, Val Loss: 0.1163\n",
      "Epoch [3542/4000] - Train Loss: 0.0291, Val Loss: 0.1147\n",
      "Epoch [3543/4000] - Train Loss: 0.0297, Val Loss: 0.1151\n",
      "Epoch [3544/4000] - Train Loss: 0.0289, Val Loss: 0.1144\n",
      "Epoch [3545/4000] - Train Loss: 0.0284, Val Loss: 0.1156\n",
      "Epoch [3546/4000] - Train Loss: 0.0287, Val Loss: 0.1155\n",
      "Epoch [3547/4000] - Train Loss: 0.0296, Val Loss: 0.1149\n",
      "Epoch [3548/4000] - Train Loss: 0.0292, Val Loss: 0.1164\n",
      "Epoch [3549/4000] - Train Loss: 0.0297, Val Loss: 0.1139\n",
      "Epoch [3550/4000] - Train Loss: 0.0300, Val Loss: 0.1150\n",
      "Epoch [3551/4000] - Train Loss: 0.0291, Val Loss: 0.1181\n",
      "Epoch [3552/4000] - Train Loss: 0.0300, Val Loss: 0.1153\n",
      "Epoch [3553/4000] - Train Loss: 0.0295, Val Loss: 0.1166\n",
      "Epoch [3554/4000] - Train Loss: 0.0296, Val Loss: 0.1183\n",
      "Epoch [3555/4000] - Train Loss: 0.0296, Val Loss: 0.1155\n",
      "Epoch [3556/4000] - Train Loss: 0.0298, Val Loss: 0.1149\n",
      "Epoch [3557/4000] - Train Loss: 0.0300, Val Loss: 0.1149\n",
      "Epoch [3558/4000] - Train Loss: 0.0308, Val Loss: 0.1151\n",
      "Epoch [3559/4000] - Train Loss: 0.0309, Val Loss: 0.1171\n",
      "Epoch [3560/4000] - Train Loss: 0.0302, Val Loss: 0.1148\n",
      "Epoch [3561/4000] - Train Loss: 0.0291, Val Loss: 0.1171\n",
      "Epoch [3562/4000] - Train Loss: 0.0299, Val Loss: 0.1168\n",
      "Epoch [3563/4000] - Train Loss: 0.0296, Val Loss: 0.1140\n",
      "Epoch [3564/4000] - Train Loss: 0.0301, Val Loss: 0.1154\n",
      "Epoch [3565/4000] - Train Loss: 0.0293, Val Loss: 0.1169\n",
      "Epoch [3566/4000] - Train Loss: 0.0286, Val Loss: 0.1148\n",
      "Epoch [3567/4000] - Train Loss: 0.0286, Val Loss: 0.1158\n",
      "Epoch [3568/4000] - Train Loss: 0.0289, Val Loss: 0.1155\n",
      "Epoch [3569/4000] - Train Loss: 0.0301, Val Loss: 0.1170\n",
      "Epoch [3570/4000] - Train Loss: 0.0301, Val Loss: 0.1140\n",
      "Epoch [3571/4000] - Train Loss: 0.0294, Val Loss: 0.1152\n",
      "Epoch [3572/4000] - Train Loss: 0.0295, Val Loss: 0.1159\n",
      "Epoch [3573/4000] - Train Loss: 0.0295, Val Loss: 0.1147\n",
      "Epoch [3574/4000] - Train Loss: 0.0287, Val Loss: 0.1141\n",
      "Epoch [3575/4000] - Train Loss: 0.0287, Val Loss: 0.1146\n",
      "Epoch [3576/4000] - Train Loss: 0.0292, Val Loss: 0.1159\n",
      "Epoch [3577/4000] - Train Loss: 0.0294, Val Loss: 0.1182\n",
      "Epoch [3578/4000] - Train Loss: 0.0294, Val Loss: 0.1148\n",
      "Epoch [3579/4000] - Train Loss: 0.0304, Val Loss: 0.1130\n",
      "Epoch [3580/4000] - Train Loss: 0.0306, Val Loss: 0.1151\n",
      "Epoch [3581/4000] - Train Loss: 0.0297, Val Loss: 0.1168\n",
      "Epoch [3582/4000] - Train Loss: 0.0286, Val Loss: 0.1168\n",
      "Epoch [3583/4000] - Train Loss: 0.0297, Val Loss: 0.1151\n",
      "Epoch [3584/4000] - Train Loss: 0.0291, Val Loss: 0.1157\n",
      "Epoch [3585/4000] - Train Loss: 0.0286, Val Loss: 0.1155\n",
      "Epoch [3586/4000] - Train Loss: 0.0292, Val Loss: 0.1143\n",
      "Epoch [3587/4000] - Train Loss: 0.0294, Val Loss: 0.1173\n",
      "Epoch [3588/4000] - Train Loss: 0.0298, Val Loss: 0.1151\n",
      "Epoch [3589/4000] - Train Loss: 0.0297, Val Loss: 0.1173\n",
      "Epoch [3590/4000] - Train Loss: 0.0300, Val Loss: 0.1182\n",
      "Epoch [3591/4000] - Train Loss: 0.0310, Val Loss: 0.1161\n",
      "Epoch [3592/4000] - Train Loss: 0.0313, Val Loss: 0.1170\n",
      "Epoch [3593/4000] - Train Loss: 0.0298, Val Loss: 0.1176\n",
      "Epoch [3594/4000] - Train Loss: 0.0299, Val Loss: 0.1199\n",
      "Epoch [3595/4000] - Train Loss: 0.0306, Val Loss: 0.1149\n",
      "Epoch [3596/4000] - Train Loss: 0.0301, Val Loss: 0.1169\n",
      "Epoch [3597/4000] - Train Loss: 0.0288, Val Loss: 0.1138\n",
      "Epoch [3598/4000] - Train Loss: 0.0287, Val Loss: 0.1158\n",
      "Epoch [3599/4000] - Train Loss: 0.0293, Val Loss: 0.1137\n",
      "Epoch [3600/4000] - Train Loss: 0.0297, Val Loss: 0.1149\n",
      "Epoch [3601/4000] - Train Loss: 0.0292, Val Loss: 0.1165\n",
      "Epoch [3602/4000] - Train Loss: 0.0294, Val Loss: 0.1150\n",
      "Epoch [3603/4000] - Train Loss: 0.0293, Val Loss: 0.1159\n",
      "Epoch [3604/4000] - Train Loss: 0.0289, Val Loss: 0.1141\n",
      "Epoch [3605/4000] - Train Loss: 0.0299, Val Loss: 0.1160\n",
      "Epoch [3606/4000] - Train Loss: 0.0289, Val Loss: 0.1160\n",
      "Epoch [3607/4000] - Train Loss: 0.0289, Val Loss: 0.1150\n",
      "Epoch [3608/4000] - Train Loss: 0.0289, Val Loss: 0.1156\n",
      "Epoch [3609/4000] - Train Loss: 0.0284, Val Loss: 0.1149\n",
      "Epoch [3610/4000] - Train Loss: 0.0295, Val Loss: 0.1146\n",
      "Epoch [3611/4000] - Train Loss: 0.0295, Val Loss: 0.1157\n",
      "Epoch [3612/4000] - Train Loss: 0.0292, Val Loss: 0.1166\n",
      "Epoch [3613/4000] - Train Loss: 0.0293, Val Loss: 0.1150\n",
      "Epoch [3614/4000] - Train Loss: 0.0291, Val Loss: 0.1152\n",
      "Epoch [3615/4000] - Train Loss: 0.0291, Val Loss: 0.1176\n",
      "Epoch [3616/4000] - Train Loss: 0.0295, Val Loss: 0.1144\n",
      "Epoch [3617/4000] - Train Loss: 0.0291, Val Loss: 0.1136\n",
      "Epoch [3618/4000] - Train Loss: 0.0283, Val Loss: 0.1138\n",
      "Epoch [3619/4000] - Train Loss: 0.0284, Val Loss: 0.1155\n",
      "Epoch [3620/4000] - Train Loss: 0.0289, Val Loss: 0.1172\n",
      "Epoch [3621/4000] - Train Loss: 0.0293, Val Loss: 0.1144\n",
      "Epoch [3622/4000] - Train Loss: 0.0292, Val Loss: 0.1152\n",
      "Epoch [3623/4000] - Train Loss: 0.0289, Val Loss: 0.1148\n",
      "Epoch [3624/4000] - Train Loss: 0.0292, Val Loss: 0.1160\n",
      "Epoch [3625/4000] - Train Loss: 0.0291, Val Loss: 0.1143\n",
      "Epoch [3626/4000] - Train Loss: 0.0287, Val Loss: 0.1167\n",
      "Epoch [3627/4000] - Train Loss: 0.0294, Val Loss: 0.1140\n",
      "Epoch [3628/4000] - Train Loss: 0.0302, Val Loss: 0.1145\n",
      "Epoch [3629/4000] - Train Loss: 0.0305, Val Loss: 0.1164\n",
      "Epoch [3630/4000] - Train Loss: 0.0290, Val Loss: 0.1151\n",
      "Epoch [3631/4000] - Train Loss: 0.0290, Val Loss: 0.1140\n",
      "Epoch [3632/4000] - Train Loss: 0.0297, Val Loss: 0.1151\n",
      "Epoch [3633/4000] - Train Loss: 0.0300, Val Loss: 0.1166\n",
      "Epoch [3634/4000] - Train Loss: 0.0301, Val Loss: 0.1150\n",
      "Epoch [3635/4000] - Train Loss: 0.0302, Val Loss: 0.1148\n",
      "Epoch [3636/4000] - Train Loss: 0.0298, Val Loss: 0.1161\n",
      "Epoch [3637/4000] - Train Loss: 0.0299, Val Loss: 0.1153\n",
      "Epoch [3638/4000] - Train Loss: 0.0295, Val Loss: 0.1159\n",
      "Epoch [3639/4000] - Train Loss: 0.0285, Val Loss: 0.1151\n",
      "Epoch [3640/4000] - Train Loss: 0.0284, Val Loss: 0.1151\n",
      "Epoch [3641/4000] - Train Loss: 0.0282, Val Loss: 0.1159\n",
      "Epoch [3642/4000] - Train Loss: 0.0289, Val Loss: 0.1143\n",
      "Epoch [3643/4000] - Train Loss: 0.0282, Val Loss: 0.1156\n",
      "Epoch [3644/4000] - Train Loss: 0.0295, Val Loss: 0.1181\n",
      "Epoch [3645/4000] - Train Loss: 0.0302, Val Loss: 0.1160\n",
      "Epoch [3646/4000] - Train Loss: 0.0289, Val Loss: 0.1147\n",
      "Epoch [3647/4000] - Train Loss: 0.0286, Val Loss: 0.1173\n",
      "Epoch [3648/4000] - Train Loss: 0.0302, Val Loss: 0.1148\n",
      "Epoch [3649/4000] - Train Loss: 0.0290, Val Loss: 0.1142\n",
      "Epoch [3650/4000] - Train Loss: 0.0288, Val Loss: 0.1141\n",
      "Epoch [3651/4000] - Train Loss: 0.0286, Val Loss: 0.1153\n",
      "Epoch [3652/4000] - Train Loss: 0.0286, Val Loss: 0.1160\n",
      "Epoch [3653/4000] - Train Loss: 0.0294, Val Loss: 0.1159\n",
      "Epoch [3654/4000] - Train Loss: 0.0294, Val Loss: 0.1144\n",
      "Epoch [3655/4000] - Train Loss: 0.0293, Val Loss: 0.1176\n",
      "Epoch [3656/4000] - Train Loss: 0.0307, Val Loss: 0.1188\n",
      "Epoch [3657/4000] - Train Loss: 0.0294, Val Loss: 0.1133\n",
      "Epoch [3658/4000] - Train Loss: 0.0293, Val Loss: 0.1147\n",
      "Epoch [3659/4000] - Train Loss: 0.0292, Val Loss: 0.1177\n",
      "Epoch [3660/4000] - Train Loss: 0.0301, Val Loss: 0.1165\n",
      "Epoch [3661/4000] - Train Loss: 0.0309, Val Loss: 0.1155\n",
      "Epoch [3662/4000] - Train Loss: 0.0306, Val Loss: 0.1160\n",
      "Epoch [3663/4000] - Train Loss: 0.0312, Val Loss: 0.1167\n",
      "Epoch [3664/4000] - Train Loss: 0.0296, Val Loss: 0.1164\n",
      "Epoch [3665/4000] - Train Loss: 0.0293, Val Loss: 0.1150\n",
      "Epoch [3666/4000] - Train Loss: 0.0289, Val Loss: 0.1156\n",
      "Epoch [3667/4000] - Train Loss: 0.0284, Val Loss: 0.1155\n",
      "Epoch [3668/4000] - Train Loss: 0.0288, Val Loss: 0.1151\n",
      "Epoch [3669/4000] - Train Loss: 0.0284, Val Loss: 0.1150\n",
      "Epoch [3670/4000] - Train Loss: 0.0286, Val Loss: 0.1158\n",
      "Epoch [3671/4000] - Train Loss: 0.0292, Val Loss: 0.1146\n",
      "Epoch [3672/4000] - Train Loss: 0.0290, Val Loss: 0.1166\n",
      "Epoch [3673/4000] - Train Loss: 0.0292, Val Loss: 0.1170\n",
      "Epoch [3674/4000] - Train Loss: 0.0304, Val Loss: 0.1145\n",
      "Epoch [3675/4000] - Train Loss: 0.0296, Val Loss: 0.1157\n",
      "Epoch [3676/4000] - Train Loss: 0.0299, Val Loss: 0.1175\n",
      "Epoch [3677/4000] - Train Loss: 0.0285, Val Loss: 0.1144\n",
      "Epoch [3678/4000] - Train Loss: 0.0290, Val Loss: 0.1147\n",
      "Epoch [3679/4000] - Train Loss: 0.0292, Val Loss: 0.1173\n",
      "Epoch [3680/4000] - Train Loss: 0.0293, Val Loss: 0.1172\n",
      "Epoch [3681/4000] - Train Loss: 0.0299, Val Loss: 0.1155\n",
      "Epoch [3682/4000] - Train Loss: 0.0291, Val Loss: 0.1147\n",
      "Epoch [3683/4000] - Train Loss: 0.0286, Val Loss: 0.1151\n",
      "Epoch [3684/4000] - Train Loss: 0.0286, Val Loss: 0.1147\n",
      "Epoch [3685/4000] - Train Loss: 0.0299, Val Loss: 0.1145\n",
      "Epoch [3686/4000] - Train Loss: 0.0295, Val Loss: 0.1145\n",
      "Epoch [3687/4000] - Train Loss: 0.0291, Val Loss: 0.1140\n",
      "Epoch [3688/4000] - Train Loss: 0.0290, Val Loss: 0.1157\n",
      "Epoch [3689/4000] - Train Loss: 0.0295, Val Loss: 0.1156\n",
      "Epoch [3690/4000] - Train Loss: 0.0291, Val Loss: 0.1143\n",
      "Epoch [3691/4000] - Train Loss: 0.0282, Val Loss: 0.1152\n",
      "Epoch [3692/4000] - Train Loss: 0.0297, Val Loss: 0.1145\n",
      "Epoch [3693/4000] - Train Loss: 0.0303, Val Loss: 0.1142\n",
      "Epoch [3694/4000] - Train Loss: 0.0290, Val Loss: 0.1147\n",
      "Epoch [3695/4000] - Train Loss: 0.0294, Val Loss: 0.1156\n",
      "Epoch [3696/4000] - Train Loss: 0.0284, Val Loss: 0.1146\n",
      "Epoch [3697/4000] - Train Loss: 0.0291, Val Loss: 0.1137\n",
      "Epoch [3698/4000] - Train Loss: 0.0286, Val Loss: 0.1141\n",
      "Epoch [3699/4000] - Train Loss: 0.0286, Val Loss: 0.1163\n",
      "Epoch [3700/4000] - Train Loss: 0.0285, Val Loss: 0.1146\n",
      "Epoch [3701/4000] - Train Loss: 0.0291, Val Loss: 0.1148\n",
      "Epoch [3702/4000] - Train Loss: 0.0288, Val Loss: 0.1136\n",
      "Epoch [3703/4000] - Train Loss: 0.0290, Val Loss: 0.1156\n",
      "Epoch [3704/4000] - Train Loss: 0.0294, Val Loss: 0.1151\n",
      "Epoch [3705/4000] - Train Loss: 0.0285, Val Loss: 0.1138\n",
      "Epoch [3706/4000] - Train Loss: 0.0284, Val Loss: 0.1160\n",
      "Epoch [3707/4000] - Train Loss: 0.0290, Val Loss: 0.1154\n",
      "Epoch [3708/4000] - Train Loss: 0.0287, Val Loss: 0.1152\n",
      "Epoch [3709/4000] - Train Loss: 0.0296, Val Loss: 0.1164\n",
      "Epoch [3710/4000] - Train Loss: 0.0287, Val Loss: 0.1164\n",
      "Epoch [3711/4000] - Train Loss: 0.0295, Val Loss: 0.1142\n",
      "Epoch [3712/4000] - Train Loss: 0.0291, Val Loss: 0.1144\n",
      "Epoch [3713/4000] - Train Loss: 0.0290, Val Loss: 0.1165\n",
      "Epoch [3714/4000] - Train Loss: 0.0294, Val Loss: 0.1194\n",
      "Epoch [3715/4000] - Train Loss: 0.0297, Val Loss: 0.1154\n",
      "Epoch [3716/4000] - Train Loss: 0.0294, Val Loss: 0.1148\n",
      "Epoch [3717/4000] - Train Loss: 0.0299, Val Loss: 0.1149\n",
      "Epoch [3718/4000] - Train Loss: 0.0295, Val Loss: 0.1178\n",
      "Epoch [3719/4000] - Train Loss: 0.0301, Val Loss: 0.1188\n",
      "Epoch [3720/4000] - Train Loss: 0.0311, Val Loss: 0.1181\n",
      "Epoch [3721/4000] - Train Loss: 0.0299, Val Loss: 0.1159\n",
      "Epoch [3722/4000] - Train Loss: 0.0292, Val Loss: 0.1151\n",
      "Epoch [3723/4000] - Train Loss: 0.0293, Val Loss: 0.1129\n",
      "Epoch [3724/4000] - Train Loss: 0.0291, Val Loss: 0.1159\n",
      "Epoch [3725/4000] - Train Loss: 0.0285, Val Loss: 0.1134\n",
      "Epoch [3726/4000] - Train Loss: 0.0285, Val Loss: 0.1144\n",
      "Epoch [3727/4000] - Train Loss: 0.0289, Val Loss: 0.1137\n",
      "Epoch [3728/4000] - Train Loss: 0.0290, Val Loss: 0.1149\n",
      "Epoch [3729/4000] - Train Loss: 0.0287, Val Loss: 0.1141\n",
      "Epoch [3730/4000] - Train Loss: 0.0285, Val Loss: 0.1153\n",
      "Epoch [3731/4000] - Train Loss: 0.0284, Val Loss: 0.1161\n",
      "Epoch [3732/4000] - Train Loss: 0.0284, Val Loss: 0.1161\n",
      "Epoch [3733/4000] - Train Loss: 0.0292, Val Loss: 0.1148\n",
      "Epoch [3734/4000] - Train Loss: 0.0288, Val Loss: 0.1144\n",
      "Epoch [3735/4000] - Train Loss: 0.0284, Val Loss: 0.1136\n",
      "Epoch [3736/4000] - Train Loss: 0.0283, Val Loss: 0.1168\n",
      "Epoch [3737/4000] - Train Loss: 0.0292, Val Loss: 0.1168\n",
      "Epoch [3738/4000] - Train Loss: 0.0286, Val Loss: 0.1155\n",
      "Epoch [3739/4000] - Train Loss: 0.0287, Val Loss: 0.1169\n",
      "Epoch [3740/4000] - Train Loss: 0.0287, Val Loss: 0.1149\n",
      "Epoch [3741/4000] - Train Loss: 0.0291, Val Loss: 0.1152\n",
      "Epoch [3742/4000] - Train Loss: 0.0292, Val Loss: 0.1140\n",
      "Epoch [3743/4000] - Train Loss: 0.0285, Val Loss: 0.1149\n",
      "Epoch [3744/4000] - Train Loss: 0.0290, Val Loss: 0.1173\n",
      "Epoch [3745/4000] - Train Loss: 0.0298, Val Loss: 0.1148\n",
      "Epoch [3746/4000] - Train Loss: 0.0286, Val Loss: 0.1165\n",
      "Epoch [3747/4000] - Train Loss: 0.0285, Val Loss: 0.1144\n",
      "Epoch [3748/4000] - Train Loss: 0.0296, Val Loss: 0.1148\n",
      "Epoch [3749/4000] - Train Loss: 0.0288, Val Loss: 0.1169\n",
      "Epoch [3750/4000] - Train Loss: 0.0287, Val Loss: 0.1167\n",
      "Epoch [3751/4000] - Train Loss: 0.0291, Val Loss: 0.1158\n",
      "Epoch [3752/4000] - Train Loss: 0.0291, Val Loss: 0.1153\n",
      "Epoch [3753/4000] - Train Loss: 0.0301, Val Loss: 0.1155\n",
      "Epoch [3754/4000] - Train Loss: 0.0290, Val Loss: 0.1163\n",
      "Epoch [3755/4000] - Train Loss: 0.0297, Val Loss: 0.1167\n",
      "Epoch [3756/4000] - Train Loss: 0.0300, Val Loss: 0.1174\n",
      "Epoch [3757/4000] - Train Loss: 0.0294, Val Loss: 0.1149\n",
      "Epoch [3758/4000] - Train Loss: 0.0289, Val Loss: 0.1157\n",
      "Epoch [3759/4000] - Train Loss: 0.0293, Val Loss: 0.1163\n",
      "Epoch [3760/4000] - Train Loss: 0.0308, Val Loss: 0.1144\n",
      "Epoch [3761/4000] - Train Loss: 0.0306, Val Loss: 0.1157\n",
      "Epoch [3762/4000] - Train Loss: 0.0310, Val Loss: 0.1140\n",
      "Epoch [3763/4000] - Train Loss: 0.0296, Val Loss: 0.1142\n",
      "Epoch [3764/4000] - Train Loss: 0.0286, Val Loss: 0.1158\n",
      "Epoch [3765/4000] - Train Loss: 0.0284, Val Loss: 0.1161\n",
      "Epoch [3766/4000] - Train Loss: 0.0287, Val Loss: 0.1140\n",
      "Epoch [3767/4000] - Train Loss: 0.0283, Val Loss: 0.1138\n",
      "Epoch [3768/4000] - Train Loss: 0.0285, Val Loss: 0.1159\n",
      "Epoch [3769/4000] - Train Loss: 0.0286, Val Loss: 0.1136\n",
      "Epoch [3770/4000] - Train Loss: 0.0278, Val Loss: 0.1145\n",
      "Epoch [3771/4000] - Train Loss: 0.0281, Val Loss: 0.1150\n",
      "Epoch [3772/4000] - Train Loss: 0.0290, Val Loss: 0.1155\n",
      "Epoch [3773/4000] - Train Loss: 0.0286, Val Loss: 0.1158\n",
      "Epoch [3774/4000] - Train Loss: 0.0297, Val Loss: 0.1151\n",
      "Epoch [3775/4000] - Train Loss: 0.0290, Val Loss: 0.1133\n",
      "Epoch [3776/4000] - Train Loss: 0.0291, Val Loss: 0.1146\n",
      "Epoch [3777/4000] - Train Loss: 0.0293, Val Loss: 0.1162\n",
      "Epoch [3778/4000] - Train Loss: 0.0283, Val Loss: 0.1152\n",
      "Epoch [3779/4000] - Train Loss: 0.0290, Val Loss: 0.1150\n",
      "Epoch [3780/4000] - Train Loss: 0.0290, Val Loss: 0.1146\n",
      "Epoch [3781/4000] - Train Loss: 0.0286, Val Loss: 0.1145\n",
      "Epoch [3782/4000] - Train Loss: 0.0283, Val Loss: 0.1126\n",
      "Epoch [3783/4000] - Train Loss: 0.0287, Val Loss: 0.1173\n",
      "Epoch [3784/4000] - Train Loss: 0.0285, Val Loss: 0.1136\n",
      "Epoch [3785/4000] - Train Loss: 0.0278, Val Loss: 0.1162\n",
      "Epoch [3786/4000] - Train Loss: 0.0287, Val Loss: 0.1150\n",
      "Epoch [3787/4000] - Train Loss: 0.0289, Val Loss: 0.1154\n",
      "Epoch [3788/4000] - Train Loss: 0.0288, Val Loss: 0.1153\n",
      "Epoch [3789/4000] - Train Loss: 0.0294, Val Loss: 0.1174\n",
      "Epoch [3790/4000] - Train Loss: 0.0309, Val Loss: 0.1176\n",
      "Epoch [3791/4000] - Train Loss: 0.0310, Val Loss: 0.1189\n",
      "Epoch [3792/4000] - Train Loss: 0.0302, Val Loss: 0.1160\n",
      "Epoch [3793/4000] - Train Loss: 0.0291, Val Loss: 0.1171\n",
      "Epoch [3794/4000] - Train Loss: 0.0290, Val Loss: 0.1150\n",
      "Epoch [3795/4000] - Train Loss: 0.0291, Val Loss: 0.1142\n",
      "Epoch [3796/4000] - Train Loss: 0.0298, Val Loss: 0.1150\n",
      "Epoch [3797/4000] - Train Loss: 0.0302, Val Loss: 0.1183\n",
      "Epoch [3798/4000] - Train Loss: 0.0300, Val Loss: 0.1146\n",
      "Epoch [3799/4000] - Train Loss: 0.0298, Val Loss: 0.1145\n",
      "Epoch [3800/4000] - Train Loss: 0.0312, Val Loss: 0.1184\n",
      "Epoch [3801/4000] - Train Loss: 0.0300, Val Loss: 0.1114\n",
      "Epoch [3802/4000] - Train Loss: 0.0301, Val Loss: 0.1184\n",
      "Epoch [3803/4000] - Train Loss: 0.0294, Val Loss: 0.1149\n",
      "Epoch [3804/4000] - Train Loss: 0.0293, Val Loss: 0.1148\n",
      "Epoch [3805/4000] - Train Loss: 0.0300, Val Loss: 0.1168\n",
      "Epoch [3806/4000] - Train Loss: 0.0295, Val Loss: 0.1157\n",
      "Epoch [3807/4000] - Train Loss: 0.0287, Val Loss: 0.1148\n",
      "Epoch [3808/4000] - Train Loss: 0.0279, Val Loss: 0.1141\n",
      "Epoch [3809/4000] - Train Loss: 0.0280, Val Loss: 0.1156\n",
      "Epoch [3810/4000] - Train Loss: 0.0293, Val Loss: 0.1165\n",
      "Epoch [3811/4000] - Train Loss: 0.0303, Val Loss: 0.1147\n",
      "Epoch [3812/4000] - Train Loss: 0.0290, Val Loss: 0.1139\n",
      "Epoch [3813/4000] - Train Loss: 0.0285, Val Loss: 0.1162\n",
      "Epoch [3814/4000] - Train Loss: 0.0292, Val Loss: 0.1142\n",
      "Epoch [3815/4000] - Train Loss: 0.0286, Val Loss: 0.1145\n",
      "Epoch [3816/4000] - Train Loss: 0.0283, Val Loss: 0.1143\n",
      "Epoch [3817/4000] - Train Loss: 0.0285, Val Loss: 0.1153\n",
      "Epoch [3818/4000] - Train Loss: 0.0285, Val Loss: 0.1160\n",
      "Epoch [3819/4000] - Train Loss: 0.0288, Val Loss: 0.1139\n",
      "Epoch [3820/4000] - Train Loss: 0.0294, Val Loss: 0.1150\n",
      "Epoch [3821/4000] - Train Loss: 0.0287, Val Loss: 0.1155\n",
      "Epoch [3822/4000] - Train Loss: 0.0286, Val Loss: 0.1154\n",
      "Epoch [3823/4000] - Train Loss: 0.0286, Val Loss: 0.1150\n",
      "Epoch [3824/4000] - Train Loss: 0.0286, Val Loss: 0.1153\n",
      "Epoch [3825/4000] - Train Loss: 0.0283, Val Loss: 0.1142\n",
      "Epoch [3826/4000] - Train Loss: 0.0283, Val Loss: 0.1153\n",
      "Epoch [3827/4000] - Train Loss: 0.0284, Val Loss: 0.1143\n",
      "Epoch [3828/4000] - Train Loss: 0.0279, Val Loss: 0.1151\n",
      "Epoch [3829/4000] - Train Loss: 0.0284, Val Loss: 0.1140\n",
      "Epoch [3830/4000] - Train Loss: 0.0281, Val Loss: 0.1139\n",
      "Epoch [3831/4000] - Train Loss: 0.0277, Val Loss: 0.1159\n",
      "Epoch [3832/4000] - Train Loss: 0.0282, Val Loss: 0.1147\n",
      "Epoch [3833/4000] - Train Loss: 0.0287, Val Loss: 0.1151\n",
      "Epoch [3834/4000] - Train Loss: 0.0289, Val Loss: 0.1142\n",
      "Epoch [3835/4000] - Train Loss: 0.0288, Val Loss: 0.1150\n",
      "Epoch [3836/4000] - Train Loss: 0.0296, Val Loss: 0.1167\n",
      "Epoch [3837/4000] - Train Loss: 0.0302, Val Loss: 0.1164\n",
      "Epoch [3838/4000] - Train Loss: 0.0303, Val Loss: 0.1174\n",
      "Epoch [3839/4000] - Train Loss: 0.0287, Val Loss: 0.1150\n",
      "Epoch [3840/4000] - Train Loss: 0.0285, Val Loss: 0.1138\n",
      "Epoch [3841/4000] - Train Loss: 0.0281, Val Loss: 0.1171\n",
      "Epoch [3842/4000] - Train Loss: 0.0281, Val Loss: 0.1150\n",
      "Epoch [3843/4000] - Train Loss: 0.0286, Val Loss: 0.1133\n",
      "Epoch [3844/4000] - Train Loss: 0.0284, Val Loss: 0.1162\n",
      "Epoch [3845/4000] - Train Loss: 0.0282, Val Loss: 0.1165\n",
      "Epoch [3846/4000] - Train Loss: 0.0289, Val Loss: 0.1147\n",
      "Epoch [3847/4000] - Train Loss: 0.0287, Val Loss: 0.1159\n",
      "Epoch [3848/4000] - Train Loss: 0.0291, Val Loss: 0.1151\n",
      "Epoch [3849/4000] - Train Loss: 0.0287, Val Loss: 0.1157\n",
      "Epoch [3850/4000] - Train Loss: 0.0297, Val Loss: 0.1154\n",
      "Epoch [3851/4000] - Train Loss: 0.0293, Val Loss: 0.1143\n",
      "Epoch [3852/4000] - Train Loss: 0.0286, Val Loss: 0.1149\n",
      "Epoch [3853/4000] - Train Loss: 0.0282, Val Loss: 0.1155\n",
      "Epoch [3854/4000] - Train Loss: 0.0281, Val Loss: 0.1146\n",
      "Epoch [3855/4000] - Train Loss: 0.0287, Val Loss: 0.1174\n",
      "Epoch [3856/4000] - Train Loss: 0.0289, Val Loss: 0.1165\n",
      "Epoch [3857/4000] - Train Loss: 0.0289, Val Loss: 0.1132\n",
      "Epoch [3858/4000] - Train Loss: 0.0289, Val Loss: 0.1136\n",
      "Epoch [3859/4000] - Train Loss: 0.0292, Val Loss: 0.1154\n",
      "Epoch [3860/4000] - Train Loss: 0.0291, Val Loss: 0.1128\n",
      "Epoch [3861/4000] - Train Loss: 0.0292, Val Loss: 0.1166\n",
      "Epoch [3862/4000] - Train Loss: 0.0294, Val Loss: 0.1138\n",
      "Epoch [3863/4000] - Train Loss: 0.0304, Val Loss: 0.1164\n",
      "Epoch [3864/4000] - Train Loss: 0.0289, Val Loss: 0.1131\n",
      "Epoch [3865/4000] - Train Loss: 0.0296, Val Loss: 0.1155\n",
      "Epoch [3866/4000] - Train Loss: 0.0295, Val Loss: 0.1162\n",
      "Epoch [3867/4000] - Train Loss: 0.0291, Val Loss: 0.1172\n",
      "Epoch [3868/4000] - Train Loss: 0.0291, Val Loss: 0.1154\n",
      "Epoch [3869/4000] - Train Loss: 0.0291, Val Loss: 0.1168\n",
      "Epoch [3870/4000] - Train Loss: 0.0294, Val Loss: 0.1143\n",
      "Epoch [3871/4000] - Train Loss: 0.0293, Val Loss: 0.1163\n",
      "Epoch [3872/4000] - Train Loss: 0.0287, Val Loss: 0.1145\n",
      "Epoch [3873/4000] - Train Loss: 0.0285, Val Loss: 0.1162\n",
      "Epoch [3874/4000] - Train Loss: 0.0292, Val Loss: 0.1168\n",
      "Epoch [3875/4000] - Train Loss: 0.0286, Val Loss: 0.1170\n",
      "Epoch [3876/4000] - Train Loss: 0.0290, Val Loss: 0.1169\n",
      "Epoch [3877/4000] - Train Loss: 0.0285, Val Loss: 0.1159\n",
      "Epoch [3878/4000] - Train Loss: 0.0285, Val Loss: 0.1140\n",
      "Epoch [3879/4000] - Train Loss: 0.0289, Val Loss: 0.1153\n",
      "Epoch [3880/4000] - Train Loss: 0.0286, Val Loss: 0.1139\n",
      "Epoch [3881/4000] - Train Loss: 0.0286, Val Loss: 0.1143\n",
      "Epoch [3882/4000] - Train Loss: 0.0279, Val Loss: 0.1154\n",
      "Epoch [3883/4000] - Train Loss: 0.0288, Val Loss: 0.1143\n",
      "Epoch [3884/4000] - Train Loss: 0.0290, Val Loss: 0.1154\n",
      "Epoch [3885/4000] - Train Loss: 0.0286, Val Loss: 0.1134\n",
      "Epoch [3886/4000] - Train Loss: 0.0285, Val Loss: 0.1160\n",
      "Epoch [3887/4000] - Train Loss: 0.0286, Val Loss: 0.1151\n",
      "Epoch [3888/4000] - Train Loss: 0.0279, Val Loss: 0.1151\n",
      "Epoch [3889/4000] - Train Loss: 0.0279, Val Loss: 0.1142\n",
      "Epoch [3890/4000] - Train Loss: 0.0293, Val Loss: 0.1139\n",
      "Epoch [3891/4000] - Train Loss: 0.0285, Val Loss: 0.1156\n",
      "Epoch [3892/4000] - Train Loss: 0.0299, Val Loss: 0.1162\n",
      "Epoch [3893/4000] - Train Loss: 0.0297, Val Loss: 0.1153\n",
      "Epoch [3894/4000] - Train Loss: 0.0287, Val Loss: 0.1142\n",
      "Epoch [3895/4000] - Train Loss: 0.0284, Val Loss: 0.1142\n",
      "Epoch [3896/4000] - Train Loss: 0.0276, Val Loss: 0.1155\n",
      "Epoch [3897/4000] - Train Loss: 0.0283, Val Loss: 0.1150\n",
      "Epoch [3898/4000] - Train Loss: 0.0281, Val Loss: 0.1158\n",
      "Epoch [3899/4000] - Train Loss: 0.0286, Val Loss: 0.1126\n",
      "Epoch [3900/4000] - Train Loss: 0.0283, Val Loss: 0.1149\n",
      "Epoch [3901/4000] - Train Loss: 0.0282, Val Loss: 0.1174\n",
      "Epoch [3902/4000] - Train Loss: 0.0285, Val Loss: 0.1132\n",
      "Epoch [3903/4000] - Train Loss: 0.0285, Val Loss: 0.1155\n",
      "Epoch [3904/4000] - Train Loss: 0.0297, Val Loss: 0.1169\n",
      "Epoch [3905/4000] - Train Loss: 0.0315, Val Loss: 0.1185\n",
      "Epoch [3906/4000] - Train Loss: 0.0319, Val Loss: 0.1163\n",
      "Epoch [3907/4000] - Train Loss: 0.0301, Val Loss: 0.1141\n",
      "Epoch [3908/4000] - Train Loss: 0.0283, Val Loss: 0.1159\n",
      "Epoch [3909/4000] - Train Loss: 0.0284, Val Loss: 0.1149\n",
      "Epoch [3910/4000] - Train Loss: 0.0285, Val Loss: 0.1158\n",
      "Epoch [3911/4000] - Train Loss: 0.0292, Val Loss: 0.1155\n",
      "Epoch [3912/4000] - Train Loss: 0.0286, Val Loss: 0.1152\n",
      "Epoch [3913/4000] - Train Loss: 0.0276, Val Loss: 0.1151\n",
      "Epoch [3914/4000] - Train Loss: 0.0290, Val Loss: 0.1137\n",
      "Epoch [3915/4000] - Train Loss: 0.0291, Val Loss: 0.1148\n",
      "Epoch [3916/4000] - Train Loss: 0.0294, Val Loss: 0.1129\n",
      "Epoch [3917/4000] - Train Loss: 0.0287, Val Loss: 0.1139\n",
      "Epoch [3918/4000] - Train Loss: 0.0288, Val Loss: 0.1142\n",
      "Epoch [3919/4000] - Train Loss: 0.0283, Val Loss: 0.1130\n",
      "Epoch [3920/4000] - Train Loss: 0.0283, Val Loss: 0.1135\n",
      "Epoch [3921/4000] - Train Loss: 0.0282, Val Loss: 0.1140\n",
      "Epoch [3922/4000] - Train Loss: 0.0288, Val Loss: 0.1147\n",
      "Epoch [3923/4000] - Train Loss: 0.0291, Val Loss: 0.1146\n",
      "Epoch [3924/4000] - Train Loss: 0.0290, Val Loss: 0.1133\n",
      "Epoch [3925/4000] - Train Loss: 0.0287, Val Loss: 0.1150\n",
      "Epoch [3926/4000] - Train Loss: 0.0287, Val Loss: 0.1141\n",
      "Epoch [3927/4000] - Train Loss: 0.0285, Val Loss: 0.1163\n",
      "Epoch [3928/4000] - Train Loss: 0.0295, Val Loss: 0.1140\n",
      "Epoch [3929/4000] - Train Loss: 0.0293, Val Loss: 0.1162\n",
      "Epoch [3930/4000] - Train Loss: 0.0295, Val Loss: 0.1165\n",
      "Epoch [3931/4000] - Train Loss: 0.0285, Val Loss: 0.1158\n",
      "Epoch [3932/4000] - Train Loss: 0.0294, Val Loss: 0.1162\n",
      "Epoch [3933/4000] - Train Loss: 0.0289, Val Loss: 0.1164\n",
      "Epoch [3934/4000] - Train Loss: 0.0286, Val Loss: 0.1155\n",
      "Epoch [3935/4000] - Train Loss: 0.0289, Val Loss: 0.1189\n",
      "Epoch [3936/4000] - Train Loss: 0.0294, Val Loss: 0.1153\n",
      "Epoch [3937/4000] - Train Loss: 0.0294, Val Loss: 0.1188\n",
      "Epoch [3938/4000] - Train Loss: 0.0288, Val Loss: 0.1128\n",
      "Epoch [3939/4000] - Train Loss: 0.0282, Val Loss: 0.1168\n",
      "Epoch [3940/4000] - Train Loss: 0.0286, Val Loss: 0.1159\n",
      "Epoch [3941/4000] - Train Loss: 0.0287, Val Loss: 0.1175\n",
      "Epoch [3942/4000] - Train Loss: 0.0292, Val Loss: 0.1153\n",
      "Epoch [3943/4000] - Train Loss: 0.0280, Val Loss: 0.1156\n",
      "Epoch [3944/4000] - Train Loss: 0.0279, Val Loss: 0.1143\n",
      "Epoch [3945/4000] - Train Loss: 0.0282, Val Loss: 0.1137\n",
      "Epoch [3946/4000] - Train Loss: 0.0279, Val Loss: 0.1150\n",
      "Epoch [3947/4000] - Train Loss: 0.0282, Val Loss: 0.1145\n",
      "Epoch [3948/4000] - Train Loss: 0.0281, Val Loss: 0.1145\n",
      "Epoch [3949/4000] - Train Loss: 0.0280, Val Loss: 0.1151\n",
      "Epoch [3950/4000] - Train Loss: 0.0280, Val Loss: 0.1140\n",
      "Epoch [3951/4000] - Train Loss: 0.0277, Val Loss: 0.1143\n",
      "Epoch [3952/4000] - Train Loss: 0.0284, Val Loss: 0.1146\n",
      "Epoch [3953/4000] - Train Loss: 0.0291, Val Loss: 0.1141\n",
      "Epoch [3954/4000] - Train Loss: 0.0299, Val Loss: 0.1151\n",
      "Epoch [3955/4000] - Train Loss: 0.0288, Val Loss: 0.1144\n",
      "Epoch [3956/4000] - Train Loss: 0.0285, Val Loss: 0.1134\n",
      "Epoch [3957/4000] - Train Loss: 0.0287, Val Loss: 0.1168\n",
      "Epoch [3958/4000] - Train Loss: 0.0282, Val Loss: 0.1162\n",
      "Epoch [3959/4000] - Train Loss: 0.0287, Val Loss: 0.1149\n",
      "Epoch [3960/4000] - Train Loss: 0.0292, Val Loss: 0.1156\n",
      "Epoch [3961/4000] - Train Loss: 0.0292, Val Loss: 0.1177\n",
      "Epoch [3962/4000] - Train Loss: 0.0285, Val Loss: 0.1154\n",
      "Epoch [3963/4000] - Train Loss: 0.0286, Val Loss: 0.1146\n",
      "Epoch [3964/4000] - Train Loss: 0.0281, Val Loss: 0.1153\n",
      "Epoch [3965/4000] - Train Loss: 0.0281, Val Loss: 0.1146\n",
      "Epoch [3966/4000] - Train Loss: 0.0288, Val Loss: 0.1137\n",
      "Epoch [3967/4000] - Train Loss: 0.0280, Val Loss: 0.1137\n",
      "Epoch [3968/4000] - Train Loss: 0.0281, Val Loss: 0.1143\n",
      "Epoch [3969/4000] - Train Loss: 0.0277, Val Loss: 0.1139\n",
      "Epoch [3970/4000] - Train Loss: 0.0275, Val Loss: 0.1141\n",
      "Epoch [3971/4000] - Train Loss: 0.0278, Val Loss: 0.1151\n",
      "Epoch [3972/4000] - Train Loss: 0.0287, Val Loss: 0.1133\n",
      "Epoch [3973/4000] - Train Loss: 0.0287, Val Loss: 0.1155\n",
      "Epoch [3974/4000] - Train Loss: 0.0278, Val Loss: 0.1150\n",
      "Epoch [3975/4000] - Train Loss: 0.0285, Val Loss: 0.1162\n",
      "Epoch [3976/4000] - Train Loss: 0.0282, Val Loss: 0.1140\n",
      "Epoch [3977/4000] - Train Loss: 0.0286, Val Loss: 0.1150\n",
      "Epoch [3978/4000] - Train Loss: 0.0292, Val Loss: 0.1148\n",
      "Epoch [3979/4000] - Train Loss: 0.0296, Val Loss: 0.1153\n",
      "Epoch [3980/4000] - Train Loss: 0.0286, Val Loss: 0.1127\n",
      "Epoch [3981/4000] - Train Loss: 0.0297, Val Loss: 0.1172\n",
      "Epoch [3982/4000] - Train Loss: 0.0302, Val Loss: 0.1184\n",
      "Epoch [3983/4000] - Train Loss: 0.0292, Val Loss: 0.1178\n",
      "Epoch [3984/4000] - Train Loss: 0.0290, Val Loss: 0.1141\n",
      "Epoch [3985/4000] - Train Loss: 0.0282, Val Loss: 0.1160\n",
      "Epoch [3986/4000] - Train Loss: 0.0291, Val Loss: 0.1136\n",
      "Epoch [3987/4000] - Train Loss: 0.0292, Val Loss: 0.1160\n",
      "Epoch [3988/4000] - Train Loss: 0.0287, Val Loss: 0.1152\n",
      "Epoch [3989/4000] - Train Loss: 0.0280, Val Loss: 0.1145\n",
      "Epoch [3990/4000] - Train Loss: 0.0285, Val Loss: 0.1144\n",
      "Epoch [3991/4000] - Train Loss: 0.0283, Val Loss: 0.1147\n",
      "Epoch [3992/4000] - Train Loss: 0.0284, Val Loss: 0.1155\n",
      "Epoch [3993/4000] - Train Loss: 0.0279, Val Loss: 0.1151\n",
      "Epoch [3994/4000] - Train Loss: 0.0277, Val Loss: 0.1145\n",
      "Epoch [3995/4000] - Train Loss: 0.0280, Val Loss: 0.1145\n",
      "Epoch [3996/4000] - Train Loss: 0.0284, Val Loss: 0.1141\n",
      "Epoch [3997/4000] - Train Loss: 0.0282, Val Loss: 0.1150\n",
      "Epoch [3998/4000] - Train Loss: 0.0278, Val Loss: 0.1158\n",
      "Epoch [3999/4000] - Train Loss: 0.0278, Val Loss: 0.1135\n",
      "Epoch [4000/4000] - Train Loss: 0.0283, Val Loss: 0.1141\n",
      "\n",
      "Best Val Loss: 0.1114 at epoch 3801\n",
      "Saved best model to my_models/best_model_airfoil_data.csv__seed18__2025-03-20-13-16-03.pth\n",
      "Saved loss curve to my_models/loss_curve_airfoil_data.csv__seed18__2025-03-20-13-16-03.png\n",
      "Figure(800x500)\n",
      "Saved overlay figure for sample 81 to my_models/sample_81_overlay.png\n",
      "Saved overlay figure for sample 92 to my_models/sample_92_overlay.png\n",
      "Saved overlay figure for sample 80 to my_models/sample_80_overlay.png\n",
      "\n",
      "Test Predictions vs. True (first 10):\n",
      "  Sample   0 → Predicted=1.2565, True=1.1826\n",
      "  Sample   1 → Predicted=0.7515, True=0.7667\n",
      "  Sample   2 → Predicted=1.0095, True=0.9837\n",
      "  Sample   3 → Predicted=1.1988, True=1.1364\n",
      "  Sample   4 → Predicted=1.1603, True=1.1687\n",
      "  Sample   5 → Predicted=0.6243, True=0.6352\n",
      "  Sample   6 → Predicted=1.1115, True=1.1040\n",
      "  Sample   7 → Predicted=1.0271, True=1.1346\n",
      "  Sample   8 → Predicted=0.8120, True=0.7869\n",
      "  Sample   9 → Predicted=1.2825, True=1.1785\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing your AutoGluon script\n",
    "sys.path.append(os.path.abspath(\"../EngiOpt/engiopt\"))\n",
    "\n",
    "# Now you can import and use the script as if it's in the same directory\n",
    "#from autogluon_multimodal import main\n",
    "\n",
    "\n",
    "# CHECKED:\n",
    "\n",
    "# data_dir\n",
    "# data_input\n",
    "# input_cols\n",
    "# target_col\n",
    "# flatten_columns\n",
    "# hidden_layers\n",
    "# activation\n",
    "# optimizer\n",
    "# learning_rate\n",
    "# n_epochs\n",
    "# batch_size\n",
    "# patience\n",
    "# scale_target\n",
    "# wandb_project\n",
    "# seed\n",
    "# save_model\n",
    "# track\n",
    "# model_output_dir\n",
    "# wandb_project\n",
    "\n",
    "\n",
    "\n",
    "!python ./engiopt/shape2shape_leastV_vae.py \\\n",
    "    --data_dir \"../EngiOpt/data\" \\\n",
    "    --data_input \"airfoil_data.csv\" \\\n",
    "    --init_col \"optimal_design\" \\\n",
    "    --opt_col \"optimal_design\" \\\n",
    "    --target_col \"cl_val\" \\\n",
    "    --params_cols '[\"mach\",\"reynolds\",\"alpha\"]' \\\n",
    "    --flatten_columns '[\"initial_design\",\"optimal_design\"]' \\\n",
    "    --lambda_lv 1e-2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --gamma 1.0 \\\n",
    "    --structured \\\n",
    "    --hidden_layers 3 \\\n",
    "    --hidden_size 32 \\\n",
    "    --latent_dim 8 \\\n",
    "    --n_epochs 4000 \\\n",
    "    --batch_size 32 \\\n",
    "    --patience 500 \\\n",
    "    --scale_target \\\n",
    "    --no-track \\\n",
    "    --seed 18 \\\n",
    "    --save_model \\\n",
    "    --model_output_dir \"my_models\" \\\n",
    "    --test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d5389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
